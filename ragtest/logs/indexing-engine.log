16:32:47,293 graphrag.cli.index INFO Logging enabled at /Users/apple/Documents/project/KG-RAG/ragtest/logs/indexing-engine.log
16:32:47,294 graphrag.cli.index INFO Starting pipeline run for: 20241127-163247, dry_run=False
16:32:47,295 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4-turbo-preview",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/Users/apple/Documents/project/KG-RAG/ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "/Users/apple/Documents/project/KG-RAG/ragtest/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/Users/apple/Documents/project/KG-RAG/ragtest/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 3,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0.0,
        "local_search_top_p": 1.0,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 2000
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
16:32:47,296 graphrag.index.create_pipeline_config INFO skipping workflows 
16:32:47,296 graphrag.index.run.run INFO Running pipeline
16:32:47,296 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /Users/apple/Documents/project/KG-RAG/ragtest/output
16:32:47,296 graphrag.index.input.load_input INFO loading input from root_dir=input
16:32:47,296 graphrag.index.input.load_input INFO using file storage for input
16:32:47,297 graphrag.index.storage.file_pipeline_storage INFO search /Users/apple/Documents/project/KG-RAG/ragtest/input for files matching .*\.txt$
16:32:47,297 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
16:32:47,298 graphrag.index.input.text INFO Found 1 files, loading 1
16:32:47,300 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_final_documents', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'generate_text_embeddings']
16:32:47,300 graphrag.index.run.run INFO Final # of rows loaded: 1
16:32:47,343 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
16:32:47,345 datashaper.workflow.workflow INFO executing verb create_base_text_units
16:33:18,639 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_text_units']
16:33:18,639 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:33:18,642 datashaper.workflow.workflow INFO executing verb create_final_documents
16:33:18,648 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
16:33:18,724 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
16:33:18,724 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:33:18,728 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
16:33:18,731 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
16:33:18,768 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4-turbo-preview: TPM=0, RPM=0
16:33:18,768 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4-turbo-preview: 25
16:33:20,245 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,248 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,253 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,259 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,262 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,271 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,273 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,274 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,282 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,289 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,290 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,302 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,303 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,312 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,334 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,350 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,354 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,368 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,381 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,391 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,419 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,425 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,788 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,734 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,736 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,772 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,843 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,844 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,889 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,935 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,936 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,940 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,980 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,21 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,80 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,93 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,94 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,130 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,251 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,252 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,302 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,304 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,306 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,306 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,353 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,378 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,469 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,472 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,577 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,638 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,641 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,308 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,435 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,440 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,606 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,704 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,707 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,768 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,770 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,778 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,780 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,840 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,889 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,920 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,922 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,924 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,941 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,977 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,979 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,61 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,139 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,141 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,159 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,251 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,405 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,456 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:25,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.666573833001166. input_tokens=2936, output_tokens=153
16:33:25,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,791 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,840 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:26,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:26,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.71959216599862. input_tokens=2936, output_tokens=186
16:33:26,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:26,846 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:28,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:28,642 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,210 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,212 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,299 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,303 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,306 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,437 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,439 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,455 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,457 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,658 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,811 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,897 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,899 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,937 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,941 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,977 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,979 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,30 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,47 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,150 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,152 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,153 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,173 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,175 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,671 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,674 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,676 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,685 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,697 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:31,610 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:31,613 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:36,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:36,323 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:37,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:37,609 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,20 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,338 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,542 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,636 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,855 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,914 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,934 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,935 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,936 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,63 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,72 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,73 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:39,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 12.171458417000395. input_tokens=2936, output_tokens=388
16:33:39,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,411 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,424 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,425 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,450 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,461 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,488 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,585 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,587 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:39,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 14.22006516699912. input_tokens=2936, output_tokens=258
16:33:39,668 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,671 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,726 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,913 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:40,683 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:40,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.89539666699784. input_tokens=2935, output_tokens=391
16:33:41,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:41,33 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:41,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:41,417 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:41,552 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:41,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.741565917000116. input_tokens=2935, output_tokens=400
16:33:41,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:41,574 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:43,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:43,266 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:44,9 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:44,13 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:44,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:44,882 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:45,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:45,435 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:46,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:46,459 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:47,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:47,996 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:48,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:48,496 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:48,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:48,900 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,133 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,290 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,291 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,425 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,427 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,619 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,626 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,628 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,700 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,701 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,717 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,718 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,773 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,787 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,788 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,856 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,858 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,22 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,30 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,31 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,53 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,55 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,70 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,270 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,429 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,517 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:51,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:51,170 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:53,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:53,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.49813712499963. input_tokens=2936, output_tokens=303
16:33:55,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:55,880 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:58,13 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:58,14 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:58,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:58,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 19.708580082999106. input_tokens=2936, output_tokens=470
16:33:58,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:58,806 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:58,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:58,874 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,276 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,471 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,503 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,680 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,680 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,953 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,95 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,159 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,161 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,221 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,365 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,367 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,631 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,712 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,719 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,768 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,769 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,836 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,849 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,852 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,875 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,877 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,944 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,945 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:01,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:01,18 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:03,269 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:03,272 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:06,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:06,254 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:06,787 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:06,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 27.87185904099897. input_tokens=2935, output_tokens=496
16:34:08,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:08,377 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:08,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:08,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.603592332998232. input_tokens=2936, output_tokens=266
16:34:08,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:08,995 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:09,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 10.862156040999253. input_tokens=2936, output_tokens=351
16:34:09,176 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,177 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,574 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,631 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,860 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,19 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,293 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,316 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,438 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,439 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,544 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,739 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,897 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,14 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,15 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,49 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,128 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,129 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,140 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,287 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,404 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,524 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,648 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,777 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,778 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,796 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,819 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:14,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:14,282 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:16,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:16,623 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:18,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:18,745 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:19,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:19,201 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:19,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:19,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 11.322165582998423. input_tokens=2935, output_tokens=354
16:34:19,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:19,955 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:19,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:19,999 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,378 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,629 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,765 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,805 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,70 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,72 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,88 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,375 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,415 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,460 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,464 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,510 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,775 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,149 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,159 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,160 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,161 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,274 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,467 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:24,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:24,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.170364416000666. input_tokens=2935, output_tokens=517
16:34:25,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:25,370 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:27,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:27,469 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:28,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:28,63 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:29,107 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:29,109 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,323 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,323 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 28114, Requested 6818. Please try again in 9.864s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:30,332 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "with my eyes turned down,\nand never raise them to that blessed Star which led the Wise Men to a\npoor abode? Were there no poor homes to which its light would have\nconducted _me_?'\n\nScrooge was very much dismayed to hear the spectre going on at this\nrate, and began to quake exceedingly.\n\n'Hear me!' cried the Ghost. 'My time is nearly gone.'\n\n'I will,' said Scrooge. 'But don't be hard upon me! Don't be flowery,\nJacob! Pray!'\n\n'How it is that I appear before you in a shape that you can see, I may\nnot tell. I have sat invisible beside you many and many a day.'\n\nIt was not an agreeable idea. Scrooge shivered, and wiped the\nperspiration from his brow.\n\n'That is no light part of my penance,' pursued the Ghost. 'I am here\nto-night to warn you that you have yet a chance and hope of escaping my\nfate. A chance and hope of my procuring, Ebenezer.'\n\n'You were always a good friend to me,' said Scrooge. 'Thankee!'\n\n'You will be haunted,' resumed the Ghost, 'by Three Spirits.'\n\nScrooge's countenance fell almost as low as the Ghost's had done.\n\n'Is that the chance and hope you mentioned, Jacob?' he demanded in a\nfaltering voice.\n\n'It is.'\n\n'I--I think I'd rather not,' said Scrooge.\n\n'Without their visits,' said the Ghost, 'you cannot hope to shun the\npath I tread. Expect the first to-morrow when the bell tolls One.'\n\n'Couldn't I take 'em all at once, and have it over, Jacob?' hinted\nScrooge.\n\n'Expect the second on the next night at the same hour. The third, upon\nthe next night when the last stroke of Twelve has ceased to vibrate.\nLook to see me no more; and look that, for your own sake, you remember\nwhat has passed between us!'\n\nWhen it had said these words, the spectre took its wrapper from the\ntable, and bound it round its head as before. Scrooge knew this by the\nsmart sound its teeth made when the jaws were brought together by the\nbandage. He ventured to raise his eyes again, and found his supernatural\nvisitor confronting him in an erect attitude, with its chain wound over\nand about its arm.\n\n[Illustration: _The air was filled with phantoms, wandering hither and\nthither in restless haste and moaning as they went_]\n\nThe apparition walked backward from him; and, at every step it took, the\nwindow raised itself a little, so that, when the spectre reached it, it\nwas wide open. It beckoned Scrooge to approach, which he did. When they\nwere within two paces of each other, Marley's Ghost held up its hand,\nwarning him to come no nearer. Scrooge stopped.\n\nNot so much in obedience as in surprise and fear; for, on the raising of\nthe hand, he became sensible of confused noises in the air; incoherent\nsounds of lamentation and regret; wailings inexpressibly sorrowful and\nself-accusatory. The spectre, after listening for a moment, joined in\nthe mournful dirge; and floated out upon the bleak, dark night.\n\nScrooge followed to the window: desperate in his curiosity. He looked\nout.\n\nThe air was filled with phantoms, wandering hither and thither in\nrestless haste, and moaning as they went. Every one of them wore chains\nlike Marley's Ghost; some few (they might be guilty governments) were\nlinked together; none were free. Many had been personally known to\nScrooge in their lives. He had been quite familiar with one old ghost in\na white waistcoat, with a monstrous iron safe attached to its ankle, who\ncried piteously at being unable to assist a wretched woman with an\ninfant, whom it saw below upon a doorstep. The misery with them all was\nclearly, that they sought to interfere, for good, in human matters, and\nhad lost the power for ever.\n\nWhether these creatures faded into mist, or mist enshrouded them, he\ncould not tell. But they and their spirit voices faded together; and\nthe night became as it had been when he walked home.\n\nScrooge closed the window, and examined the door by which the Ghost had\nentered. It was double locked, as he had locked it with his own hands,\nand the bolts were undisturbed. He tried to say 'Humbug!' but stopped at\nthe first syllable. And being, from the emotions he had undergone, or\nthe fatigues of the day, or his glimpse of the Invisible World, or the\ndull conversation of the Ghost, or the lateness of the hour, much in\nneed of repose, went straight to bed without undressing, and fell asleep\nupon the instant.\n\n[Illustration]\n\n\nSTAVE TWO\n\n[Illustration]\n\n\n\n\nTHE FIRST OF THE THREE SPIRITS\n\n\nWhen Scrooge awoke it was so dark, that, looking out of bed, he could\nscarcely distinguish the transparent window from the opaque walls of his\nchamber. He was endeavouring to pierce the darkness with his ferret\neyes, when the chimes of a neighbouring church struck the four quarters.\nSo he listened for the hour.\n\nTo his great astonishment, the heavy bell went on from six to seven, and\nfrom seven to eight, and"}
16:34:30,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,707 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,775 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,776 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27877, Requested 6785. Please try again in 9.324s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:30,777 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '\'Bah!\' again; and followed it up with \'Humbug!\'\n\n\'Don\'t be cross, uncle!\' said the nephew.\n\n\'What else can I be,\' returned the uncle, \'when I live in such a world\nof fools as this? Merry Christmas! Out upon merry Christmas! What\'s\nChristmas-time to you but a time for paying bills without money; a time\nfor finding yourself a year older, and not an hour richer; a time for\nbalancing your books, and having every item in \'em through a round dozen\nof months presented dead against you? If I could work my will,\' said\nScrooge indignantly, \'every idiot who goes about with "Merry Christmas"\non his lips should be boiled with his own pudding, and buried with a\nstake of holly through his heart. He should!\'\n\n\'Uncle!\' pleaded the nephew.\n\n\'Nephew!\' returned the uncle sternly, \'keep Christmas in your own way,\nand let me keep it in mine.\'\n\n\'Keep it!\' repeated Scrooge\'s nephew. \'But you don\'t keep it.\'\n\n\'Let me leave it alone, then,\' said Scrooge. \'Much good may it do you!\nMuch good it has ever done you!\'\n\n\'There are many things from which I might have derived good, by which I\nhave not profited, I dare say,\' returned the nephew; \'Christmas among\nthe rest. But I am sure I have always thought of Christmas-time, when\nit has come round--apart from the veneration due to its sacred name and\norigin, if anything belonging to it can be apart from that--as a good\ntime; a kind, forgiving, charitable, pleasant time; the only time I know\nof, in the long calendar of the year, when men and women seem by one\nconsent to open their shut-up hearts freely, and to think of people\nbelow them as if they really were fellow-passengers to the grave, and\nnot another race of creatures bound on other journeys. And therefore,\nuncle, though it has never put a scrap of gold or silver in my pocket, I\nbelieve that it _has_ done me good and _will_ do me good; and I say, God\nbless it!\'\n\nThe clerk in the tank involuntarily applauded. Becoming immediately\nsensible of the impropriety, he poked the fire, and extinguished the\nlast frail spark for ever.\n\n\'Let me hear another sound from _you_,\' said Scrooge, \'and you\'ll keep\nyour Christmas by losing your situation! You\'re quite a powerful\nspeaker, sir,\' he added, turning to his nephew. \'I wonder you don\'t go\ninto Parliament.\'\n\n\'Don\'t be angry, uncle. Come! Dine with us to-morrow.\'\n\nScrooge said that he would see him----Yes, indeed he did. He went the\nwhole length of the expression, and said that he would see him in that\nextremity first.\n\n\'But why?\' cried Scrooge\'s nephew. \'Why?\'\n\n\'Why did you get married?\' said Scrooge.\n\n\'Because I fell in love.\'\n\n\'Because you fell in love!\' growled Scrooge, as if that were the only\none thing in the world more ridiculous than a merry Christmas. \'Good\nafternoon!\'\n\n\'Nay, uncle, but you never came to see me before that happened. Why give\nit as a reason for not coming now?\'\n\n\'Good afternoon,\' said Scrooge.\n\n\'I want nothing from you; I ask nothing of you; why cannot we be\nfriends?\'\n\n\'Good afternoon!\' said Scrooge.\n\n\'I am sorry, with all my heart, to find you so resolute. We have never\nhad any quarrel to which I have been a party. But I have made the trial\nin homage to Christmas, and I\'ll keep my Christmas humour to the last.\nSo A Merry Christmas, uncle!\'\n\n\'Good afternoon,\' said Scrooge.\n\n\'And A Happy New Year!\'\n\n\'Good afternoon!\' said Scrooge.\n\nHis nephew left the room without an angry word, notwithstanding. He\nstopped at the outer door to bestow the greetings of the season on the\nclerk, who, cold as he was, was warmer than Scrooge; for he returned\nthem cordially.\n\n\'There\'s another fellow,\' muttered Scrooge, who overheard him: \'my\nclerk, with fifteen shillings a week, and a wife and family, talking\nabout a merry Christmas. I\'ll retire to Bedlam.\'\n\nThis lunatic, in letting Scrooge\'s nephew out, had let two other people\nin. They were portly gentlemen, pleasant to behold, and now stood, with\ntheir hats off, in Scrooge\'s office. They had books and papers in their\nhands, and bowed to him.\n\n\'Scrooge and Marley\'s, I believe,\' said one of the gentlemen, referring\nto his list. \'Have I the pleasure of addressing Mr. Scrooge, or Mr.\nMarley?\'\n\n\'Mr. Marley has been dead these seven years,\' Scrooge replied. \'He died\nseven years ago, this very night.\'\n\n\'We have no doubt his liberality is well represented by his surviving\npartner,\' said the gentleman, presenting his credentials.\n\n[Illustration: THEY WERE PORTLY GENTLEMEN, PLEASANT TO BEHOLD]\n\nIt certainly was; for they had been two kindred spirits. At the ominous\nword \'liberality\' Scrooge frowned, and shook his head, and handed the\ncredentials back.\n\n\'At this festive season of the year, Mr. Scro'}
16:34:30,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,985 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,130 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,130 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27707, Requested 6806. Please try again in 9.026s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,131 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'that the crisp air laughed to hear it.\n\n\'These are but shadows of the things that have been,\' said the Ghost.\n\'They have no consciousness of us.\'\n\nThe jocund travellers came on; and as they came, Scrooge knew and named\nthem every one. Why was he rejoiced beyond all bounds to see them? Why\ndid his cold eye glisten, and his heart leap up as they went past? Why\nwas he filled with gladness when he heard them give each other Merry\nChristmas, as they parted at cross-roads and by-ways for their several\nhomes? What was merry Christmas to Scrooge? Out upon merry Christmas!\nWhat good had it ever done to him?\n\n\'The school is not quite deserted,\' said the Ghost. \'A solitary child,\nneglected by his friends, is left there still.\'\n\nScrooge said he knew it. And he sobbed.\n\nThey left the high-road by a well-remembered lane and soon approached a\nmansion of dull red brick, with a little weather-cock surmounted cupola\non the roof, and a bell hanging in it. It was a large house, but one of\nbroken fortunes; for the spacious offices were little used, their walls\nwere damp and mossy, their windows broken, and their gates decayed.\nFowls clucked and strutted in the stables; and the coach-houses and\nsheds were overrun with grass. Nor was it more retentive of its ancient\nstate within; for, entering the dreary hall, and glancing through the\nopen doors of many rooms, they found them poorly furnished, cold, and\nvast. There was an earthy savour in the air, a chilly bareness in the\nplace, which associated itself somehow with too much getting up by\ncandle light and not too much to eat.\n\nThey went, the Ghost and Scrooge, across the hall, to a door at the back\nof the house. It opened before them, and disclosed a long, bare,\nmelancholy room, made barer still by lines of plain deal forms and\ndesks. At one of these a lonely boy was reading near a feeble fire; and\nScrooge sat down upon a form, and wept to see his poor forgotten self as\nhe had used to be.\n\nNot a latent echo in the house, not a squeak and scuffle from the mice\nbehind the panelling, not a drip from the half-thawed waterspout in the\ndull yard behind, not a sigh among the leafless boughs of one despondent\npoplar, not the idle swinging of an empty storehouse door, no, not a\nclicking in the fire, but fell upon the heart of Scrooge with softening\ninfluence, and gave a freer passage to his tears.\n\nThe Spirit touched him on the arm, and pointed to his younger self,\nintent upon his reading. Suddenly a man in foreign garments, wonderfully\nreal and distinct to look at, stood outside the window, with an axe\nstuck in his belt, and leading by the bridle an ass laden with wood.\n\n\'Why, it\'s Ali Baba!\' Scrooge exclaimed in ecstasy. \'It\'s dear old\nhonest Ali Baba! Yes, yes, I know. One Christmas-time, when yonder\nsolitary child was left here all alone, he _did_ come, for the first\ntime, just like that. Poor boy! And Valentine,\' said Scrooge, \'and his\nwild brother, Orson; there they go! And what\'s his name, who was put\ndown in his drawers, asleep, at the gate of Damascus; don\'t you see him?\nAnd the Sultan\'s Groom turned upside down by the Genii; there he is upon\nhis head! Serve him right! I\'m glad of it. What business had he to be\nmarried to the Princess?\'\n\nTo hear Scrooge expending all the earnestness of his nature on such\nsubjects, in a most extraordinary voice between laughing and crying; and\nto see his heightened and excited face; would have been a surprise to\nhis business friends in the City, indeed.\n\n\'There\'s the Parrot!\' cried Scrooge. \'Green body and yellow tail, with a\nthing like a lettuce growing out of the top of his head; there he is!\nPoor Robin Crusoe he called him, when he came home again after sailing\nround the island. "Poor Robin Crusoe, where have you been, Robin\nCrusoe?" The man thought he was dreaming, but he wasn\'t. It was the\nParrot, you know. There goes Friday, running for his life to the little\ncreek! Halloa! Hoop! Halloo!\'\n\nThen, with a rapidity of transition very foreign to his usual character,\nhe said, in pity for his former self, \'Poor boy!\' and cried again.\n\n\'I wish,\' Scrooge muttered, putting his hand in his pocket, and looking\nabout him, after drying his eyes with his cuff; \'but it\'s too late now.\'\n\n\'What is the matter?\' asked the Spirit.\n\n\'Nothing,\' said Scrooge. \'Nothing. There was a boy singing a Christmas\ncarol at my door last night. I should like to have given him something:\nthat\'s all.\'\n\nThe Ghost smiled thoughtfully, and waved its hand, saying as it did so,\n\'Let us see another Christmas!\'\n\nScrooge\'s former self grew larger at the words, and the room became a\nlittle darker and more dirty. The panels shrunk, the windows cracked;\nfragments of plaster fell out of the ceiling, and the naked l'}
16:34:31,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,142 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,273 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,274 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,274 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27634, Requested 6837. Please try again in 8.942s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,275 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "ITS\n\n\nWhen Scrooge awoke it was so dark, that, looking out of bed, he could\nscarcely distinguish the transparent window from the opaque walls of his\nchamber. He was endeavouring to pierce the darkness with his ferret\neyes, when the chimes of a neighbouring church struck the four quarters.\nSo he listened for the hour.\n\nTo his great astonishment, the heavy bell went on from six to seven, and\nfrom seven to eight, and regularly up to twelve; then stopped. Twelve!\nIt was past two when he went to bed. The clock was wrong. An icicle must\nhave got into the works. Twelve!\n\nHe touched the spring of his repeater, to correct this most preposterous\nclock. Its rapid little pulse beat twelve, and stopped.\n\n'Why, it isn't possible,' said Scrooge, 'that I can have slept through a\nwhole day and far into another night. It isn't possible that anything\nhas happened to the sun, and this is twelve at noon!'\n\nThe idea being an alarming one, he scrambled out of bed, and groped his\nway to the window. He was obliged to rub the frost off with the sleeve\nof his dressing-gown before he could see anything; and could see very\nlittle then. All he could make out was, that it was still very foggy and\nextremely cold, and that there was no noise of people running to and\nfro, and making a great stir, as there unquestionably would have been if\nnight had beaten off bright day, and taken possession of the world. This\nwas a great relief, because 'Three days after sight of this First of\nExchange pay to Mr. Ebenezer Scrooge or his order,' and so forth, would\nhave become a mere United States security if there were no days to count\nby.\n\nScrooge went to bed again, and thought, and thought, and thought it over\nand over, and could make nothing of it. The more he thought, the more\nperplexed he was; and, the more he endeavoured not to think, the more he\nthought.\n\nMarley's Ghost bothered him exceedingly. Every time he resolved within\nhimself, after mature inquiry that it was all a dream, his mind flew\nback again, like a strong spring released, to its first position, and\npresented the same problem to be worked all through, 'Was it a dream or\nnot?'\n\nScrooge lay in this state until the chime had gone three-quarters more,\nwhen he remembered, on a sudden, that the Ghost had warned him of a\nvisitation when the bell tolled one. He resolved to lie awake until the\nhour was passed; and, considering that he could no more go to sleep than\ngo to heaven, this was, perhaps, the wisest resolution in his power.\n\nThe quarter was so long, that he was more than once convinced he must\nhave sunk into a doze unconsciously, and missed the clock. At length it\nbroke upon his listening ear.\n\n'Ding, dong!'\n\n'A quarter past,' said Scrooge, counting.\n\n'Ding, dong!'\n\n'Half past,' said Scrooge.\n\n'Ding, dong!'\n\n'A quarter to it.' said Scrooge.\n\n'Ding, dong!'\n\n'The hour itself,' said Scrooge triumphantly, 'and nothing else!'\n\nHe spoke before the hour bell sounded, which it now did with a deep,\ndull, hollow, melancholy ONE. Light flashed up in the room upon the\ninstant, and the curtains of his bed were drawn.\n\nThe curtains of his bed were drawn aside, I tell you, by a hand. Not\nthe curtains at his feet, nor the curtains at his back, but those to\nwhich his face was addressed. The curtains of his bed were drawn aside;\nand Scrooge, starting up into a half-recumbent attitude, found himself\nface to face with the unearthly visitor who drew them: as close to it as\nI am now to you, and I am standing in the spirit at your elbow.\n\nIt was a strange figure--like a child; yet not so like a child as like\nan old man, viewed through some supernatural medium, which gave him the\nappearance of having receded from the view, and being diminished to a\nchild's proportions. Its hair, which hung about its neck and down its\nback, was white, as if with age; and yet the face had not a wrinkle in\nit, and the tenderest bloom was on the skin. The arms were very long and\nmuscular; the hands the same, as if its hold were of uncommon strength.\nIts legs and feet, most delicately formed, were, like those upper\nmembers, bare. It wore a tunic of the purest white; and round its waist\nwas bound a lustrous belt, the sheen of which was beautiful. It held a\nbranch of fresh green holly in its hand; and, in singular contradiction\nof that wintry emblem, had its dress trimmed with summer flowers. But\nthe strangest thing about it was, that from the crown of its head there\nsprang a bright clear jet of light, by which all this was visible; and\nwhich was doubtless the occasion of its using, in its duller moments, a\ngreat extinguisher for a cap, which it now held under its arm.\n\nEven this, though, when Scrooge looked at it with increasing steadiness,\nwas _not_ its strangest quality. For, as its belt sparkled and\nglittered, now in one part and now in another, and what was light one\ninstant"}
16:34:31,399 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,401 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,401 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27564, Requested 6968. Please try again in 9.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,402 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'and thither in\n    restless haste and moaning as\n    they went                                                         32\n\n  Then old Fezziwig stood out to\n    dance with Mrs. Fezziwig                                          54\n\n  A flushed and boisterous group                                      62\n\n  Laden with Christmas toys and\n    presents                                                          64\n\n  The way he went after that plump\n    sister in the lace tucker!                                       100\n\n  "How are you?" said one.\n    "How are you?" returned the other.\n   "Well!" said the first. "Old\n    Scratch has got his own at last,\n    hey?"                                                            114\n\n  "What do you call this?" said Joe.\n    "Bed-curtains!" "Ah!" returned\n    the woman, laughing....\n    "Bed-curtains!"\n\n  "You don\'t mean to say you took\n    \'em down, rings and all, with him\n    lying there?" said Joe.\n\n  "Yes, I do," replied the woman.\n    "Why not?"                                                       120\n\n  "It\'s I, your uncle Scrooge. I have\n    come to dinner. Will you let\n    me in, Fred?"                                                    144\n\n  "Now, I\'ll tell you what, my friend,"\n    said Scrooge. "I am not going\n    to stand this sort of thing any\n    longer."                                                         146\n\n[Illustration]\n\n_IN BLACK AND WHITE_\n\n\n  Tailpiece                                                           vi\n  Tailpiece to List of Coloured Illustrations                          x\n  Tailpiece to List of Black and White Illustrations                  xi\n  Heading to Stave One                                                 3\n  They were portly gentlemen, pleasant to behold                      12\n  On the wings of the wind                                         28-29\n  Tailpiece to Stave One                                              34\n  Heading to Stave Two                                                37\n  He produced a decanter of curiously\n  light wine and a block of curiously heavy cake                      50\n  She left him, and they parted                                       60\n  Tailpiece to Stave Two                                              65\n  Heading to Stave Three                                              69\n  There was nothing very cheerful in the climate                      75\n  He had been Tim\'s blood-horse all the way from church            84-85\n  With the pudding                                                    88\n  Heading to Stave Four                                              111\n  Heading to Stave Five                                              137\n  Tailpiece to Stave Five                                            147\n\n[Illustration]\n\n\nSTAVE ONE\n\n\n[Illustration]\n\n\n\n\nMARLEY\'S GHOST\n\n\nMarley was dead, to begin with. There is no doubt whatever about that.\nThe register of his burial was signed by the clergyman, the clerk, the\nundertaker, and the chief mourner. Scrooge signed it. And Scrooge\'s name\nwas good upon \'Change for anything he chose to put his hand to. Old\nMarley was as dead as a door-nail.\n\nMind! I don\'t mean to say that I know of my own knowledge, what there is\nparticularly dead about a door-nail. I might have been inclined, myself,\nto regard a coffin-nail as the deadest piece of ironmongery in the\ntrade. But the wisdom of our ancestors is in the simile; and my\nunhallowed hands shall not disturb it, or the country\'s done for. You\nwill, therefore, permit me to repeat, emphatically, that Marley was as\ndead as a door-nail.\n\nScrooge knew he was dead? Of course he did. How could it be otherwise?\nScrooge and he were partners for I don\'t know how many years. Scrooge\nwas his sole executor, his sole administrator, his sole assign, his sole\nresiduary legatee, his sole friend, and sole mourner. And even Scrooge\nwas not so dreadfully cut up by the sad event but that he was an\nexcellent man of business on the very day of the funeral, and solemnised\nit with an undoubted bargain.\n\nThe mention of Marley\'s funeral brings me back to the point I started\nfrom. There is no doubt that Marley was dead. This must be distinctly\nunderstood, or nothing wonderful can come of the story I am going to\nrelate. If we were not perfectly convinced that Hamlet\'s father died\nbefore the play began, there would be nothing more remarkable in his\ntaking a stroll at night, in an easterly wind, upon his own ramparts,\nthan there would be in any other middle-aged gentleman rashly turning\nout after dark in a breezy spot--say St. Paul\'s Churchyard, for\ninstance--literally to astonish his son\'s weak mind.\n\nScrooge never painted out Old Marley\'s name. There it stood, years\nafterwards, above the warehouse door: Scrooge and Marley. The firm was\nknown as Scrooge and Marley. Sometimes people new to the business called\nScrooge Scrooge, and sometimes Marley, but he answered to both names. It\nwas all the same to him.\n\nOh! but he was a tight-fisted hand at the grindstone, Scrooge! a\nsqueezing, wrenching, grasping, scraping, clutching, covetous old\nsinner! Hard and sharp as flint, from which no steel had ever struck out\ngenerous fire; secret, and self-contained, and solitary as an oyster.\nThe cold within him froze his old features, nipped his pointed nose,\nshrivelled his cheek, stiffened his gait; made his'}
16:34:31,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,414 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,415 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27559, Requested 6779. Please try again in 8.676s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,416 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "-fisted hand at the grindstone, Scrooge! a\nsqueezing, wrenching, grasping, scraping, clutching, covetous old\nsinner! Hard and sharp as flint, from which no steel had ever struck out\ngenerous fire; secret, and self-contained, and solitary as an oyster.\nThe cold within him froze his old features, nipped his pointed nose,\nshrivelled his cheek, stiffened his gait; made his eyes red, his thin\nlips blue; and spoke out shrewdly in his grating voice. A frosty rime\nwas on his head, and on his eyebrows, and his wiry chin. He carried his\nown low temperature always about with him; he iced his office in the\ndog-days, and didn't thaw it one degree at Christmas.\n\nExternal heat and cold had little influence on Scrooge. No warmth could\nwarm, no wintry weather chill him. No wind that blew was bitterer than\nhe, no falling snow was more intent upon its purpose, no pelting rain\nless open to entreaty. Foul weather didn't know where to have him. The\nheaviest rain, and snow, and hail, and sleet could boast of the\nadvantage over him in only one respect. They often 'came down'\nhandsomely, and Scrooge never did.\n\nNobody ever stopped him in the street to say, with gladsome looks, 'My\ndear Scrooge, how are you? When will you come to see me?' No beggars\nimplored him to bestow a trifle, no children asked him what it was\no'clock, no man or woman ever once in all his life inquired the way to\nsuch and such a place, of Scrooge. Even the blind men's dogs appeared to\nknow him; and, when they saw him coming on, would tug their owners into\ndoorways and up courts; and then would wag their tails as though they\nsaid, 'No eye at all is better than an evil eye, dark master!'\n\nBut what did Scrooge care? It was the very thing he liked. To edge his\nway along the crowded paths of life, warning all human sympathy to keep\nits distance, was what the knowing ones call 'nuts' to Scrooge.\n\nOnce upon a time--of all the good days in the year, on Christmas\nEve--old Scrooge sat busy in his counting-house. It was cold, bleak,\nbiting weather; foggy withal; and he could hear the people in the court\noutside go wheezing up and down, beating their hands upon their breasts,\nand stamping their feet upon the pavement stones to warm them. The City\nclocks had only just gone three, but it was quite dark already--it had\nnot been light all day--and candles were flaring in the windows of the\nneighbouring offices, like ruddy smears upon the palpable brown air. The\nfog came pouring in at every chink and keyhole, and was so dense\nwithout, that, although the court was of the narrowest, the houses\nopposite were mere phantoms. To see the dingy cloud come drooping down,\nobscuring everything, one might have thought that nature lived hard by,\nand was brewing on a large scale.\n\nThe door of Scrooge's counting-house was open, that he might keep his\neye upon his clerk, who in a dismal little cell beyond, a sort of tank,\nwas copying letters. Scrooge had a very small fire, but the clerk's fire\nwas so very much smaller that it looked like one coal. But he couldn't\nreplenish it, for Scrooge kept the coal-box in his own room; and so\nsurely as the clerk came in with the shovel, the master predicted that\nit would be necessary for them to part. Wherefore the clerk put on his\nwhite comforter, and tried to warm himself at the candle; in which\neffort, not being a man of strong imagination, he failed.\n\n'A merry Christmas, uncle! God save you!' cried a cheerful voice. It was\nthe voice of Scrooge's nephew, who came upon him so quickly that this\nwas the first intimation he had of his approach.\n\n'Bah!' said Scrooge. 'Humbug!'\n\nHe had so heated himself with rapid walking in the fog and frost, this\nnephew of Scrooge's, that he was all in a glow; his face was ruddy and\nhandsome; his eyes sparkled, and his breath smoked again.\n\n'Christmas a humbug, uncle!' said Scrooge's nephew. 'You don't mean\nthat, I am sure?'\n\n'I do,' said Scrooge. 'Merry Christmas! What right have you to be merry?\nWhat reason have you to be merry? You're poor enough.'\n\n'Come, then,' returned the nephew gaily. 'What right have you to be\ndismal? What reason have you to be morose? You're rich enough.'\n\nScrooge, having no better answer ready on the spur of the moment, said,\n'Bah!' again; and followed it up with 'Humbug!'\n\n'Don't be cross, uncle!' said the nephew.\n\n'What else can I be,' returned the uncle, 'when I live in such a world\nof fools as this? Merry Christmas! Out upon merry Christmas! What's\nChristmas-time to you but a time for paying bills without money; a time\nfor finding yourself a year older, and not an hour richer; a time for\nbalancing your books"}
16:34:31,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,500 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,569 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,570 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,571 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27475, Requested 6810. Please try again in 8.57s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,573 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "nothing on which it is so hard as poverty; and there is nothing it\nprofesses to condemn with such severity as the pursuit of wealth!'\n\n'You fear the world too much,' she answered gently. 'All your other\nhopes have merged into the hope of being beyond the chance of its sordid\nreproach. I have seen your nobler aspirations fall off one by one, until\nthe master passion, Gain, engrosses you. Have I not?'\n\n'What then?' he retorted. 'Even if I have grown so much wiser, what\nthen? I am not changed towards you.'\n\nShe shook her head.\n\n'Am I?'\n\n'Our contract is an old one. It was made when we were both poor, and\ncontent to be so, until, in good season, we could improve our worldly\nfortune by our patient industry. You _are_ changed. When it was made you\nwere another man.'\n\n'I was a boy,' he said impatiently.\n\n'Your own feeling tells you that you were not what you are,' she\nreturned. 'I am. That which promised happiness when we were one in heart\nis fraught with misery now that we are two. How often and how keenly I\nhave thought of this I will not say. It is enough that I _have_ thought\nof it, and can release you.'\n\n'Have I ever sought release?'\n\n'In words. No. Never.'\n\n'In what, then?'\n\n'In a changed nature; in an altered spirit; in another atmosphere of\nlife; another Hope as its great end. In everything that made my love of\nany worth or value in your sight. If this had never been between us,'\nsaid the girl, looking mildly, but with steadiness, upon him; 'tell me,\nwould you seek me out and try to win me now? Ah, no!'\n\nHe seemed to yield to the justice of this supposition in spite of\nhimself. But he said, with a struggle, 'You think not.'\n\n'I would gladly think otherwise if I could,' she answered. 'Heaven\nknows! When _I_ have learned a Truth like this, I know how strong and\nirresistible it must be. But if you were free to-day, to-morrow,\nyesterday, can even I believe that you would choose a dowerless\ngirl--you who, in your very confidence with her, weigh everything by\nGain: or, choosing her, if for a moment you were false enough to your\none guiding principle to do so, do I not know that your repentance and\nregret would surely follow? I do; and I release you. With a full heart,\nfor the love of him you once were.'\n\n[Illustration: SHE LEFT HIM, AND THEY PARTED]\n\nHe was about to speak; but, with her head turned from him, she resumed:\n\n'You may--the memory of what is past half makes me hope you will--have\npain in this. A very, very brief time, and you will dismiss the\nrecollection of it gladly, as an unprofitable dream, from which it\nhappened well that you awoke. May you be happy in the life you have\nchosen!'\n\nShe left him, and they parted.\n\n'Spirit!' said Scrooge, 'show me no more! Conduct me home. Why do you\ndelight to torture me?'\n\n'One shadow more!' exclaimed the Ghost.\n\n'No more!' cried Scrooge. 'No more! I don't wish to see it. Show me no\nmore!'\n\nBut the relentless Ghost pinioned him in both his arms, and forced him\nto observe what happened next.\n\nThey were in another scene and place; a room, not very large or\nhandsome, but full of comfort. Near to the winter fire sat a beautiful\nyoung girl, so like that last that Scrooge believed it was the same,\nuntil he saw _her_, now a comely matron, sitting opposite her daughter.\nThe noise in this room was perfectly tumultuous, for there were more\nchildren there than Scrooge in his agitated state of mind could count;\nand, unlike the celebrated herd in the poem, they were not forty\nchildren conducting themselves like one, but every child was conducting\nitself like forty. The consequences were uproarious beyond belief; but\nno one seemed to care; on the contrary, the mother and daughter laughed\nheartily, and enjoyed it very much; and the latter, soon beginning to\nmingle in the sports, got pillaged by the young brigands most\nruthlessly. What would I not have given to be one of them! Though I\nnever could have been so rude, no, no! I wouldn't for the wealth of all\nthe world have crushed that braided hair, and torn it down; and for the\nprecious little shoe, I wouldn't have plucked it off, God bless my soul!\nto save my life. As to measuring her waist in sport, as they did, bold\nyoung brood, I couldn't have done it; I should have expected my arm to\nhave grown round it for a punishment, and never come straight again. And\nyet I should have dearly liked, I own, to have touched her lips; to have\nquestioned her, that she might have opened them; to have looked upon the\nlashes of her downcast eyes, and never raised a blush; to have let loose\nwaves of hair, an inch of which would be a keepsake beyond price: in\nshort, I should have liked, I do confess, to have had the lightest\nlicense of a child, and yet to have been man enough to know its"}
16:34:31,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,608 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,747 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,747 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27402, Requested 6779. Please try again in 8.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,748 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '\ufeffThe Project Gutenberg eBook of A Christmas Carol\n    \nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n\nTitle: A Christmas Carol\n\nAuthor: Charles Dickens\n\nIllustrator: Arthur Rackham\n\nRelease date: December 24, 2007 [eBook #24022]\n\nLanguage: English\n\nOriginal publication: Philadelphia and New York: J. B. Lippincott Company,, 1915\n\nCredits: Produced by Suzanne Shell, Janet Blenkinship and the Online\n        Distributed Proofreading Team at http://www.pgdp.net\n\n\n*** START OF THE PROJECT GUTENBERG EBOOK A CHRISTMAS CAROL ***\n\n\n\n\nProduced by Suzanne Shell, Janet Blenkinship and the Online\nDistributed Proofreading Team at http://www.pgdp.net\n\n\n\n\n\n\n\n\n\n\n\n  A CHRISTMAS CAROL\n\n  [Illustration: _"How now?" said Scrooge, caustic and cold as ever.\n  "What do you want with me?"_]\n\n\n  A CHRISTMAS CAROL\n\n  [Illustration]\n\n  BY\n\n  CHARLES DICKENS\n\n  [Illustration]\n\n  ILLUSTRATED BY ARTHUR RACKHAM\n\n  [Illustration]\n\n  J. B. LIPPINCOTT COMPANY PHILADELPHIA AND NEW YORK\n\n  FIRST PUBLISHED 1915\n\n  REPRINTED 1923, 1927, 1932, 1933, 1934, 1935, 1947, 1948, 1952, 1958,\n  1962, 1964, 1966, 1967, 1969, 1971, 1972, 1973\n\n  ISBN: 0-397-00033-2\n\n  PRINTED IN GREAT BRITAIN\n\n\n\n\n  PREFACE\n\n  I have endeavoured in this Ghostly little book to raise the Ghost of an\n  Idea which shall not put my readers out of humour with themselves, with\n  each other, with the season, or with me. May it haunt their house\n  pleasantly, and no one wish to lay it.\n\n  Their faithful Friend and Servant,\n\n  C. D.\n\n  _December, 1843._\n\n\n\n\n  CHARACTERS\n\n  Bob Cratchit, clerk to Ebenezer Scrooge.\n  Peter Cratchit, a son of the preceding.\n  Tim Cratchit ("Tiny Tim"), a cripple, youngest son of Bob Cratchit.\n  Mr. Fezziwig, a kind-hearted, jovial old merchant.\n  Fred, Scrooge\'s nephew.\n  Ghost of Christmas Past, a phantom showing things past.\n  Ghost of Christmas Present, a spirit of a kind, generous,\n    and hearty nature.\n  Ghost of Christmas Yet to Come, an apparition showing the shadows\n    of things which yet may happen.\n  Ghost of Jacob Marley, a spectre of Scrooge\'s former partner in business.\n  Joe, a marine-store dealer and receiver of stolen goods.\n  Ebenezer Scrooge, a grasping, covetous old man, the surviving partner\n    of the firm of Scrooge and Marley.\n  Mr. Topper, a bachelor.\n  Dick Wilkins, a fellow apprentice of Scrooge\'s.\n\n  Belle, a comely matron, an old sweetheart of Scrooge\'s.\n  Caroline, wife of one of Scrooge\'s debtors.\n  Mrs. Cratchit, wife of Bob Cratchit.\n  Belinda and Martha Cratchit, daughters of the preceding.\n\n  Mrs. Dilber, a laundress.\n  Fan, the sister of Scrooge.\n  Mrs. Fezziwig, the worthy partner of Mr. Fezziwig.\n\n\n\n\n  CONTENTS\n\n  STAVE ONE--MARLEY\'S GHOST                                             3\n  STAVE TWO--THE FIRST OF THE THREE SPIRITS                            37\n  STAVE THREE--THE SECOND OF THE THREE SPIRITS                         69\n  STAVE FOUR--THE LAST OF THE SPIRITS                                 111\n  STAVE FIVE--THE END OF IT                                           137\n\n\n  LIST OF ILLUSTRATIONS\n\n  _IN COLOUR_\n\n\n  "How now?" said Scrooge, caustic\n    and cold as ever. "What do you\n    want with me?"                                           _Frontispiece_\n\n  Bob Cratchit went down a slide on\n    Cornhill, at the end of a lane of\n    boys, twenty times, in honour of\n    its being Christmas Eve                                           16\n\n  Nobody under the bed; nobody in\n    the closet; nobody in his dressing-gown,\n    which was hanging up\n    in a suspicious attitude against\n    the wall                                                          20\n\n  The air was filled with phantoms,\n   wandering hither and thither in\n    restless haste and moaning as\n    they went                                                         32\n\n  Then old Fezziwig stood out to\n    dance with Mrs. Fezziwig                                          54\n\n  A flushed and boisterous group                                      62\n\n  Laden with Christmas toys and\n    presents                                                          64\n\n  The way he went after that plump\n    sister in the lace tucker!                                       100\n\n  "How are you?" said one.'}
16:34:31,760 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,762 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,762 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,868 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,869 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,870 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27339, Requested 6834. Please try again in 8.346s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,871 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "would\nbe untrue. But he put his hand upon the key he had relinquished, turned\nit sturdily, walked in, and lighted his candle.\n\nHe _did_ pause, with a moment's irresolution, before he shut the door;\nand he _did_ look cautiously behind it first, as if he half expected to\nbe terrified with the sight of Marley's pigtail sticking out into the\nhall. But there was nothing on the back of the door, except the screws\nand nuts that held the knocker on, so he said, 'Pooh, pooh!' and closed\nit with a bang.\n\nThe sound resounded through the house like thunder. Every room above,\nand every cask in the wine-merchant's cellars below, appeared to have a\nseparate peal of echoes of its own. Scrooge was not a man to be\nfrightened by echoes. He fastened the door, and walked across the hall,\nand up the stairs: slowly, too: trimming his candle as he went.\n\nYou may talk vaguely about driving a coach and six up a good old flight\nof stairs, or through a bad young Act of Parliament; but I mean to say\nyou might have got a hearse up that staircase, and taken it broadwise,\nwith the splinter-bar towards the wall, and the door towards the\nbalustrades: and done it easy. There was plenty of width for that, and\nroom to spare; which is perhaps the reason why Scrooge thought he saw a\nlocomotive hearse going on before him in the gloom. Half-a-dozen\ngas-lamps out of the street wouldn't have lighted the entry too well, so\nyou may suppose that it was pretty dark with Scrooge's dip.\n\nUp Scrooge went, not caring a button for that. Darkness is cheap, and\nScrooge liked it. But, before he shut his heavy door, he walked through\nhis rooms to see that all was right. He had just enough recollection of\nthe face to desire to do that.\n\nSitting-room, bedroom, lumber-room. All as they should be. Nobody under\nthe table, nobody under the sofa; a small fire in the grate; spoon and\nbasin ready; and the little saucepan of gruel (Scrooge had a cold in his\nhead) upon the hob. Nobody under the bed; nobody in the closet; nobody\nin his dressing-gown, which was hanging up in a suspicious attitude\nagainst the wall. Lumber-room as usual. Old fire-guard, old shoes, two\nfish baskets, washing-stand on three legs, and a poker.\n\n[Illustration: _Nobody under the bed; nobody in the closet; nobody in\nhis dressing-gown, which was hanging up in a suspicious attitude against\nthe wall_]\n\nQuite satisfied, he closed his door, and locked himself in; double\nlocked himself in, which was not his custom. Thus secured against\nsurprise, he took off his cravat; put on his dressing-gown and slippers,\nand his nightcap; and sat down before the fire to take his gruel.\n\nIt was a very low fire indeed; nothing on such a bitter night. He was\nobliged to sit close to it, and brood over it, before he could extract\nthe least sensation of warmth from such a handful of fuel. The fireplace\nwas an old one, built by some Dutch merchant long ago, and paved all\nround with quaint Dutch tiles, designed to illustrate the Scriptures.\nThere were Cains and Abels, Pharaoh's daughters, Queens of Sheba,\nAngelic messengers descending through the air on clouds like\nfeather-beds, Abrahams, Belshazzars, Apostles putting off to sea in\nbutter-boats, hundreds of figures to attract his thoughts; and yet that\nface of Marley, seven years dead, came like the ancient Prophet's rod,\nand swallowed up the whole. If each smooth tile had been a blank at\nfirst, with power to shape some picture on its surface from the\ndisjointed fragments of his thoughts, there would have been a copy of\nold Marley's head on every one.\n\n'Humbug!' said Scrooge; and walked across the room.\n\nAfter several turns he sat down again. As he threw his head back in the\nchair, his glance happened to rest upon a bell, a disused bell, that\nhung in the room, and communicated, for some purpose now forgotten, with\na chamber in the highest storey of the building. It was with great\nastonishment, and with a strange, inexplicable dread, that, as he\nlooked, he saw this bell begin to swing. It swung so softly in the\noutset that it scarcely made a sound; but soon it rang out loudly, and\nso did every bell in the house.\n\nThis might have lasted half a minute, or a minute, but it seemed an\nhour. The bells ceased, as they had begun, together. They were succeeded\nby a clanking noise deep down below as if some person were dragging a\nheavy chain over the casks in the wine-merchant's cellar. Scrooge then\nremembered to have heard that ghosts in haunted houses were described as\ndragging chains.\n\nThe cellar door flew open with a booming sound, and then he heard the\nnoise much louder on the floors below; then coming up the stairs; then\ncoming straight towards his door.\n\n'It's humbug still!' said Scrooge. 'I won't believe it.'\n\nHis colour changed, though, when, without a pause, it came on through\nthe heavy door and passed into the"}
16:34:31,881 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,882 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,882 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27322, Requested 6849. Please try again in 8.342s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,883 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "scales descending on the counter made a merry sound, or that\nthe twine and roller parted company so briskly, or that the canisters\nwere rattled up and down like juggling tricks, or even that the blended\nscents of tea and coffee were so grateful to the nose, or even that the\nraisins were so plentiful and rare, the almonds so extremely white, the\nsticks of cinnamon so long and straight, the other spices so delicious,\nthe candied fruits so caked and spotted with molten sugar as to make the\ncoldest lookers-on feel faint, and subsequently bilious. Nor was it that\nthe figs were moist and pulpy, or that the French plums blushed in\nmodest tartness from their highly-decorated boxes, or that everything\nwas good to eat and in its Christmas dress; but the customers were all\nso hurried and so eager in the hopeful promise of the day, that they\ntumbled up against each other at the door, crashing their wicker baskets\nwildly, and left their purchases upon the counter, and came running\nback to fetch them, and committed hundreds of the like mistakes, in the\nbest humour possible; while the grocer and his people were so frank and\nfresh, that the polished hearts with which they fastened their aprons\nbehind might have been their own, worn outside for general inspection,\nand for Christmas daws to peck at if they chose.\n\nBut soon the steeples called good people all to church and chapel, and\naway they came, flocking through the streets in their best clothes and\nwith their gayest faces. And at the same time there emerged, from scores\nof by-streets, lanes, and nameless turnings, innumerable people,\ncarrying their dinners to the bakers' shops. The sight of these poor\nrevellers appeared to interest the Spirit very much, for he stood with\nScrooge beside him in a baker's doorway, and, taking off the covers as\ntheir bearers passed, sprinkled incense on their dinners from his torch.\nAnd it was a very uncommon kind of torch, for once or twice, when there\nwere angry words between some dinner-carriers who had jostled each\nother, he shed a few drops of water on them from it, and their\ngood-humour was restored directly. For they said, it was a shame to\nquarrel upon Christmas Day. And so it was! God love it, so it was!\n\nIn time the bells ceased, and the bakers were shut up; and yet there was\na genial shadowing forth of all these dinners, and the progress of their\ncooking, in the thawed blotch of wet above each baker's oven, where the\npavement smoked as if its stones were cooking too.\n\n'Is there a peculiar flavour in what you sprinkle from your torch?'\nasked Scrooge.\n\n'There is. My own.'\n\n'Would it apply to any kind of dinner on this day?' asked Scrooge.\n\n'To any kindly given. To a poor one most.'\n\n'Why to a poor one most?' asked Scrooge.\n\n'Because it needs it most.'\n\n'Spirit!' said Scrooge, after a moment's thought, 'I wonder you, of all\nthe beings in the many worlds about us, should desire to cramp these\npeople's opportunities of innocent enjoyment.\n\n'I!' cried the Spirit.\n\n'You would deprive them of their means of dining every seventh day,\noften the only day on which they can be said to dine at all,' said\nScrooge; 'wouldn't you?'\n\n'I!' cried the Spirit.\n\n'You seek to close these places on the Seventh Day,' said Scrooge. 'And\nit comes to the same thing.'\n\n'I seek!' exclaimed the Spirit.\n\n'Forgive me if I am wrong. It has been done in your name, or at least in\nthat of your family,' said Scrooge.\n\n'There are some upon this earth of yours,' returned the Spirit, 'who\nlay claim to know us, and who do their deeds of passion, pride,\nill-will, hatred, envy, bigotry, and selfishness in our name, who are as\nstrange to us, and all our kith and kin, as if they had never lived.\nRemember that, and charge their doings on themselves, not us.'\n\nScrooge promised that he would; and they went on, invisible, as they had\nbeen before, into the suburbs of the town. It was a remarkable quality\nof the Ghost (which Scrooge had observed at the baker's), that\nnotwithstanding his gigantic size, he could accommodate himself to any\nplace with ease; and that he stood beneath a low roof quite as\ngracefully and like a supernatural creature as it was possible he could\nhave done in any lofty hall.\n\nAnd perhaps it was the pleasure the good Spirit had in showing off this\npower of his, or else it was his own kind, generous, hearty nature, and\nhis sympathy with all poor men, that led him straight to Scrooge's\nclerk's; for there he went, and took Scrooge with him, holding to his\nrobe; and on the threshold of the door the Spirit smiled, and stopped to\nbless Bob Cratchit's dwelling with the sprinklings of his torch. Think\nof that! Bob had but fifteen 'Bob' a week himself; he pocketed on\nSaturdays but fifteen copies of his Christian name; and yet the Ghost of\nChristmas Present blessed his four-roomed house!\n\nThen up rose Mrs. Cratchit, Cratchit's wife, dressed out but poorly in a\ntw"}
16:34:31,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,907 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,102 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,102 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27207, Requested 6816. Please try again in 8.046s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,103 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "tell\nme if Tiny Tim will live.'\n\n'I see a vacant seat,' replied the Ghost, 'in the poor chimney corner,\nand a crutch without an owner, carefully preserved. If these shadows\nremain unaltered by the Future, the child will die.'\n\n'No, no,' said Scrooge. 'Oh no, kind Spirit! say he will be spared.'\n\n'If these shadows remain unaltered by the Future none other of my race,'\nreturned the Ghost, 'will find him here. What then? If he be like to\ndie, he had better do it, and decrease the surplus population.'\n\nScrooge hung his head to hear his own words quoted by the Spirit, and\nwas overcome with penitence and grief.\n\n'Man,' said the Ghost, 'if man you be in heart, not adamant, forbear\nthat wicked cant until you have discovered what the surplus is, and\nwhere it is. Will you decide what men shall live, what men shall die? It\nmay be that, in the sight of Heaven, you are more worthless and less fit\nto live than millions like this poor man's child. O God! to hear the\ninsect on the leaf pronouncing on the too much life among his hungry\nbrothers in the dust!'\n\nScrooge bent before the Ghost's rebuke, and, trembling, cast his eyes\nupon the ground. But he raised them speedily on hearing his own name.\n\n'Mr. Scrooge!' said Bob. 'I'll give you Mr. Scrooge, the Founder of the\nFeast!'\n\n'The Founder of the Feast, indeed!' cried Mrs. Cratchit, reddening. 'I\nwish I had him here. I'd give him a piece of my mind to feast upon, and\nI hope he'd have a good appetite for it.'\n\n'My dear,' said Bob, 'the children! Christmas Day.'\n\n'It should be Christmas Day, I am sure,' said she, 'on which one drinks\nthe health of such an odious, stingy, hard, unfeeling man as Mr.\nScrooge. You know he is, Robert! Nobody knows it better than you do,\npoor fellow!'\n\n'My dear!' was Bob's mild answer. 'Christmas Day.'\n\n'I'll drink his health for your sake and the Day's,' said Mrs. Cratchit,\n'not for his. Long life to him! A merry Christmas and a happy New Year!\nHe'll be very merry and very happy, I have no doubt!'\n\nThe children drank the toast after her. It was the first of their\nproceedings which had no heartiness in it. Tiny Tim drank it last of\nall, but he didn't care twopence for it. Scrooge was the Ogre of the\nfamily. The mention of his name cast a dark shadow on the party, which\nwas not dispelled for full five minutes.\n\nAfter it had passed away they were ten times merrier than before, from\nthe mere relief of Scrooge the Baleful being done with. Bob Cratchit\ntold them how he had a situation in his eye for Master Peter, which\nwould bring in, if obtained, full five-and-sixpence weekly. The two\nyoung Cratchits laughed tremendously at the idea of Peter's being a man\nof business; and Peter himself looked thoughtfully at the fire from\nbetween his collars, as if he were deliberating what particular\ninvestments he should favour when he came into the receipt of that\nbewildering income. Martha, who was a poor apprentice at a milliner's,\nthen told them what kind of work she had to do, and how many hours she\nworked at a stretch and how she meant to lie abed to-morrow morning for\na good long rest; to-morrow being a holiday she passed at home. Also how\nshe had seen a countess and a lord some days before, and how the lord\n'was much about as tall as Peter'; at which Peter pulled up his collar\nso high that you couldn't have seen his head if you had been there. All\nthis time the chestnuts and the jug went round and round; and by-and-by\nthey had a song, about a lost child travelling in the snow, from Tiny\nTim, who had a plaintive little voice, and sang it very well indeed.\n\nThere was nothing of high mark in this. They were not a handsome family;\nthey were not well dressed; their shoes were far from being waterproof;\ntheir clothes were scanty; and Peter might have known, and very likely\ndid, the inside of a pawnbroker's. But they were happy, grateful,\npleased with one another, and contented with the time; and when they\nfaded, and looked happier yet in the bright sprinklings of the Spirit's\ntorch at parting, Scrooge had his eye upon them, and especially on Tiny\nTim, until the last.\n\nBy this time it was getting dark, and snowing pretty heavily; and as\nScrooge and the Spirit went along the streets, the brightness of the\nroaring fires in kitchens, parlours, and all sorts of rooms was\nwonderful. Here, the flickering of the blaze showed preparations for a\ncosy dinner, with hot plates baking through and through before the fire,\nand deep red curtains, ready to be drawn to shut out cold and darkness.\nThere, all the children of the house were running out into the snow to\nmeet their married sisters, brothers, cousins, uncles, aunts, and be the\nfirst to greet them. Here, again, were shadows on the window-blinds of\nguests assembling; and there a group of"}
16:34:32,121 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,122 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,188 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,188 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27166, Requested 6753. Please try again in 7.837s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,188 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "in. At sight of an old gentleman in a Welsh wig, sitting\nbehind such a high desk, that if he had been two inches taller, he must\nhave knocked his head against the ceiling, Scrooge cried in great\nexcitement--\n\n'Why, it's old Fezziwig! Bless his heart, it's Fezziwig alive again!'\n\nOld Fezziwig laid down his pen, and looked up at the clock, which\npointed to the hour of seven. He rubbed his hands; adjusted his\ncapacious waistcoat; laughed all over himself, from his shoes to his\norgan of benevolence; and called out, in a comfortable, oily, rich, fat,\njovial voice--\n\n'Yo ho, there! Ebenezer! Dick!'\n\nScrooge's former self, now grown a young man, came briskly in,\naccompanied by his fellow-'prentice.\n\n'Dick Wilkins, to be sure!' said Scrooge to the Ghost. 'Bless me, yes.\nThere he is. He was very much attached to me, was Dick. Poor Dick! Dear,\ndear!'\n\n'Yo ho, my boys!' said Fezziwig. 'No more work to-night. Christmas Eve,\nDick. Christmas, Ebenezer! Let's have the shutters up,' cried old\nFezziwig, with a sharp clap of his hands, 'before a man can say Jack\nRobinson!'\n\nYou wouldn't believe how those two fellows went at it! They charged into\nthe street with the shutters--one, two, three--had 'em up in their\nplaces--four, five, six--barred 'em and pinned 'em--seven, eight,\nnine--and came back before you could have got to twelve, panting like\nracehorses.\n\n'Hilli-ho!' cried old Fezziwig, skipping down from the high desk with\nwonderful agility. 'Clear away, my lads, and let's have lots of room\nhere! Hilli-ho, Dick! Chirrup, Ebenezer!'\n\nClear away! There was nothing they wouldn't have cleared away, or\ncouldn't have cleared away, with old Fezziwig looking on. It was done in\na minute. Every movable was packed off, as if it were dismissed from\npublic life for evermore; the floor was swept and watered, the lamps\nwere trimmed, fuel was heaped upon the fire; and the warehouse was as\nsnug, and warm, and dry, and bright a ball-room as you would desire to\nsee upon a winter's night.\n\nIn came a fiddler with a music-book, and went up to the lofty desk, and\nmade an orchestra of it, and tuned like fifty stomach-aches. In came\nMrs. Fezziwig, one vast substantial smile. In came the three Miss\nFezziwigs, beaming and lovable. In came the six young followers whose\nhearts they broke. In came all the young men and women employed in the\nbusiness. In came the housemaid, with her cousin the baker. In came the\ncook with her brother's particular friend the milkman. In came the boy\nfrom over the way, who was suspected of not having board enough from his\nmaster; trying to hide himself behind the girl from next door but one,\nwho was proved to have had her ears pulled by her mistress. In they all\ncame, one after another; some shyly, some boldly, some gracefully, some\nawkwardly, some pushing, some pulling; in they all came, any how and\nevery how. Away they all went, twenty couple at once; hands half round\nand back again the other way; down the middle and up again; round and\nround in various stages of affectionate grouping; old top couple always\nturning up in the wrong place; new top couple starting off again as soon\nas they got there; all top couples at last, and not a bottom one to help\nthem! When this result was brought about, old Fezziwig, clapping his\nhands to stop the dance, cried out, 'Well done!' and the fiddler plunged\nhis hot face into a pot of porter, especially provided for that purpose.\nBut, scorning rest upon his reappearance, he instantly began again,\nthough there were no dancers yet, as if the other fiddler had been\ncarried home, exhausted, on a shutter, and he were a bran-new man\nresolved to beat him out of sight, or perish.\n\n[Illustration: _Then old Fezziwig stood out to dance with Mrs.\nFezziwig_]\n\nThere were more dances, and there were forfeits, and more dances, and\nthere was cake, and there was negus, and there was a great piece of Cold\nRoast, and there was a great piece of Cold Boiled, and there were\nmince-pies, and plenty of beer. But the great effect of the evening came\nafter the Roast and Boiled, when the fiddler (an artful dog, mind! The\nsort of man who knew his business better than you or I could have told\nit him!) struck up 'Sir Roger de Coverley.' Then old Fezziwig stood\nout to dance with Mrs. Fezziwig. Top couple, too; with a good stiff\npiece of work cut out for them; three or four and twenty pair of\npartners; people who were not to be trifled with; people who would\ndance, and had no notion of walking.\n\nBut if they had been twice as many--ah!"}
16:34:32,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,259 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,269 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,347 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,348 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27100, Requested 6842. Please try again in 7.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,349 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "that he turned\nuncomfortably cold when he began to wonder which of his curtains this\nnew spectre would draw back, he put them every one aside with his own\nhands, and, lying down again, established a sharp look-out all round the\nbed. For he wished to challenge the Spirit on the moment of its\nappearance, and did not wish to be taken by surprise and made nervous.\n\nGentlemen of the free-and-easy sort, who plume themselves on being\nacquainted with a move or two, and being usually equal to the time of\nday, express the wide range of their capacity for adventure by observing\nthat they are good for anything from pitch-and-toss to manslaughter;\nbetween which opposite extremes, no doubt, there lies a tolerably wide\nand comprehensive range of subjects. Without venturing for Scrooge quite\nas hardily as this, I don't mind calling on you to believe that he was\nready for a good broad field of strange appearances, and that nothing\nbetween a baby and a rhinoceros would have astonished him very much.\n\nNow, being prepared for almost anything, he was not by any means\nprepared for nothing; and consequently, when the bell struck One, and no\nshape appeared, he was taken with a violent fit of trembling. Five\nminutes, ten minutes, a quarter of an hour went by, yet nothing came.\nAll this time he lay upon his bed, the very core and centre of a blaze\nof ruddy light, which streamed upon it when the clock proclaimed the\nhour; and which, being only light, was more alarming than a dozen\nghosts, as he was powerless to make out what it meant, or would be at;\nand was sometimes apprehensive that he might be at that very moment an\ninteresting case of spontaneous combustion, without having the\nconsolation of knowing it. At last, however, he began to think--as you\nor I would have thought at first; for it is always the person not in the\npredicament who knows what ought to have been done in it, and would\nunquestionably have done it too--at last, I say, he began to think that\nthe source and secret of this ghostly light might be in the adjoining\nroom, from whence, on further tracing it, it seemed to shine. This idea\ntaking full possession of his mind, he got up softly, and shuffled in\nhis slippers to the door.\n\nThe moment Scrooge's hand was on the lock a strange voice called him by\nhis name, and bade him enter. He obeyed.\n\nIt was his own room. There was no doubt about that. But it had undergone\na surprising transformation. The walls and ceiling were so hung with\nliving green, that it looked a perfect grove; from every part of which\nbright gleaming berries glistened. The crisp leaves of holly, mistletoe,\nand ivy reflected back the light, as if so many little mirrors had been\nscattered there; and such a mighty blaze went roaring up the chimney as\nthat dull petrification of a hearth had never known in Scrooge's time,\nor Marley's, or for many and many a winter season gone. Heaped up on the\nfloor, to form a kind of throne, were turkeys, geese, game, poultry,\nbrawn, great joints of meat, sucking-pigs, long wreaths of sausages,\nmince-pies, plum-puddings, barrels of oysters, red-hot chestnuts,\ncherry-cheeked apples, juicy oranges, luscious pears, immense\ntwelfth-cakes, and seething bowls of punch, that made the chamber dim\nwith their delicious steam. In easy state upon this couch there sat a\njolly Giant, glorious to see; who bore a glowing torch, in shape not\nunlike Plenty's horn, and held it up, high up, to shed its light on\nScrooge as he came peeping round the door.\n\n'Come in!' exclaimed the Ghost. 'Come in! and know me better, man!'\n\nScrooge entered timidly, and hung his head before this Spirit. He was\nnot the dogged Scrooge he had been; and though the Spirit's eyes were\nclear and kind, he did not like to meet them.\n\n'I am the Ghost of Christmas Present,' said the Spirit. 'Look upon me!'\n\nScrooge reverently did so. It was clothed in one simple deep green robe,\nor mantle, bordered with white fur. This garment hung so loosely on the\nfigure, that its capacious breast was bare, as if disdaining to be\nwarded or concealed by any artifice. Its feet, observable beneath the\nample folds of the garment, were also bare; and on its head it wore no\nother covering than a holly wreath, set here and there with shining\nicicles. Its dark-brown curls were long and free; free as its genial\nface, its sparkling eye, its open hand, its cheery voice, its\nunconstrained demeanour, and its joyful air. Girded round its middle was\nan antique scabbard: but no sword was in it, and the ancient sheath was\neaten up with rust.\n\n'You have never seen the like of me before!' exclaimed the Spirit.\n\n'Never,' Scrooge made answer to it.\n\n'Have never walked forth with the younger members of my family; meaning\n(for I am very young) my elder brothers born in these later years?'\npursued the Phantom.\n\n'I don't think I have,' said Scrooge. 'I am afraid I have not. Have"}
16:34:32,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,382 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,382 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27083, Requested 6809. Please try again in 7.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,383 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "ation]\n\nSuch a bustle ensued that you might have thought a goose the rarest of\nall birds; a feathered phenomenon, to which a black swan was a matter of\ncourse--and, in truth, it was something very like it in that house. Mrs.\nCratchit made the gravy (ready beforehand in a little saucepan) hissing\nhot; Master Peter mashed the potatoes with incredible vigour; Miss\nBelinda sweetened up the apple sauce; Martha dusted the hot plates; Bob\ntook Tiny Tim beside him in a tiny corner at the table; the two young\nCratchits set chairs for everybody, not forgetting themselves, and,\nmounting guard upon their posts, crammed spoons into their mouths, lest\nthey should shriek for goose before their turn came to be helped. At\nlast the dishes were set on, and grace was said. It was succeeded by a\nbreathless pause, as Mrs. Cratchit, looking slowly all along the\ncarving-knife, prepared to plunge it in the breast; but when she did,\nand when the long-expected gush of stuffing issued forth, one murmur of\ndelight arose all round the board, and even Tiny Tim, excited by the two\nyoung Cratchits, beat on the table with the handle of his knife and\nfeebly cried Hurrah!\n\n[Illustration: HE HAD BEEN TIM'S BLOOD-HORSE ALL THE WAY FROM CHURCH]\n\nThere never was such a goose. Bob said he didn't believe there ever was\nsuch a goose cooked. Its tenderness and flavour, size and cheapness,\nwere the themes of universal admiration. Eked out by apple sauce and\nmashed potatoes, it was a sufficient dinner for the whole family;\nindeed, as Mrs. Cratchit said with great delight (surveying one small\natom of a bone upon the dish), they hadn't ate it all at last! Yet every\none had had enough, and the youngest Cratchits, in particular, were\nsteeped in sage and onion to the eyebrows! But now, the plates being\nchanged by Miss Belinda, Mrs. Cratchit left the room alone--too nervous\nto bear witnesses--to take the pudding up, and bring it in.\n\nSuppose it should not be done enough! Suppose it should break in turning\nout! Suppose somebody should have got over the wall of the back-yard and\nstolen it, while they were merry with the goose--a supposition at which\nthe two young Cratchits became livid! All sorts of horrors were\nsupposed.\n\nHallo! A great deal of steam! The pudding was out of the copper. A smell\nlike a washing-day! That was the cloth. A smell like an eating-house and\na pastry-cook's next door to each other, with a laundress's next door to\nthat! That was the pudding! In half a minute Mrs. Cratchit\nentered--flushed, but smiling proudly--with the pudding, like a speckled\ncannon-ball, so hard and firm, blazing in half of half-a-quartern of\nignited brandy, and bedight with Christmas holly stuck into the top.\n\nOh, a wonderful pudding! Bob Cratchit said, and calmly too, that he\nregarded it as the greatest success achieved by Mrs. Cratchit since\ntheir marriage. Mrs. Cratchit said that, now the weight was off her\nmind, she would confess she had her doubts about the quantity of flour.\nEverybody had something to say about it, but nobody said or thought it\nwas at all a small pudding for a large family. It would have been flat\nheresy to do so. Any Cratchit would have blushed to hint at such a\nthing.\n\n[Illustration: WITH THE PUDDING]\n\nAt last the dinner was all done, the cloth was cleared, the hearth\nswept, and the fire made up. The compound in the jug being tasted and\nconsidered perfect, apples and oranges were put upon the table, and a\nshovel full of chestnuts on the fire. Then all the Cratchit family\ndrew round the hearth in what Bob Cratchit called a circle, meaning half\na one; and at Bob Cratchit's elbow stood the family display of glass.\nTwo tumblers and a custard cup without a handle.\n\nThese held the hot stuff from the jug, however, as well as golden\ngoblets would have done; and Bob served it out with beaming looks, while\nthe chestnuts on the fire sputtered and cracked noisily. Then Bob\nproposed:\n\n'A merry Christmas to us all, my dears. God bless us!'\n\nWhich all the family re-echoed.\n\n'God bless us every one!' said Tiny Tim, the last of all.\n\nHe sat very close to his father's side, upon his little stool. Bob held\nhis withered little hand to his, as if he loved the child, and wished to\nkeep him by his side, and dreaded that he might be taken from him.\n\n'Spirit,' said Scrooge, with an interest he had never felt before, 'tell\nme if Tiny Tim will live.'\n\n'I see a vacant seat,' replied the Ghost, 'in the poor chimney corner,\nand a crutch without an owner, carefully preserved. If these shadows\nremain unaltered by the Future, the child will die.'\n\n'No, no,' said Scrooge. 'Oh no, kind Spirit! say he will be spared.'\n\n'If these shadows remain unaltered by the Future none other of my race,'\nreturned the Ghost, '"}
16:34:32,440 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,440 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,493 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,493 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27018, Requested 6843. Please try again in 7.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,494 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "have no doubt his liberality is well represented by his surviving\npartner,' said the gentleman, presenting his credentials.\n\n[Illustration: THEY WERE PORTLY GENTLEMEN, PLEASANT TO BEHOLD]\n\nIt certainly was; for they had been two kindred spirits. At the ominous\nword 'liberality' Scrooge frowned, and shook his head, and handed the\ncredentials back.\n\n'At this festive season of the year, Mr. Scrooge,' said the gentleman,\ntaking up a pen, 'it is more than usually desirable that we should make\nsome slight provision for the poor and destitute, who suffer greatly at\nthe present time. Many thousands are in want of common necessaries;\nhundreds of thousands are in want of common comforts, sir.'\n\n'Are there no prisons?' asked Scrooge.\n\n'Plenty of prisons,' said the gentleman, laying down the pen again.\n\n'And the Union workhouses?' demanded Scrooge. 'Are they still in\noperation?'\n\n'They are. Still,' returned the gentleman, 'I wish I could say they were\nnot.'\n\n'The Treadmill and the Poor Law are in full vigour, then?' said Scrooge.\n\n'Both very busy, sir.'\n\n'Oh! I was afraid, from what you said at first, that something had\noccurred to stop them in their useful course,' said Scrooge. 'I am very\nglad to hear it.'\n\n'Under the impression that they scarcely furnish Christian cheer of mind\nor body to the multitude,' returned the gentleman, 'a few of us are\nendeavouring to raise a fund to buy the Poor some meat and drink, and\nmeans of warmth. We choose this time, because it is a time, of all\nothers, when Want is keenly felt, and Abundance rejoices. What shall I\nput you down for?'\n\n'Nothing!' Scrooge replied.\n\n'You wish to be anonymous?'\n\n'I wish to be left alone,' said Scrooge. 'Since you ask me what I wish,\ngentlemen, that is my answer. I don't make merry myself at Christmas,\nand I can't afford to make idle people merry. I help to support the\nestablishments I have mentioned--they cost enough: and those who are\nbadly off must go there.'\n\n'Many can't go there; and many would rather die.'\n\n'If they would rather die,' said Scrooge, 'they had better do it, and\ndecrease the surplus population. Besides--excuse me--I don't know that.'\n\n'But you might know it,' observed the gentleman.\n\n'It's not my business,' Scrooge returned. 'It's enough for a man to\nunderstand his own business, and not to interfere with other people's.\nMine occupies me constantly. Good afternoon, gentlemen!'\n\nSeeing clearly that it would be useless to pursue their point, the\ngentlemen withdrew. Scrooge resumed his labours with an improved opinion\nof himself, and in a more facetious temper than was usual with him.\n\nMeanwhile the fog and darkness thickened so, that people ran about with\nflaring links, proffering their services to go before horses in\ncarriages, and conduct them on their way. The ancient tower of a church,\nwhose gruff old bell was always peeping slyly down at Scrooge out of a\nGothic window in the wall, became invisible, and struck the hours and\nquarters in the clouds, with tremulous vibrations afterwards, as if its\nteeth were chattering in its frozen head up there. The cold became\nintense. In the main street, at the corner of the court, some labourers\nwere repairing the gas-pipes, and had lighted a great fire in a brazier,\nround which a party of ragged men and boys were gathered: warming their\nhands and winking their eyes before the blaze in rapture. The water-plug\nbeing left in solitude, its overflowings suddenly congealed, and turned\nto misanthropic ice. The brightness of the shops, where holly sprigs and\nberries crackled in the lamp heat of the windows, made pale faces ruddy\nas they passed. Poulterers' and grocers' trades became a splendid joke:\na glorious pageant, with which it was next to impossible to believe that\nsuch dull principles as bargain and sale had anything to do. The Lord\nMayor, in the stronghold of the mighty Mansion House, gave orders to his\nfifty cooks and butlers to keep Christmas as a Lord Mayor's household\nshould; and even the little tailor, whom he had fined five shillings on\nthe previous Monday for being drunk and bloodthirsty in the streets,\nstirred up to-morrow's pudding in his garret, while his lean wife and\nthe baby sallied out to buy the beef.\n\nFoggier yet, and colder! Piercing, searching, biting cold. If the good\nSt. Dunstan had but nipped the Evil Spirit's nose with a touch of such\nweather as that, instead of using his familiar weapons, then indeed he\nwould have roared to lusty purpose. The owner of one scant young nose,\ngnawed and mumbled by the hungry cold as bones are gnawed by dogs,\nstooped down at Scrooge's keyhole to regale him with a Christmas carol;\nbut, at the first sound of\n\n  'God bless you, merry gentleman,\n  May nothing you dismay!'\n\nScrooge seized the ruler with such energy of action that the singer fled\nin terror, leaving the keyhole to the fog, and even more congenial\nfrost."}
16:34:32,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,505 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,505 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27014, Requested 6837. Please try again in 7.701s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,506 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "in it, and the ancient sheath was\neaten up with rust.\n\n'You have never seen the like of me before!' exclaimed the Spirit.\n\n'Never,' Scrooge made answer to it.\n\n'Have never walked forth with the younger members of my family; meaning\n(for I am very young) my elder brothers born in these later years?'\npursued the Phantom.\n\n'I don't think I have,' said Scrooge. 'I am afraid I have not. Have you\nhad many brothers, Spirit?'\n\n'More than eighteen hundred,' said the Ghost.\n\n'A tremendous family to provide for,' muttered Scrooge.\n\nThe Ghost of Christmas Present rose.\n\n'Spirit,' said Scrooge submissively, 'conduct me where you will. I went\nforth last night on compulsion, and I learned a lesson which is working\nnow. To-night if you have aught to teach me, let me profit by it.'\n\n'Touch my robe!'\n\nScrooge did as he was told, and held it fast.\n\nHolly, mistletoe, red berries, ivy, turkeys, geese, game, poultry,\nbrawn, meat, pigs, sausages, oysters, pies, puddings, fruit, and punch,\nall vanished instantly. So did the room, the fire, the ruddy glow, the\nhour of night, and they stood in the city streets on Christmas morning,\nwhere (for the weather was severe) the people made a rough, but brisk\nand not unpleasant kind of music, in scraping the snow from the pavement\nin front of their dwellings, and from the tops of their houses, whence\nit was mad delight to the boys to see it come plumping down into the\nroad below, and splitting into artificial little snowstorms.\n\nThe house-fronts looked black enough, and the windows blacker,\ncontrasting with the smooth white sheet of snow upon the roofs, and with\nthe dirtier snow upon the ground; which last deposit had been ploughed\nup in deep furrows by the heavy wheels of carts and waggons: furrows\nthat crossed and recrossed each other hundreds of times where the great\nstreets branched off; and made intricate channels, hard to trace in the\nthick yellow mud and icy water. The sky was gloomy, and the shortest\nstreets were choked up with a dingy mist, half thawed, half frozen,\nwhose heavier particles descended in a shower of sooty atoms, as if all\nthe chimneys in Great Britain had, by one consent, caught fire, and were\nblazing away to their dear heart's content. There was nothing very\ncheerful in the climate or the town, and yet was there an air of\ncheerfulness abroad that the clearest summer air and brightest summer\nsun might have endeavoured to diffuse in vain.\n\n[Illustration: THERE WAS NOTHING VERY CHEERFUL IN THE CLIMATE]\n\nFor the people who were shovelling away on the house-tops were jovial\nand full of glee; calling out to one another from the parapets, and now\nand then exchanging a facetious snowball--better-natured missile far\nthan many a wordy jest--laughing heartily if it went right, and not less\nheartily if it went wrong. The poulterers' shops were still half open,\nand the fruiterers' were radiant in their glory. There were great,\nround, pot-bellied baskets of chestnuts, shaped like the waistcoats of\njolly old gentlemen, lolling at the doors, and tumbling out into the\nstreet in their apoplectic opulence: There were ruddy, brown-faced,\nbroad-girthed Spanish onions, shining in the fatness of their growth\nlike Spanish friars, and winking from their shelves in wanton slyness at\nthe girls as they went by, and glanced demurely at the hung-up\nmistletoe. There were pears and apples clustered high in blooming\npyramids; there were bunches of grapes, made, in the shopkeepers'\nbenevolence, to dangle from conspicuous hooks that people's mouths might\nwater gratis as they passed; there were piles of filberts, mossy and\nbrown, recalling, in their fragrance, ancient walks among the woods, and\npleasant shufflings ankle deep through withered leaves; there were\nNorfolk Biffins, squab and swarthy, setting off the yellow of the\noranges and lemons, and, in the great compactness of their juicy\npersons, urgently entreating and beseeching to be carried home in paper\nbags and eaten after dinner. The very gold and silver fish, set forth\namong these choice fruits in a bowl, though members of a dull and\nstagnant-blooded race, appeared to know that there was something going\non; and, to a fish, went gasping round and round their little world in\nslow and passionless excitement.\n\nThe Grocers'! oh, the Grocers'! nearly closed, with perhaps two shutters\ndown, or one; but through those gaps such glimpses! It was not alone\nthat the scales descending on the counter made a merry sound, or that\nthe twine and roller parted company so briskly, or that the canisters\nwere rattled up and down like juggling tricks, or even that the blended\nscents of tea and coffee were so grateful to the nose, or even that the\nraisins were so plentiful and rare, the almonds so extremely white, the\nsticks of cinnamon so long and straight, the other spices so delicious,\nthe candied fruits so"}
16:34:32,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,513 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,513 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27013, Requested 6816. Please try again in 7.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,514 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "and on the threshold of the door the Spirit smiled, and stopped to\nbless Bob Cratchit's dwelling with the sprinklings of his torch. Think\nof that! Bob had but fifteen 'Bob' a week himself; he pocketed on\nSaturdays but fifteen copies of his Christian name; and yet the Ghost of\nChristmas Present blessed his four-roomed house!\n\nThen up rose Mrs. Cratchit, Cratchit's wife, dressed out but poorly in a\ntwice-turned gown, but brave in ribbons, which are cheap, and make a\ngoodly show for sixpence; and she laid the cloth, assisted by Belinda\nCratchit, second of her daughters, also brave in ribbons; while Master\nPeter Cratchit plunged a fork into the saucepan of potatoes, and getting\nthe corners of his monstrous shirt-collar (Bob's private property,\nconferred upon his son and heir in honour of the day,) into his mouth,\nrejoiced to find himself so gallantly attired, and yearned to show his\nlinen in the fashionable Parks. And now two smaller Cratchits, boy and\ngirl, came tearing in, screaming that outside the baker's they had smelt\nthe goose, and known it for their own; and basking in luxurious thoughts\nof sage and onion, these young Cratchits danced about the table, and\nexalted Master Peter Cratchit to the skies, while he (not proud,\nalthough his collars nearly choked him) blew the fire, until the slow\npotatoes, bubbling up, knocked loudly at the saucepan-lid to be let out\nand peeled.\n\n'What has ever got your precious father, then?' said Mrs. Cratchit. 'And\nyour brother, Tiny Tim? And Martha warn't as late last Christmas Day by\nhalf an hour!'\n\n'Here's Martha, mother!' said a girl, appearing as she spoke.\n\n'Here's Martha, mother!' cried the two young Cratchits. 'Hurrah! There's\n_such_ a goose, Martha!'\n\n'Why, bless your heart alive, my dear, how late you are!' said Mrs.\nCratchit, kissing her a dozen times, and taking off her shawl and bonnet\nfor her with officious zeal.\n\n'We'd a deal of work to finish up last night,' replied the girl, 'and\nhad to clear away this morning, mother!'\n\n'Well! never mind so long as you are come,' said Mrs. Cratchit. 'Sit ye\ndown before the fire, my dear, and have a warm, Lord bless ye!'\n\n'No, no! There's father coming,' cried the two young Cratchits, who were\neverywhere at once. 'Hide, Martha, hide!'\n\nSo Martha hid herself, and in came little Bob, the father, with at least\nthree feet of comforter, exclusive of the fringe, hanging down before\nhim, and his threadbare clothes darned up and brushed to look\nseasonable, and Tiny Tim upon his shoulder. Alas for Tiny Tim, he bore a\nlittle crutch, and had his limbs supported by an iron frame!\n\n'Why, where's our Martha?' cried Bob Cratchit, looking round.\n\n'Not coming,' said Mrs. Cratchit.\n\n'Not coming!' said Bob, with a sudden declension in his high spirits;\nfor he had been Tim's blood-horse all the way from church, and had come\nhome rampant. 'Not coming upon Christmas Day!'\n\nMartha didn't like to see him disappointed, if it were only in joke; so\nshe came out prematurely from behind the closet door, and ran into his\narms, while the two young Cratchits hustled Tiny Tim, and bore him off\ninto the wash-house, that he might hear the pudding singing in the\ncopper.\n\n'And how did little Tim behave?' asked Mrs. Cratchit when she had\nrallied Bob on his credulity, and Bob had hugged his daughter to his\nheart's content.\n\n'As good as gold,' said Bob, 'and better. Somehow, he gets thoughtful,\nsitting by himself so much, and thinks the strangest things you ever\nheard. He told me, coming home, that he hoped the people saw him in the\nchurch, because he was a cripple, and it might be pleasant to them to\nremember upon Christmas Day who made lame beggars walk and blind men\nsee.'\n\nBob's voice was tremulous when he told them this, and trembled more when\nhe said that Tiny Tim was growing strong and hearty.\n\nHis active little crutch was heard upon the floor, and back came Tiny\nTim before another word was spoken, escorted by his brother and\nsister to his stool beside the fire; and while Bob, turning up his\ncuffs--as if, poor fellow, they were capable of being made more\nshabby--compounded some hot mixture in a jug with gin and lemons, and\nstirred it round and round, and put it on the hob to simmer, Master\nPeter and the two ubiquitous young Cratchits went to fetch the goose,\nwith which they soon returned in high procession.\n\n[Illustration]\n\nSuch a bustle ensued that you might have thought a goose the rarest of\nall birds; a feathered phenomenon, to which a black swan was a matter of\ncourse--and, in truth, it was something very like it in that house. Mrs.\nCratchit made the gravy (ready beforehand in a little saucepan) hissing\nhot; Master Peter mashed the potatoes with incredible vigour; Miss\nBelinda sweetened up the apple sauce; Martha dust"}
16:34:32,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,519 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,636 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,638 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,717 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,721 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,721 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,722 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,812 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,812 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26856, Requested 6842. Please try again in 7.396s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,814 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "liked, I own, to have touched her lips; to have\nquestioned her, that she might have opened them; to have looked upon the\nlashes of her downcast eyes, and never raised a blush; to have let loose\nwaves of hair, an inch of which would be a keepsake beyond price: in\nshort, I should have liked, I do confess, to have had the lightest\nlicense of a child, and yet to have been man enough to know its value.\n\n[Illustration: _A flushed and boisterous group_]\n\nBut now a knocking at the door was heard, and such a rush immediately\nensued that she, with laughing face and plundered dress, was borne\ntowards it the centre of a flushed and boisterous group, just in time to\ngreet the father, who came home attended by a man laden with Christmas\ntoys and presents. Then the shouting and the struggling, and the\nonslaught that was made on the defenceless porter! The scaling him, with\nchairs for ladders, to dive into his pockets, despoil him of\nbrown-paper parcels, hold on tight by his cravat, hug him round his\nneck, pummel his back, and kick his legs in irrepressible affection! The\nshouts of wonder and delight with which the development of every package\nwas received! The terrible announcement that the baby had been taken in\nthe act of putting a doll's frying pan into his mouth, and was more than\nsuspected of having swallowed a fictitious turkey, glued on a wooden\nplatter! The immense relief of finding this a false alarm! The joy, and\ngratitude, and ecstasy! They are all indescribable alike. It is enough\nthat, by degrees, the children and their emotions got out of the\nparlour, and, by one stair at a time, up to the top of the house, where\nthey went to bed, and so subsided.\n\nAnd now Scrooge looked on more attentively than ever, when the master of\nthe house, having his daughter leaning fondly on him, sat down with her\nand her mother at his own fireside; and when he thought that such\nanother creature, quite as graceful and as full of promise, might have\ncalled him father, and been a spring-time in the haggard winter of his\nlife, his sight grew very dim indeed.\n\n'Belle,' said the husband, turning to his wife with a smile, 'I saw an\nold friend of yours this afternoon.'\n\n'Who was it?'\n\n'Guess!'\n\n'How can I? Tut, don't I know?' she added in the same breath, laughing\nas he laughed. 'Mr. Scrooge.'\n\n'Mr. Scrooge it was. I passed his office window; and as it was not shut\nup, and he had a candle inside, I could scarcely help seeing him. His\npartner lies upon the point of death, I hear; and there he sat alone.\nQuite alone in the world, I do believe.'\n\n'Spirit!' said Scrooge in a broken voice, 'remove me from this place.'\n\n'I told you these were shadows of the things that have been,' said the\nGhost. 'That they are what they are do not blame me!'\n\n'Remove me!' Scrooge exclaimed, 'I cannot bear it!'\n\nHe turned upon the Ghost, and seeing that it looked upon him with a\nface, in which in some strange way there were fragments of all the faces\nit had shown him, wrestled with it.\n\n'Leave me! Take me back. Haunt me no longer!'\n\nIn the struggle, if that can be called a struggle in which the Ghost\nwith no visible resistance on its own part was undisturbed by any effort\nof its adversary, Scrooge observed that its light was burning high and\nbright; and dimly connecting that with its influence over him, he seized\nthe extinguisher-cap, and by a sudden action pressed it down upon its\nhead.\n\n[Illustration: _Laden with Christmas toys and presents_]\n\nThe Spirit dropped beneath it, so that the extinguisher covered its\nwhole form; but though Scrooge pressed it down with all his force, he\ncould not hide the light, which streamed from under it, in an unbroken\nflood upon the ground.\n\nHe was conscious of being exhausted, and overcome by an irresistible\ndrowsiness; and, further, of being in his own bedroom. He gave the cap a\nparting squeeze, in which his hand relaxed; and had barely time to reel\nto bed, before he sank into a heavy sleep.\n\n[Illustration]\n\n\nSTAVE THREE\n\n\n[Illustration]\n\n\n\n\nTHE SECOND OF THE THREE SPIRITS\n\n\nAwaking in the middle of a prodigiously tough snore, and sitting up in\nbed to get his thoughts together, Scrooge had no occasion to be told\nthat the bell was again upon the stroke of One. He felt that he was\nrestored to consciousness in the right nick of time, for the especial\npurpose of holding a conference with the second messenger despatched to\nhim through Jacob Marley's intervention. But finding that he turned\nuncomfortably cold when he began to wonder which of his curtains this\nnew spectre would draw back, he put them every one aside with his own\nhands, and, lying down again, established a sharp look-out all round the\nbed. For he wished to challenge the Spirit on the moment of its\nappearance, and did not wish to be taken by surprise and made nervous.\n\nGentlemen of the free-and-easy sort, who plume themselves on being"}
16:34:32,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,837 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,838 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,839 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,854 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,942 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,137 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,313 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,316 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,502 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,826 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:33,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.501272916000744. input_tokens=2936, output_tokens=408
16:34:34,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,100 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,177 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,178 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,254 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,298 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,372 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,398 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,401 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,478 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,481 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,682 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,684 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,830 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,86 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,156 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,549 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,550 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,575 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,787 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,881 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,883 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,217 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,322 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,431 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,532 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,762 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,837 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,20 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,119 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,124 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,286 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,289 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,384 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,485 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,938 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,268 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,361 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:38,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 24.612589666998247. input_tokens=2937, output_tokens=494
16:34:38,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,368 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,415 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,544 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,697 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:39,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:39,504 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:39,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:39,943 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,125 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,560 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,561 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,897 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,898 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,899 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,231 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,233 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,802 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,808 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,854 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,943 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,267 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,270 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,448 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,494 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,648 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,650 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,791 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,975 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,437 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,446 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,449 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:43,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 8 retries took 16.82636666700273. input_tokens=2935, output_tokens=570
16:34:43,609 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,610 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,802 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:44,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:44,658 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:45,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:45,518 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:47,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:47,281 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:48,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:48,243 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:48,849 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:48,851 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:49,533 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:49,533 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,32 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,34 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,426 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,427 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,428 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 28787, Requested 6831. Please try again in 11.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:50,429 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "intently at the Spirit's robe, 'but I see something strange, and not\nbelonging to yourself, protruding from your skirts. Is it a foot or a\nclaw?'\n\n'It might be a claw, for the flesh there is upon it,' was the Spirit's\nsorrowful reply. 'Look here!'\n\nFrom the foldings of its robe it brought two children, wretched, abject,\nfrightful, hideous, miserable. They knelt down at its feet, and clung\nupon the outside of its garment.\n\n'O Man! look here! Look, look down here!' exclaimed the Ghost.\n\nThey were a boy and girl. Yellow, meagre, ragged, scowling, wolfish, but\nprostrate, too, in their humility. Where graceful youth should have\nfilled their features out, and touched them with its freshest tints, a\nstale and shrivelled hand, like that of age, had pinched and twisted\nthem, and pulled them into shreds. Where angels might have sat\nenthroned, devils lurked, and glared out menacing. No change, no\ndegradation, no perversion of humanity in any grade, through all the\nmysteries of wonderful creation, has monsters half so horrible and\ndread.\n\nScrooge started back, appalled. Having them shown to him in this way, he\ntried to say they were fine children, but the words choked themselves,\nrather than be parties to a lie of such enormous magnitude.\n\n'Spirit! are they yours?' Scrooge could say no more.\n\n'They are Man's,' said the Spirit, looking down upon them. 'And they\ncling to me, appealing from their fathers. This boy is Ignorance. This\ngirl is Want. Beware of them both, and all of their degree, but most of\nall beware this boy, for on his brow I see that written which is Doom,\nunless the writing be erased. Deny it!' cried the Spirit, stretching out\nhis hand towards the city. 'Slander those who tell it ye! Admit it for\nyour factious purposes, and make it worse! And bide the end!'\n\n'Have they no refuge or resource?' cried Scrooge.\n\n'Are there no prisons?' said the Spirit, turning on him for the last\ntime with his own words. 'Are there no workhouses?'\n\nThe bell struck Twelve.\n\nScrooge looked about him for the Ghost, and saw it not. As the last\nstroke ceased to vibrate, he remembered the prediction of old Jacob\nMarley, and, lifting up his eyes, beheld a solemn Phantom, draped and\nhooded, coming like a mist along the ground towards him.\n\n\nSTAVE FOUR\n\n\n\n\nTHE LAST OF THE SPIRITS\n\n\nThe Phantom slowly, gravely, silently approached. When it came near him,\nScrooge bent down upon his knee; for in the very air through which this\nSpirit moved it seemed to scatter gloom and mystery.\n\nIt was shrouded in a deep black garment, which concealed its head, its\nface, its form, and left nothing of it visible, save one outstretched\nhand. But for this, it would have been difficult to detach its figure\nfrom the night, and separate it from the darkness by which it was\nsurrounded.\n\nHe felt that it was tall and stately when it came beside him, and that\nits mysterious presence filled him with a solemn dread. He knew no more,\nfor the Spirit neither spoke nor moved.\n\n'I am in the presence of the Ghost of Christmas Yet to Come?' said\nScrooge.\n\nThe Spirit answered not, but pointed onward with its hand.\n\n'You are about to show me shadows of the things that have not happened,\nbut will happen in the time before us,' Scrooge pursued. 'Is that so,\nSpirit?'\n\nThe upper portion of the garment was contracted for an instant in its\nfolds, as if the Spirit had inclined its head. That was the only answer\nhe received.\n\nAlthough well used to ghostly company by this time, Scrooge feared the\nsilent shape so much that his legs trembled beneath him, and he found\nthat he could hardly stand when he prepared to follow it. The Spirit\npaused a moment, as observing his condition, and giving him time to\nrecover.\n\nBut Scrooge was all the worse for this. It thrilled him with a vague,\nuncertain horror to know that, behind the dusky shroud, there were\nghostly eyes intently fixed upon him, while he, though he stretched his\nown to the utmost, could see nothing but a spectral hand and one great\nheap of black.\n\n'Ghost of the Future!' he exclaimed, 'I fear you more than any spectre\nI have seen. But as I know your purpose is to do me good, and as I hope\nto live to be another man from what I was, I am prepared to bear your\ncompany, and do it with a thankful heart. Will you not speak to me?'\n\nIt gave him no reply. The hand was pointed straight before them.\n\n'Lead on!' said Scrooge. 'Lead on! The night is waning fast, and it is\nprecious time to me, I know. Lead on, Spirit!'\n\nThe Phantom moved away as it had come towards him. Scrooge followed in\nthe shadow of its dress, which bore him up, he thought, and carried him\nalong.\n\nThey scarcely seemed to enter the City; for the City rather seemed to\nspring up about them, and encompass them of its own act. But there they\nwere in the heart of it; on 'Change, amongst the merchants,"}
16:34:50,550 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,551 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,787 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,864 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:50,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 9.853620374997263. input_tokens=34, output_tokens=314
16:34:51,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,83 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,136 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,286 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,288 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,325 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,356 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,598 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,994 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,164 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,166 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,189 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,191 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,699 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,855 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:53,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:53,406 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:53,407 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23806, Requested 6854. Please try again in 1.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:53,409 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'precious time to me, I know. Lead on, Spirit!\'\n\nThe Phantom moved away as it had come towards him. Scrooge followed in\nthe shadow of its dress, which bore him up, he thought, and carried him\nalong.\n\nThey scarcely seemed to enter the City; for the City rather seemed to\nspring up about them, and encompass them of its own act. But there they\nwere in the heart of it; on \'Change, amongst the merchants, who hurried\nup and down, and chinked the money in their pockets, and conversed in\ngroups, and looked at their watches, and trifled thoughtfully with their\ngreat gold seals, and so forth, as Scrooge had seen them often.\n\nThe Spirit stopped beside one little knot of business men. Observing\nthat the hand was pointed to them, Scrooge advanced to listen to their\ntalk.\n\n\'No,\' said a great fat man with a monstrous chin, \'I don\'t know much\nabout it either way. I only know he\'s dead.\'\n\n\'When did he die?\' inquired another.\n\n\'Last night, I believe.\'\n\n\'Why, what was the matter with him?\' asked a third, taking a vast\nquantity of snuff out of a very large snuff-box. \'I thought he\'d never\ndie.\'\n\n\'God knows,\' said the first, with a yawn.\n\n\'What has he done with his money?\' asked a red-faced gentleman with a\npendulous excrescence on the end of his nose, that shook like the gills\nof a turkey-cock.\n\n\'I haven\'t heard,\' said the man with the large chin, yawning again.\n\'Left it to his company, perhaps. He hasn\'t left it to _me_. That\'s all\nI know.\'\n\nThis pleasantry was received with a general laugh.\n\n\'It\'s likely to be a very cheap funeral,\' said the same speaker; \'for,\nupon my life, I don\'t know of anybody to go to it. Suppose we make up a\nparty, and volunteer?\'\n\n\'I don\'t mind going if a lunch is provided,\' observed the gentleman with\nthe excrescence on his nose. \'But I must be fed if I make one.\'\n\nAnother laugh.\n\n[Illustration:\n\n  _"How are you?" said one.\n   "How are you?" returned the other.\n   "Well!" said the first. "Old Scratch has got his own at last, hey?"_\n\n]\n\n\'Well, I am the most disinterested among you, after all,\' said the first\nspeaker, \'for I never wear black gloves, and I never eat lunch. But I\'ll\noffer to go if anybody else will. When I come to think of it, I\'m not\nat all sure that I wasn\'t his most particular friend; for we used to\nstop and speak whenever we met. Bye, bye!\'\n\nSpeakers and listeners strolled away, and mixed with other groups.\nScrooge knew the men, and looked towards the Spirit for an explanation.\n\nThe phantom glided on into a street. Its finger pointed to two persons\nmeeting. Scrooge listened again, thinking that the explanation might lie\nhere.\n\nHe knew these men, also, perfectly. They were men of business: very\nwealthy, and of great importance. He had made a point always of standing\nwell in their esteem in a business point of view, that is; strictly in a\nbusiness point of view.\n\n\'How are you?\' said one.\n\n\'How are you?\' returned the other.\n\n\'Well!\' said the first, \'old Scratch has got his own at last, hey?\'\n\n\'So I am told,\' returned the second. \'Cold, isn\'t it?\'\n\n\'Seasonable for Christmas-time. You are not a skater, I suppose?\'\n\n\'No, no. Something else to think of. Good-morning!\'\n\nNot another word. That was their meeting, their conversation, and their\nparting.\n\nScrooge was at first inclined to be surprised that the Spirit should\nattach importance to conversations apparently so trivial; but feeling\nassured that they must have some hidden purpose, he set himself to\nconsider what it was likely to be. They could scarcely be supposed to\nhave any bearing on the death of Jacob, his old partner, for that was\nPast, and this Ghost\'s province was the Future. Nor could he think of\nany one immediately connected with himself to whom he could apply them.\nBut nothing doubting that, to whomsoever they applied, they had some\nlatent moral for his own improvement, he resolved to treasure up every\nword he heard, and everything he saw; and especially to observe the\nshadow of himself when it appeared. For he had an expectation that the\nconduct of his future self would give him the clue he missed, and would\nrender the solution of these riddles easy.\n\nHe looked about in that very place for his own image, but another man\nstood in his accustomed corner; and though the clock pointed to his\nusual time of day for being there, he saw no likeness of himself among\nthe multitudes that poured in through the Porch. It gave him little\nsurprise, however; for he had been revolving in his mind a change of\nlife, and thought and hoped he saw his new-born resolutions carried out\nin this.\n\nQuiet and dark, beside him stood the Phantom, with its outstretched\nhand. When he roused himself from his thoughtful quest, he fancied,\nfrom the turn of the hand, and its situation in reference to himself,\nthat the Unseen Eyes were looking at him keenly. It made him shudder,\nand feel very cold.\n\nThey left the busy scene, and went into an obscure part of the town,\nwhere Scro'}
16:34:54,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:54,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 21.210672416000307. input_tokens=2790, output_tokens=435
16:34:55,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:55,143 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:56,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:56,330 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:56,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:56,808 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:59,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:59,201 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:59,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:59,536 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:59,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:59,869 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:00,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:00,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 6 retries took 11.778437624998332. input_tokens=2936, output_tokens=388
16:35:00,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:00,564 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:00,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:00,924 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,130 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,131 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,391 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,392 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,538 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,539 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,647 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,648 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,940 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,50 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,80 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,258 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,339 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,394 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,395 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,565 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,581 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,583 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,754 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:03,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:03,76 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:03,984 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:03,985 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:04,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:04,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 5 retries took 9.94266491700182. input_tokens=2936, output_tokens=275
16:35:05,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:05,997 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:06,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:06,697 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:09,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:09,570 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:10,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:10,272 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:11,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:11,77 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:11,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:11,805 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:11,885 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:11,887 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,197 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,317 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,318 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,379 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,436 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,475 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,476 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,647 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,650 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,700 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,702 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,738 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,960 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,962 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,6 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,48 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,602 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,604 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:14,827 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:14,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.223011250000127. input_tokens=34, output_tokens=296
16:35:20,202 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:20,204 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:20,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:20,412 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:20,653 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:20,655 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:20,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:20,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 4.243167958000413. input_tokens=34, output_tokens=89
16:35:22,278 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:22,279 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:22,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:22,613 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:22,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:22,692 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,20 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,133 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,135 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,136 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,547 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,548 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,549 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,550 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:23,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 23.63106274999882. input_tokens=34, output_tokens=470
16:35:23,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,996 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:24,34 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:24,37 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:24,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:24,92 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:24,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:24,792 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:26,49 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:26,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 4.244035874999099. input_tokens=34, output_tokens=91
16:35:30,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:30,542 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:31,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:31,213 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:32,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:32,636 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,41 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,475 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,581 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,581 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,814 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,905 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,61 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,333 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,342 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,422 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,424 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,432 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,517 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:35,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:35,125 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:36,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:36,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 8 retries took 5.53671629100063. input_tokens=34, output_tokens=142
16:35:42,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:42,66 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:42,66 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27658, Requested 7000. Please try again in 9.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:42,67 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '. \'A merrier Christmas,\nBob, my good fellow, than I have given you for many a year! I\'ll raise\nyour salary, and endeavour to assist your struggling family, and we will\ndiscuss your affairs this very afternoon, over a Christmas bowl of\nsmoking bishop, Bob! Make up the fires and buy another coal-scuttle\nbefore you dot another i, Bob Cratchit!\'\n\n[Illustration: _"Now, I\'ll tell you what, my friend," said Scrooge. "I\nam not going to stand this sort of thing any longer."_]\n\nScrooge was better than his word. He did it all, and infinitely more;\nand to Tiny Tim, who did NOT die, he was a second father. He became as\ngood a friend, as good a master, and as good a man as the good old\nCity knew, or any other good old city, town, or borough in the good old\nworld. Some people laughed to see the alteration in him, but he let them\nlaugh, and little heeded them; for he was wise enough to know that\nnothing ever happened on this globe, for good, at which some people did\nnot have their fill of laughter in the outset; and knowing that such as\nthese would be blind anyway, he thought it quite as well that they\nshould wrinkle up their eyes in grins as have the malady in less\nattractive forms. His own heart laughed, and that was quite enough for\nhim.\n\nHe had no further intercourse with Spirits, but lived upon the\nTotal-Abstinence Principle ever afterwards; and it was always said of\nhim that he knew how to keep Christmas well, if any man alive possessed\nthe knowledge. May that be truly said of us, and all of us! And so, as\nTiny Tim observed, God bless Us, Every One!\n\n[Illustration]\n\n+---------------------------------------------------------------+\n|Transcriber\'s note: The Contents were added by the transcriber.|\n+---------------------------------------------------------------+\n\n\n\n\n\n\n\n*** END OF THE PROJECT GUTENBERG EBOOK A CHRISTMAS CAROL ***\n\n\n    \n\nUpdated editions will replace the previous one—the old editions will\nbe renamed.\n\nCreating the works from print editions not protected by U.S. copyright\nlaw means that no one owns a United States copyright in these works,\nso the Foundation (and you!) can copy and distribute it in the United\nStates without permission and without paying copyright\nroyalties. Special rules, set forth in the General Terms of Use part\nof this license, apply to copying and distributing Project\nGutenberg™ electronic works to protect the PROJECT GUTENBERG™\nconcept and trademark. Project Gutenberg is a registered trademark,\nand may not be used if you charge for an eBook, except by following\nthe terms of the trademark license, including paying royalties for use\nof the Project Gutenberg trademark. If you do not charge anything for\ncopies of this eBook, complying with the trademark license is very\neasy. You may use this eBook for nearly any purpose such as creation\nof derivative works, reports, performances and research. Project\nGutenberg eBooks may be modified and printed and given away—you may\ndo practically ANYTHING in the United States with eBooks not protected\nby U.S. copyright law. Redistribution is subject to the trademark\nlicense, especially commercial redistribution.\n\n\nSTART: FULL LICENSE\n\nTHE FULL PROJECT GUTENBERG LICENSE\n\nPLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n\nTo protect the Project Gutenberg™ mission of promoting the free\ndistribution of electronic works, by using or distributing this work\n(or any other work associated in any way with the phrase “Project\nGutenberg”), you agree to comply with all the terms of the Full\nProject Gutenberg™ License available with this file or online at\nwww.gutenberg.org/license.\n\nSection 1. General Terms of Use and Redistributing Project Gutenberg™\nelectronic works\n\n1.A. By reading or using any part of this Project Gutenberg™\nelectronic work, you indicate that you have read, understand, agree to\nand accept all the terms of this license and intellectual property\n(trademark/copyright) agreement. If you do not agree to abide by all\nthe terms of this agreement, you must cease using and return or\ndestroy all copies of Project Gutenberg™ electronic works in your\npossession. If you paid a fee for obtaining a copy of or access to a\nProject Gutenberg™ electronic work and you do not agree to be bound\nby the terms of this agreement, you may obtain a refund from the person\nor entity to whom you paid the fee as set forth in paragraph 1.E.8.\n\n1.B. “Project Gutenberg” is a registered trademark. It may only be\nused on or associated in any way with an electronic work by people who\nagree to be bound by the terms of this agreement. There are a few\nthings that you can do with most Project Gutenberg™ electronic works\neven without complying with the full terms of this agreement. See\nparagraph 1.C below. There are a lot of things you can do with Project\nGutenberg™ electronic works if you follow the terms of this\nagreement and help preserve free future access to Project Gutenberg™\nelectronic works. See paragraph 1.E below.\n\n1.C. The Project Gutenberg Literary Archive Foundation (“the\nFoundation” or PGLAF), owns a compilation copyright in the collection\nof Project Gutenberg™ electronic works. Nearly all the individual\nworks in the collection are in the public domain in the United\nStates. If an individual work is unprotected by copyright law in the\nUnited States and you are located in the United States, we do not\nclaim a right to prevent you from copying, distributing, performing,\ndisplaying or creating derivative works based'}
16:35:42,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:42,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 19.599378208000417. input_tokens=34, output_tokens=422
16:35:42,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:42,991 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:42,992 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23719, Requested 7188. Please try again in 1.814s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:42,994 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "you or I could have told\nit him!) struck up 'Sir Roger de Coverley.' Then old Fezziwig stood\nout to dance with Mrs. Fezziwig. Top couple, too; with a good stiff\npiece of work cut out for them; three or four and twenty pair of\npartners; people who were not to be trifled with; people who would\ndance, and had no notion of walking.\n\nBut if they had been twice as many--ah! four times--old Fezziwig would\nhave been a match for them, and so would Mrs. Fezziwig. As to _her_, she\nwas worthy to be his partner in every sense of the term. If that's not\nhigh praise, tell me higher, and I'll use it. A positive light appeared\nto issue from Fezziwig's calves. They shone in every part of the dance\nlike moons. You couldn't have predicted, at any given time, what would\nbecome of them next. And when old Fezziwig and Mrs. Fezziwig had gone\nall through the dance; advance and retire, both hands to your partner,\nbow and curtsy, cork-screw, thread-the-needle, and back again to your\nplace: Fezziwig 'cut'--cut so deftly, that he appeared to wink with his\nlegs, and came upon his feet again without a stagger.\n\nWhen the clock struck eleven, this domestic ball broke up. Mr. and Mrs.\nFezziwig took their stations, one on either side the door, and, shaking\nhands with every person individually as he or she went out, wished him\nor her a Merry Christmas. When everybody had retired but the two\n'prentices, they did the same to them; and thus the cheerful voices died\naway, and the lads were left to their beds; which were under a counter\nin the back-shop.\n\nDuring the whole of this time Scrooge had acted like a man out of his\nwits. His heart and soul were in the scene, and with his former self. He\ncorroborated everything, remembered everything, enjoyed everything, and\nunderwent the strangest agitation. It was not until now, when the bright\nfaces of his former self and Dick were turned from them, that he\nremembered the Ghost, and became conscious that it was looking full upon\nhim, while the light upon its head burnt very clear.\n\n'A small matter,' said the Ghost, 'to make these silly folks so full of\ngratitude.'\n\n'Small!' echoed Scrooge.\n\nThe Spirit signed to him to listen to the two apprentices, who were\npouring out their hearts in praise of Fezziwig; and when he had done so,\nsaid:\n\n'Why! Is it not? He has spent but a few pounds of your mortal money:\nthree or four, perhaps. Is that so much that he deserves this praise?'\n\n'It isn't that,' said Scrooge, heated by the remark, and speaking\nunconsciously like his former, not his latter self. 'It isn't that,\nSpirit. He has the power to render us happy or unhappy; to make our\nservice light or burdensome; a pleasure or a toil. Say that his power\nlies in words and looks; in things so slight and insignificant that it\nis impossible to add and count 'em up: what then? The happiness he gives\nis quite as great as if it cost a fortune.'\n\nHe felt the Spirit's glance, and stopped.\n\n'What is the matter?' asked the Ghost.\n\n'Nothing particular,' said Scrooge.\n\n'Something, I think?' the Ghost insisted.\n\n'No,' said Scrooge, 'no. I should like to be able to say a word or two\nto my clerk just now. That's all.'\n\nHis former self turned down the lamps as he gave utterance to the wish;\nand Scrooge and the Ghost again stood side by side in the open air.\n\n'My time grows short,' observed the Spirit. 'Quick!'\n\nThis was not addressed to Scrooge, or to any one whom he could see, but\nit produced an immediate effect. For again Scrooge saw himself. He was\nolder now; a man in the prime of life. His face had not the harsh and\nrigid lines of later years; but it had begun to wear the signs of care\nand avarice. There was an eager, greedy, restless motion in the eye,\nwhich showed the passion that had taken root, and where the shadow of\nthe growing tree would fall.\n\nHe was not alone, but sat by the side of a fair young girl in a mourning\ndress: in whose eyes there were tears, which sparkled in the light that\nshone out of the Ghost of Christmas Past.\n\n'It matters little,' she said softly. 'To you, very little. Another idol\nhas displaced me; and, if it can cheer and comfort you in time to come\nas I would have tried to do, I have no just cause to grieve.'\n\n'What Idol has displaced you?' he rejoined.\n\n'A golden one.'\n\n'This is the even-handed dealing of the world!' he said. 'There is\nnothing on which it is so hard as poverty; and there is nothing it\nprofesses to condemn with such severity as the pursuit of wealth!'\n\n'You fear the world too much,' she answered gently. 'All your other\nhopes have merged into the hope of being beyond the chance of its sordid\nreproach. I have seen your nobler aspirations fall off one by one, until\nthe master passion, Gain, engrosses you. Have I not?'\n\n'"}
16:35:43,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:43,487 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:43,487 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23503, Requested 7021. Please try again in 1.048s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:43,487 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg™ electronic works\nprovided that:\n\n    • You pay a royalty fee of 20% of the gross profits you derive from\n        the use of Project Gutenberg™ works calculated using the method\n        you already use to calculate your applicable taxes. The fee is owed\n        to the owner of the Project Gutenberg™ trademark, but he has\n        agreed to donate royalties under this paragraph to the Project\n        Gutenberg Literary Archive Foundation. Royalty payments must be paid\n        within 60 days following each date on which you prepare (or are\n        legally required to prepare) your periodic tax returns. Royalty\n        payments should be clearly marked as such and sent to the Project\n        Gutenberg Literary Archive Foundation at the address specified in\n        Section 4, “Information about donations to the Project Gutenberg\n        Literary Archive Foundation.”\n    \n    • You provide a full refund of any money paid by a user who notifies\n        you in writing (or by e-mail) within 30 days of receipt that s/he\n        does not agree to the terms of the full Project Gutenberg™\n        License. You must require such a user to return or destroy all\n        copies of the works possessed in a physical medium and discontinue\n        all use of and all access to other copies of Project Gutenberg™\n        works.\n    \n    • You provide, in accordance with paragraph 1.F.3, a full refund of\n        any money paid for a work or a replacement copy, if a defect in the\n        electronic work is discovered and reported to you within 90 days of\n        receipt of the work.\n    \n    • You comply with all other terms of this agreement for free\n        distribution of Project Gutenberg™ works.\n    \n\n1.E.9. If you wish to charge a fee or distribute a Project\nGutenberg™ electronic work or group of works on different terms than\nare set forth in this agreement, you must obtain permission in writing\nfrom the Project Gutenberg Literary Archive Foundation, the manager of\nthe Project Gutenberg™ trademark. Contact the Foundation as set\nforth in Section 3 below.\n\n1.F.\n\n1.F.1. Project Gutenberg volunteers and employees expend considerable\neffort to identify, do copyright research on, transcribe and proofread\nworks not protected by U.S. copyright law in creating the Project\nGutenberg™ collection. Despite these efforts, Project Gutenberg™\nelectronic works, and the medium on which they may be stored, may\ncontain “Defects,” such as, but not limited to, incomplete, inaccurate\nor corrupt data, transcription errors, a copyright or other\nintellectual property infringement, a defective or damaged disk or\nother medium, a computer virus, or computer codes that damage or\ncannot be read by your equipment.\n\n1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the “Right\nof Replacement or Refund” described in paragraph 1.F.3, the Project\nGutenberg Literary Archive Foundation, the owner of the Project\nGutenberg™ trademark, and any other party distributing a Project\nGutenberg™ electronic work under this agreement, disclaim all\nliability to you for damages, costs and expenses, including legal\nfees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\nPROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\n1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\ndefect in this electronic work within 90 days of receiving it, you can\nreceive a refund of the money (if any) you paid for it by sending a\nwritten explanation to the person you received the work from. If you\nreceived the work on a physical medium, you must return the medium\nwith your written explanation. The person or entity that provided you\nwith the defective work may elect to provide a replacement copy in\nlieu of a refund. If you received the work electronically, the person\nor entity providing it to you may choose to give you a second\nopportunity to receive the work electronically in lieu of a refund. If\nthe second copy is also defective, you may demand a refund in writing\nwithout further opportunities to fix the problem.\n\n1.F.4. Except for the limited right of replacement or refund set forth\nin paragraph 1.F.3, this work is provided to you ‘AS-IS’, WITH NO\nOTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\nLIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\n\n1.F.5. Some states do not allow disclaimers of certain implied\nwarranties or the exclusion or limitation of certain types of\ndamages. If any disclaimer or limitation set forth in this agreement\nviolates the law of the state applicable to this agreement, the\nagreement shall be interpreted to make the maximum disclaimer or\nlimitation permitted by the applicable state law. The invalidity or\nunenforceability of any provision of this agreement shall not void the\nremaining provisions.\n\n1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the\ntrademark owner, any agent or employee of the Foundation, anyone\nproviding copies of Project Gutenberg™ electronic works in\naccordance with this agreement, and any volunteers associated with the\nproduction, promotion and distribution of Project Gutenberg™\nelectronic works,'}
16:35:43,811 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:43,813 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:43,813 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23310, Requested 7139. Please try again in 898ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:43,815 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'they were sharp girls too, as Topper could have told you.\nThere might have been twenty people there, young and old, but they all\nplayed, and so did Scrooge; for wholly forgetting, in the interest he\nhad in what was going on, that his voice made no sound in their ears, he\nsometimes came out with his guess quite loud, and very often guessed\nright, too; for the sharpest needle, best Whitechapel, warranted not to\ncut in the eye, was not sharper than Scrooge, blunt as he took it in\nhis head to be.\n\nThe Ghost was greatly pleased to find him in this mood, and looked upon\nhim with such favour that he begged like a boy to be allowed to stay\nuntil the guests departed. But this the Spirit said could not be done.\n\n\'Here is a new game,\' said Scrooge. \'One half-hour, Spirit, only one!\'\n\nIt was a game called Yes and No, where Scrooge\'s nephew had to think of\nsomething, and the rest must find out what, he only answering to their\nquestions yes or no, as the case was. The brisk fire of questioning to\nwhich he was exposed elicited from him that he was thinking of an\nanimal, a live animal, rather a disagreeable animal, a savage animal, an\nanimal that growled and grunted sometimes, and talked sometimes and\nlived in London, and walked about the streets, and wasn\'t made a show\nof, and wasn\'t led by anybody, and didn\'t live in a menagerie, and was\nnever killed in a market, and was not a horse, or an ass, or a cow, or a\nbull, or a tiger, or a dog, or a pig, or a cat, or a bear. At every\nfresh question that was put to him, this nephew burst into a fresh roar\nof laughter; and was so inexpressibly tickled, that he was obliged to\nget up off the sofa and stamp. At last the plump sister, falling into a\nsimilar state, cried out:\n\n\'I have found it out! I know what it is, Fred! I know what it is!\'\n\n\'What is it?\' cried Fred.\n\n\'It\'s your uncle Scro-o-o-o-oge.\'\n\nWhich it certainly was. Admiration was the universal sentiment, though\nsome objected that the reply to \'Is it a bear?\' ought to have been\n\'Yes\'; inasmuch as an answer in the negative was sufficient to have\ndiverted their thoughts from Mr. Scrooge, supposing they had ever had\nany tendency that way.\n\n\'He has given us plenty of merriment, I am sure,\' said Fred, \'and it\nwould be ungrateful not to drink his health. Here is a glass of mulled\nwine ready to our hand at the moment; and I say, "Uncle Scrooge!"\'\n\n\'Well! Uncle Scrooge!\' they cried.\n\n\'A merry Christmas and a happy New Year to the old man, whatever he is!\'\nsaid Scrooge\'s nephew. \'He wouldn\'t take it from me, but may he have it,\nnevertheless. Uncle Scrooge!\'\n\nUncle Scrooge had imperceptibly become so gay and light of heart, that\nhe would have pledged the unconscious company in return, and thanked\nthem in an inaudible speech, if the Ghost had given him time. But the\nwhole scene passed off in the breath of the last word spoken by his\nnephew; and he and the Spirit were again upon their travels.\n\nMuch they saw, and far they went, and many homes they visited, but\nalways with a happy end. The Spirit stood beside sick-beds, and they\nwere cheerful; on foreign lands, and they were close at home; by\nstruggling men, and they were patient in their greater hope; by poverty,\nand it was rich. In almshouse, hospital, and gaol, in misery\'s every\nrefuge, where vain man in his little brief authority had not made fast\nthe door, and barred the Spirit out, he left his blessing and taught\nScrooge his precepts.\n\nIt was a long night, if it were only a night; but Scrooge had his doubts\nof this, because the Christmas holidays appeared to be condensed into\nthe space of time they passed together. It was strange, too, that, while\nScrooge remained unaltered in his outward form, the Ghost grew older,\nclearly older. Scrooge had observed this change, but never spoke of it\nuntil they left a children\'s Twelfth-Night party, when, looking at the\nSpirit as they stood together in an open place, he noticed that its hair\nwas grey.\n\n\'Are spirits\' lives so short?\' asked Scrooge.\n\n\'My life upon this globe is very brief,\' replied the Ghost. \'It ends\nto-night.\'\n\n\'To-night!\' cried Scrooge.\n\n\'To-night at midnight. Hark! The time is drawing near.\'\n\nThe chimes were ringing the three-quarters past eleven at that moment.\n\n\'Forgive me if I am not justified in what I ask,\' said Scrooge, looking\nintently at the Spirit\'s robe, \'but I see something strange, and not\nbelonging to yourself, protruding from your skirts. Is it a foot or a\nclaw?\'\n\n\'It might be a claw, for the flesh there is upon it,\' was the Spirit\'s\nsorrowful reply. \'Look here!\'\n\nFrom the foldings of its robe it brought two children, wretched, abject,\nfrightful, hideous, miserable. They knelt'}
16:35:43,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:43,919 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,150 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,151 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23129, Requested 7089. Please try again in 436ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,151 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': ",\nto save himself from falling in a swoon. But how much greater was his\nhorror when the phantom, taking off the bandage round his head, as if it\nwere too warm to wear indoors, its lower jaw dropped down upon its\nbreast!\n\nScrooge fell upon his knees, and clasped his hands before his face.\n\n'Mercy!' he said. 'Dreadful apparition, why do you trouble me?'\n\n'Man of the worldly mind!' replied the Ghost, 'do you believe in me or\nnot?'\n\n'I do,' said Scrooge; 'I must. But why do spirits walk the earth, and\nwhy do they come to me?'\n\n'It is required of every man,' the Ghost returned, 'that the spirit\nwithin him should walk abroad among his fellow-men, and travel far and\nwide; and, if that spirit goes not forth in life, it is condemned to do\nso after death. It is doomed to wander through the world--oh, woe is\nme!--and witness what it cannot share, but might have shared on earth,\nand turned to happiness!'\n\nAgain the spectre raised a cry, and shook its chain and wrung its\nshadowy hands.\n\n'You are fettered,' said Scrooge, trembling. 'Tell me why?'\n\n'I wear the chain I forged in life,' replied the Ghost. 'I made it link\nby link, and yard by yard; I girded it on of my own free will, and of\nmy own free will I wore it. Is its pattern strange to _you_?'\n\nScrooge trembled more and more.\n\n'Or would you know,' pursued the Ghost, 'the weight and length of the\nstrong coil you bear yourself? It was full as heavy and as long as this\nseven Christmas Eves ago. You have laboured on it since. It is a\nponderous chain!'\n\nScrooge glanced about him on the floor, in the expectation of finding\nhimself surrounded by some fifty or sixty fathoms of iron cable; but he\ncould see nothing.\n\n'Jacob!' he said imploringly. 'Old Jacob Marley, tell me more! Speak\ncomfort to me, Jacob!'\n\n'I have none to give,' the Ghost replied. 'It comes from other regions,\nEbenezer Scrooge, and is conveyed by other ministers, to other kinds of\nmen. Nor can I tell you what I would. A very little more is all\npermitted to me. I cannot rest, I cannot stay, I cannot linger anywhere.\nMy spirit never walked beyond our counting-house--mark me;--in life my\nspirit never roved beyond the narrow limits of our money-changing hole;\nand weary journeys lie before me!'\n\nIt was a habit with Scrooge, whenever he became thoughtful, to put his\nhands in his breeches pockets. Pondering on what the Ghost had said, he\ndid so now, but without lifting up his eyes, or getting off his knees.\n\n[Illustration: ON THE WINGS OF THE WIND]\n\n'You must have been very slow about it, Jacob,' Scrooge observed in a\nbusiness-like manner, though with humility and deference.\n\n'Slow!' the Ghost repeated.\n\n'Seven years dead,' mused Scrooge. 'And travelling all the time?'\n\n'The whole time,' said the Ghost. 'No rest, no peace. Incessant torture\nof remorse.'\n\n'You travel fast?' said Scrooge.\n\n[Illustration]\n\n'On the wings of the wind,' replied the Ghost.\n\n'You might have got over a great quantity of ground in seven years,'\nsaid Scrooge.\n\nThe Ghost, on hearing this, set up another cry, and clanked its chain so\nhideously in the dead silence of the night, that the Ward would have\nbeen justified in indicting it for a nuisance.\n\n'Oh! captive, bound, and double-ironed,' cried the phantom, 'not to know\nthat ages of incessant labour, by immortal creatures, for this earth\nmust pass into eternity before the good of which it is susceptible is\nall developed! Not to know that any Christian spirit working kindly in\nits little sphere, whatever it may be, will find its mortal life too\nshort for its vast means of usefulness! Not to know that no space of\nregret can make amends for one life's opportunities misused! Yet such\nwas I! Oh, such was I!'\n\n'But you were always a good man of business, Jacob,' faltered Scrooge,\nwho now began to apply this to himself.\n\n'Business!' cried the Ghost, wringing its hands again. 'Mankind was my\nbusiness. The common welfare was my business; charity, mercy,\nforbearance, and benevolence were, all, my business. The dealings of my\ntrade were but a drop of water in the comprehensive ocean of my\nbusiness!'\n\nIt held up its chain at arm's-length, as if that were the cause of all\nits unavailing grief, and flung it heavily upon the ground again.\n\n'At this time of the rolling year,' the spectre said, 'I suffer most.\nWhy did I walk through crowds of fellow-beings with my eyes turned down,\nand never raise them to that blessed Star which led the Wise Men to a\npoor abode? Were there no poor homes to which its light would have\nconducted _me_?'\n\nScrooge was very much dismayed to hear the spectre going on at this\nrate, and began to quake exceedingly.\n\n'Hear me!' cried the Ghost. 'My time is nearly gone.'\n\n'I will,' said Scrooge. 'But don't be hard"}
16:35:44,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,396 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,396 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23008, Requested 7168. Please try again in 352ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,397 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "had a Christmas thought, or spoke below his breath to his companion of\nsome bygone Christmas Day, with homeward hopes belonging to it. And\nevery man on board, waking or sleeping, good or bad, had had a kinder\nword for one another on that day than on any day in the year; and had\nshared to some extent in its festivities; and had remembered those he\ncared for at a distance, and had known that they delighted to remember\nhim.\n\nIt was a great surprise to Scrooge, while listening to the moaning of\nthe wind, and thinking what a solemn thing it was to move on through the\nlonely darkness over an unknown abyss, whose depths were secrets as\nprofound as death: it was a great surprise to Scrooge, while thus\nengaged, to hear a hearty laugh. It was a much greater surprise to\nScrooge to recognise it as his own nephew's and to find himself in a\nbright, dry, gleaming room, with the Spirit standing smiling by his\nside, and looking at that same nephew with approving affability!\n\n'Ha, ha!' laughed Scrooge's nephew. 'Ha, ha, ha!'\n\nIf you should happen, by any unlikely chance, to know a man more blessed\nin a laugh than Scrooge's nephew, all I can say is, I should like to\nknow him too. Introduce him to me, and I'll cultivate his acquaintance.\n\nIt is a fair, even-handed, noble adjustment of things, that while there\nis infection in disease and sorrow, there is nothing in the world so\nirresistibly contagious as laughter and good-humour. When Scrooge's\nnephew laughed in this way--holding his sides, rolling his head, and\ntwisting his face into the most extravagant contortions--Scrooge's\nniece, by marriage, laughed as heartily as he. And their assembled\nfriends, being not a bit behindhand, roared out lustily.\n\n'Ha, ha! Ha, ha, ha, ha!'\n\n'He said that Christmas was a humbug, as I live!' cried Scrooge's\nnephew. 'He believed it, too!'\n\n'More shame for him, Fred!' said Scrooge's niece indignantly. Bless\nthose women! they never do anything by halves. They are always in\nearnest.\n\nShe was very pretty; exceedingly pretty. With a dimpled,\nsurprised-looking, capital face; a ripe little mouth, that seemed made\nto be kissed--as no doubt it was; all kinds of good little dots about\nher chin, that melted into one another when she laughed; and the\nsunniest pair of eyes you ever saw in any little creature's head.\nAltogether she was what you would have called provoking, you know; but\nsatisfactory, too. Oh, perfectly satisfactory!\n\n'He's a comical old fellow,' said Scrooge's nephew, 'that's the truth;\nand not so pleasant as he might be. However, his offences carry their\nown punishment, and I have nothing to say against him.'\n\n'I'm sure he is very rich, Fred,' hinted Scrooge's niece. 'At least, you\nalways tell _me_ so.'\n\n'What of that, my dear?' said Scrooge's nephew. 'His wealth is of no use\nto him. He don't do any good with it. He don't make himself comfortable\nwith it. He hasn't the satisfaction of thinking--ha, ha, ha!--that he is\never going to benefit Us with it.'\n\n'I have no patience with him,' observed Scrooge's niece. Scrooge's\nniece's sisters, and all the other ladies, expressed the same opinion.\n\n'Oh, I have!' said Scrooge's nephew. 'I am sorry for him; I couldn't be\nangry with him if I tried. Who suffers by his ill whims? Himself always.\nHere he takes it into his head to dislike us, and he won't come and dine\nwith us. What's the consequence? He don't lose much of a dinner.'\n\n'Indeed, I think he loses a very good dinner,' interrupted Scrooge's\nniece. Everybody else said the same, and they must be allowed to have\nbeen competent judges, because they had just had dinner; and with the\ndessert upon the table, were clustered round the fire, by lamplight.\n\n'Well! I am very glad to hear it,' said Scrooge's nephew, 'because I\nhaven't any great faith in these young housekeepers. What do _you_ say,\nTopper?'\n\nTopper had clearly got his eye upon one of Scrooge's niece's sisters,\nfor he answered that a bachelor was a wretched outcast, who had no right\nto express an opinion on the subject. Whereat Scrooge's niece's\nsister--the plump one with the lace tucker: not the one with the\nroses--blushed.\n\n'Do go on, Fred,' said Scrooge's niece, clapping her hands. 'He never\nfinishes what he begins to say! He is such a ridiculous fellow!'\n\nScrooge's nephew revelled in another laugh, and as it was impossible to\nkeep the infection off, though the plump sister tried hard to do it with\naromatic vinegar, his example was unanimously followed.\n\n'I was only going to say,' said Scrooge's nephew, 'that the consequence\nof his taking a dislike to us, and not making merry with us, is, as I\nthink, that he loses some pleasant moments, which"}
16:35:44,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:44,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 3.924047041000449. input_tokens=34, output_tokens=83
16:35:44,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,666 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,667 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26313, Requested 7220. Please try again in 7.066s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,669 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': ",\ngnawed and mumbled by the hungry cold as bones are gnawed by dogs,\nstooped down at Scrooge's keyhole to regale him with a Christmas carol;\nbut, at the first sound of\n\n  'God bless you, merry gentleman,\n  May nothing you dismay!'\n\nScrooge seized the ruler with such energy of action that the singer fled\nin terror, leaving the keyhole to the fog, and even more congenial\nfrost.\n\nAt length the hour of shutting up the counting-house arrived. With an\nill-will Scrooge dismounted from his stool, and tacitly admitted the\nfact to the expectant clerk in the tank, who instantly snuffed his\ncandle out, and put on his hat.\n\n'You'll want all day to-morrow, I suppose?' said Scrooge.\n\n'If quite convenient, sir.'\n\n'It's not convenient,' said Scrooge, 'and it's not fair. If I was to\nstop half-a-crown for it, you'd think yourself ill used, I'll be bound?'\n\nThe clerk smiled faintly.\n\n'And yet,' said Scrooge, 'you don't think _me_ ill used when I pay a\nday's wages for no work.'\n\n[Illustration: _Bob Cratchit went down a slide on Cornhill, at the end\nof a lane of boys, twenty times, in honour of its being Christmas\nEve_]\n\nThe clerk observed that it was only once a year.\n\n'A poor excuse for picking a man's pocket every twenty-fifth of\nDecember!' said Scrooge, buttoning his greatcoat to the chin. 'But I\nsuppose you must have the whole day. Be here all the earlier next\nmorning.'\n\nThe clerk promised that he would; and Scrooge walked out with a growl.\nThe office was closed in a twinkling, and the clerk, with the long ends\nof his white comforter dangling below his waist (for he boasted no\ngreatcoat), went down a slide on Cornhill, at the end of a lane of boys,\ntwenty times, in honour of its being Christmas Eve, and then ran home to\nCamden Town as hard as he could pelt, to play at blind man's-buff.\n\nScrooge took his melancholy dinner in his usual melancholy tavern; and\nhaving read all the newspapers, and beguiled the rest of the evening\nwith his banker's book, went home to bed. He lived in chambers which had\nonce belonged to his deceased partner. They were a gloomy suite of\nrooms, in a lowering pile of building up a yard, where it had so little\nbusiness to be, that one could scarcely help fancying it must have run\nthere when it was a young house, playing at hide-and-seek with other\nhouses, and have forgotten the way out again. It was old enough now, and\ndreary enough; for nobody lived in it but Scrooge, the other rooms\nbeing all let out as offices. The yard was so dark that even Scrooge,\nwho knew its every stone, was fain to grope with his hands. The fog and\nfrost so hung about the black old gateway of the house, that it seemed\nas if the Genius of the Weather sat in mournful meditation on the\nthreshold.\n\nNow, it is a fact that there was nothing at all particular about the\nknocker on the door, except that it was very large. It is also a fact\nthat Scrooge had seen it, night and morning, during his whole residence\nin that place; also that Scrooge had as little of what is called fancy\nabout him as any man in the City of London, even including--which is a\nbold word--the corporation, aldermen, and livery. Let it also be borne\nin mind that Scrooge had not bestowed one thought on Marley since his\nlast mention of his seven-years'-dead partner that afternoon. And then\nlet any man explain to me, if he can, how it happened that Scrooge,\nhaving his key in the lock of the door, saw in the knocker, without its\nundergoing any intermediate process of change--not a knocker, but\nMarley's face.\n\nMarley's face. It was not in impenetrable shadow, as the other objects\nin the yard were, but had a dismal light about it, like a bad lobster in\na dark cellar. It was not angry or ferocious, but looked at Scrooge as\nMarley used to look; with ghostly spectacles turned up on its ghostly\nforehead. The hair was curiously stirred, as if by breath or hot air;\nand, though the eyes were wide open, they were perfectly motionless.\nThat, and its livid colour, made it horrible; but its horror seemed to\nbe in spite of the face, and beyond its control, rather than a part of\nits own expression.\n\nAs Scrooge looked fixedly at this phenomenon, it was a knocker again.\n\nTo say that he was not startled, or that his blood was not conscious of\na terrible sensation to which it had been a stranger from infancy, would\nbe untrue. But he put his hand upon the key he had relinquished, turned\nit sturdily, walked in, and lighted his candle.\n\nHe _did_ pause, with a moment's irresolution, before he shut the door;\nand he _did_ look cautiously behind it first, as if he half expected to\nbe terrified with the sight of Marley's pigtail sticking out into the\nhall. But there was nothing on the back of the door,"}
16:35:44,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,743 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,743 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26271, Requested 7171. Please try again in 6.884s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,744 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "announced itself in awful language.\n\nThe room was very dark, too dark to be observed with any accuracy,\nthough Scrooge glanced round it in obedience to a secret impulse,\nanxious to know what kind of room it was. A pale light, rising in the\nouter air, fell straight upon the bed; and on it, plundered and bereft,\nunwatched, unwept, uncared for, was the body of this man.\n\nScrooge glanced towards the Phantom. Its steady hand was pointed to the\nhead. The cover was so carelessly adjusted that the slightest raising of\nit, the motion of a finger upon Scrooge's part, would have disclosed the\nface. He thought of it, felt how easy it would be to do, and longed to\ndo it; but he had no more power to withdraw the veil than to dismiss the\nspectre at his side.\n\nOh, cold, cold, rigid, dreadful Death, set up thine altar here, and\ndress it with such terrors as thou hast at thy command; for this is thy\ndominion! But of the loved, revered, and honoured head thou canst not\nturn one hair to thy dread purposes, or make one feature odious. It is\nnot that the hand is heavy, and will fall down when released; it is not\nthat the heart and pulse are still; but that the hand was open,\ngenerous, and true; the heart brave, warm, and tender, and the pulse a\nman's. Strike, Shadow, strike! And see his good deeds springing from the\nwound, to sow the world with life immortal!\n\nNo voice pronounced these words in Scrooge's ears, and yet he heard them\nwhen he looked upon the bed. He thought, if this man could be raised up\nnow, what would be his foremost thoughts? Avarice, hard dealing, griping\ncares? They have brought him to a rich end, truly!\n\nHe lay in the dark, empty house, with not a man, a woman, or a child to\nsay he was kind to me in this or that, and for the memory of one kind\nword I will be kind to him. A cat was tearing at the door, and there was\na sound of gnawing rats beneath the hearthstone. What _they_ wanted in\nthe room of death, and why they were so restless and disturbed, Scrooge\ndid not dare to think.\n\n'Spirit!' he said, 'this is a fearful place. In leaving it, I shall not\nleave its lesson, trust me. Let us go!'\n\nStill the Ghost pointed with an unmoved finger to the head.\n\n'I understand you,' Scrooge returned, 'and I would do it if I could. But\nI have not the power, Spirit. I have not the power.'\n\nAgain it seemed to look upon him.\n\n'If there is any person in the town who feels emotion caused by this\nman's death,' said Scrooge, quite agonised, 'show that person to me,\nSpirit, I beseech you!'\n\nThe Phantom spread its dark robe before him for a moment, like a wing;\nand, withdrawing it, revealed a room by daylight, where a mother and her\nchildren were.\n\nShe was expecting some one, and with anxious eagerness; for she walked\nup and down the room, started at every sound, looked out from the\nwindow, glanced at the clock, tried, but in vain, to work with her\nneedle, and could hardly bear the voices of her children in their play.\n\nAt length the long-expected knock was heard. She hurried to the door,\nand met her husband; a man whose face was careworn and depressed, though\nhe was young. There was a remarkable expression in it now, a kind of\nserious delight of which he felt ashamed, and which he struggled to\nrepress.\n\nHe sat down to the dinner that had been hoarding for him by the fire,\nand when she asked him faintly what news (which was not until after a\nlong silence), he appeared embarrassed how to answer.\n\n'Is it good,' she said, 'or bad?' to help him.\n\n'Bad,' he answered.\n\n'We are quite ruined?'\n\n'No. There is hope yet, Caroline.'\n\n'If _he_ relents,' she said, amazed, 'there is! Nothing is past hope, if\nsuch a miracle has happened.'\n\n'He is past relenting,' said her husband. 'He is dead.'\n\nShe was a mild and patient creature, if her face spoke truth; but she\nwas thankful in her soul to hear it, and she said so with clasped hands.\nShe prayed forgiveness the next moment, and was sorry; but the first was\nthe emotion of her heart.\n\n'What the half-drunken woman, whom I told you of last night, said to me\nwhen I tried to see him and obtain a week's delay--and what I thought\nwas a mere excuse to avoid me--turns out to have been quite true. He was\nnot only very ill, but dying, then.'\n\n'To whom will our debt be transferred?'\n\n'I don't know. But, before that time, we shall be ready with the money;\nand even though we were not, it would be bad fortune indeed to find so\nmerciless a creditor in his successor. We may sleep to-night with light\nhearts, Caroline!'\n\nYes. Soften it as they would, their hearts were lighter. The children's\nfaces, hushed and clustered round to hear what they so little\nunderstood, were brighter; and it was a happier house for this man's\ndeath! The only emotion that the Ghost could"}
16:35:44,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,839 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,839 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26226, Requested 7039. Please try again in 6.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,840 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': ',\' replied the woman: \'and it\nshould have been, you may depend upon it, if I could have laid my hands\non anything else. Open that bundle, old Joe, and let me know the value\nof it. Speak out plain. I\'m not afraid to be the first, nor afraid for\nthem to see it. We knew pretty well that we were helping ourselves\nbefore we met here, I believe. It\'s no sin. Open the bundle, Joe.\'\n\nBut the gallantry of her friends would not allow of this; and the man in\nfaded black, mounting the breach first, produced _his_ plunder. It was\nnot extensive. A seal or two, a pencil-case, a pair of sleeve-buttons,\nand a brooch of no great value, were all. They were severally examined\nand appraised by old Joe, who chalked the sums he was disposed to give\nfor each upon the wall, and added them up into a total when he found\nthat there was nothing more to come.\n\n\'That\'s your account,\' said Joe, \'and I wouldn\'t give another sixpence,\nif I was to be boiled for not doing it. Who\'s next?\'\n\n\n[Illustration: _"What do you call this?" said Joe. "Bed-curtains."_]\n\nMrs. Dilber was next. Sheets and towels, a little wearing apparel, two\nold fashioned silver teaspoons, a pair of sugar-tongs, and a few\nboots. Her account was stated on the wall in the same manner.\n\n\'I always give too much to ladies. It\'s a weakness of mine, and that\'s\nthe way I ruin myself,\' said old Joe. \'That\'s your account. If you asked\nme for another penny, and made it an open question, I\'d repent of being\nso liberal, and knock off half-a-crown.\'\n\n\'And now undo _my_ bundle, Joe,\' said the first woman.\n\nJoe went down on his knees for the greater convenience of opening it,\nand, having unfastened a great many knots, dragged out a large heavy\nroll of some dark stuff.\n\n\'What do you call this?\' said Joe. \'Bed-curtains?\'\n\n\'Ah!\' returned the woman, laughing and leaning forward on her crossed\narms. \'Bed-curtains!\'\n\n\'You don\'t mean to say you took \'em down, rings and all, with him lying\nthere?\' said Joe.\n\n\'Yes, I do,\' replied the woman. \'Why not?\'\n\n\'You were born to make your fortune,\' said Joe, \'and you\'ll certainly do\nit.\'\n\n\'I certainly shan\'t hold my hand, when I can get anything in it by\nreaching it out, for the sake of such a man as he was, I promise you,\nJoe,\' returned the woman coolly. \'Don\'t drop that oil upon the blankets,\nnow.\'\n\n\'His blankets?\' asked Joe.\n\n\'Whose else\'s do you think?\' replied the woman. \'He isn\'t likely to take\ncold without \'em, I dare say.\'\n\n\'I hope he didn\'t die of anything catching? Eh?\' said old Joe, stopping\nin his work, and looking up.\n\n\'Don\'t you be afraid of that,\' returned the woman. \'I an\'t so fond of\nhis company that I\'d loiter about him for such things, if he did. Ah!\nyou may look through that shirt till your eyes ache, but you won\'t find\na hole in it, nor a threadbare place. It\'s the best he had, and a fine\none too. They\'d have wasted it, if it hadn\'t been for me.\'\n\n\'What do you call wasting of it?\' asked old Joe.\n\n\'Putting it on him to be buried in, to be sure,\' replied the woman, with\na laugh. \'Somebody was fool enough to do it, but I took it off again. If\ncalico an\'t good enough for such a purpose, it isn\'t good enough for\nanything. It\'s quite as becoming to the body. He can\'t look uglier than\nhe did in that one.\'\n\nScrooge listened to this dialogue in horror. As they sat grouped about\ntheir spoil, in the scanty light afforded by the old man\'s lamp, he\nviewed them with a detestation and disgust which could hardly have been\ngreater, though they had been obscene demons marketing the corpse\nitself.\n\n\'Ha, ha!\' laughed the same woman when old Joe producing a flannel bag\nwith money in it, told out their several gains upon the ground. \'This\nis the end of it, you see! He frightened every one away from him when he\nwas alive, to profit us when he was dead! Ha, ha, ha!\'\n\n\'Spirit!\' said Scrooge, shuddering from head to foot. \'I see, I see. The\ncase of this unhappy man might be my own. My life tends that way now.\nMerciful heaven, what is this?\'\n\nHe recoiled in terror, for the scene had changed, and now he almost\ntouched a bed--a bare, uncurtained bed--on which, beneath a ragged\nsheet, there lay a something covered up, which, though it was dumb,\nannounced itself in awful language.\n\nThe room was very dark, too dark to be observed with any accuracy,\nthough Scrooge glanced round it in obedience to a secret impulse,\nanxious to know what kind of room it was. A pale light, rising in the\nouter air, fell straight upon the bed; and on it, plundered and bereft,\nunwatched, unwept, uncared for, was the body of this man.\n\nScrooge glanced towards the Phantom'}
16:35:45,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:45,99 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:45,99 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26096, Requested 7030. Please try again in 6.252s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:45,100 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '.C. The Project Gutenberg Literary Archive Foundation (“the\nFoundation” or PGLAF), owns a compilation copyright in the collection\nof Project Gutenberg™ electronic works. Nearly all the individual\nworks in the collection are in the public domain in the United\nStates. If an individual work is unprotected by copyright law in the\nUnited States and you are located in the United States, we do not\nclaim a right to prevent you from copying, distributing, performing,\ndisplaying or creating derivative works based on the work as long as\nall references to Project Gutenberg are removed. Of course, we hope\nthat you will support the Project Gutenberg™ mission of promoting\nfree access to electronic works by freely sharing Project Gutenberg™\nworks in compliance with the terms of this agreement for keeping the\nProject Gutenberg™ name associated with the work. You can easily\ncomply with the terms of this agreement by keeping this work in the\nsame format with its attached full Project Gutenberg™ License when\nyou share it without charge with others.\n\n1.D. The copyright laws of the place where you are located also govern\nwhat you can do with this work. Copyright laws in most countries are\nin a constant state of change. If you are outside the United States,\ncheck the laws of your country in addition to the terms of this\nagreement before downloading, copying, displaying, performing,\ndistributing or creating derivative works based on this work or any\nother Project Gutenberg™ work. The Foundation makes no\nrepresentations concerning the copyright status of any work in any\ncountry other than the United States.\n\n1.E. Unless you have removed all references to Project Gutenberg:\n\n1.E.1. The following sentence, with active links to, or other\nimmediate access to, the full Project Gutenberg™ License must appear\nprominently whenever any copy of a Project Gutenberg™ work (any work\non which the phrase “Project Gutenberg” appears, or with which the\nphrase “Project Gutenberg” is associated) is accessed, displayed,\nperformed, viewed, copied or distributed:\n\n    This eBook is for the use of anyone anywhere in the United States and most\n    other parts of the world at no cost and with almost no restrictions\n    whatsoever. You may copy it, give it away or re-use it under the terms\n    of the Project Gutenberg License included with this eBook or online\n    at www.gutenberg.org. If you\n    are not located in the United States, you will have to check the laws\n    of the country where you are located before using this eBook.\n  \n1.E.2. If an individual Project Gutenberg™ electronic work is\nderived from texts not protected by U.S. copyright law (does not\ncontain a notice indicating that it is posted with permission of the\ncopyright holder), the work can be copied and distributed to anyone in\nthe United States without paying any fees or charges. If you are\nredistributing or providing access to a work with the phrase “Project\nGutenberg” associated with or appearing on the work, you must comply\neither with the requirements of paragraphs 1.E.1 through 1.E.7 or\nobtain permission for the use of the work and the Project Gutenberg™\ntrademark as set forth in paragraphs 1.E.8 or 1.E.9.\n\n1.E.3. If an individual Project Gutenberg™ electronic work is posted\nwith the permission of the copyright holder, your use and distribution\nmust comply with both paragraphs 1.E.1 through 1.E.7 and any\nadditional terms imposed by the copyright holder. Additional terms\nwill be linked to the Project Gutenberg™ License for all works\nposted with the permission of the copyright holder found at the\nbeginning of this work.\n\n1.E.4. Do not unlink or detach or remove the full Project Gutenberg™\nLicense terms from this work, or any files containing a part of this\nwork or any other work associated with Project Gutenberg™.\n\n1.E.5. Do not copy, display, perform, distribute or redistribute this\nelectronic work, or any part of this electronic work, without\nprominently displaying the sentence set forth in paragraph 1.E.1 with\nactive links or immediate access to the full terms of the Project\nGutenberg™ License.\n\n1.E.6. You may convert to and distribute this work in any binary,\ncompressed, marked up, nonproprietary or proprietary form, including\nany word processing or hypertext form. However, if you provide access\nto or distribute copies of a Project Gutenberg™ work in a format\nother than “Plain Vanilla ASCII” or other format used in the official\nversion posted on the official Project Gutenberg™ website\n(www.gutenberg.org), you must, at no additional cost, fee or expense\nto the user, provide a copy, a means of exporting a copy, or a means\nof obtaining a copy upon request, of the work in its original “Plain\nVanilla ASCII” or other form. Any alternate format must include the\nfull Project Gutenberg™ License as specified in paragraph 1.E.1.\n\n1.E.7. Do not charge a fee for access to, viewing, displaying,\nperforming, copying or distributing any Project Gutenberg™ works\nunless you comply with paragraph 1.E.8 or 1.E.9.\n\n1.E.8. You may charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg™ electronic works\nprovided that:\n\n    • You pay a royalty fee of 20% of the gross profits you derive from\n        the use of Project Gutenberg™ works calculated using the method\n        you already use to calculate your applicable taxes. The fee is owed\n        to the owner of the Project Gutenberg™ trademark, but he has\n        agreed to donate royalties under this paragraph to the Project\n        Gutenberg Literary'}
16:35:45,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:45,233 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:45,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:45,549 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:45,550 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 25918, Requested 7270. Please try again in 6.376s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:45,551 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'the money;\nand even though we were not, it would be bad fortune indeed to find so\nmerciless a creditor in his successor. We may sleep to-night with light\nhearts, Caroline!\'\n\nYes. Soften it as they would, their hearts were lighter. The children\'s\nfaces, hushed and clustered round to hear what they so little\nunderstood, were brighter; and it was a happier house for this man\'s\ndeath! The only emotion that the Ghost could show him, caused by the\nevent, was one of pleasure.\n\n\'Let me see some tenderness connected with a death,\' said Scrooge; \'or\nthat dark chamber, Spirit, which we left just now, will be for ever\npresent to me.\'\n\nThe Ghost conducted him through several streets familiar to his feet;\nand as they went along, Scrooge looked here and there to find himself,\nbut nowhere was he to be seen. They entered poor Bob Cratchit\'s house;\nthe dwelling he had visited before; and found the mother and the\nchildren seated round the fire.\n\nQuiet. Very quiet. The noisy little Cratchits were as still as statues\nin one corner, and sat looking up at Peter, who had a book before him.\nThe mother and her daughters were engaged in sewing. But surely they\nwere very quiet!\n\n\'"And he took a child, and set him in the midst of them."\'\n\nWhere had Scrooge heard those words? He had not dreamed them. The boy\nmust have read them out as he and the Spirit crossed the threshold. Why\ndid he not go on?\n\nThe mother laid her work upon the table, and put her hand up to her\nface.\n\n\'The colour hurts my eyes,\' she said.\n\nThe colour? Ah, poor Tiny Tim!\n\n\'They\'re better now again,\' said Cratchit\'s wife. \'It makes them weak by\ncandle-light; and I wouldn\'t show weak eyes to your father when he comes\nhome for the world. It must be near his time.\'\n\n\'Past it rather,\' Peter answered, shutting up his book. \'But I think he\nhas walked a little slower than he used, these few last evenings,\nmother.\'\n\nThey were very quiet again. At last she said, and in a steady, cheerful\nvoice, that only faltered once:\n\n\'I have known him walk with--I have known him walk with Tiny Tim upon\nhis shoulder very fast indeed.\'\n\n\'And so have I,\' cried Peter. \'Often.\'\n\n\'And so have I,\' exclaimed another. So had all.\n\n\'But he was very light to carry,\' she resumed, intent upon her work,\n\'and his father loved him so, that it was no trouble, no trouble. And\nthere is your father at the door!\'\n\nShe hurried out to meet him; and little Bob in his comforter--he had\nneed of it, poor fellow--came in. His tea was ready for him on the hob,\nand they all tried who should help him to it most. Then the two young\nCratchits got upon his knees, and laid, each child, a little cheek\nagainst his face, as if they said, \'Don\'t mind it, father. Don\'t be\ngrieved!\'\n\nBob was very cheerful with them, and spoke pleasantly to all the family.\nHe looked at the work upon the table, and praised the industry and speed\nof Mrs. Cratchit and the girls. They would be done long before Sunday,\nhe said.\n\n\'Sunday! You went to-day, then, Robert?\' said his wife.\n\n\'Yes, my dear,\' returned Bob. \'I wish you could have gone. It would have\ndone you good to see how green a place it is. But you\'ll see it often. I\npromised him that I would walk there on a Sunday. My little, little\nchild!\' cried Bob. \'My little child!\'\n\nHe broke down all at once. He couldn\'t help it. If he could have helped\nit, he and his child would have been farther apart, perhaps, than they\nwere.\n\nHe left the room, and went upstairs into the room above, which was\nlighted cheerfully, and hung with Christmas. There was a chair set close\nbeside the child, and there were signs of some one having been there\nlately. Poor Bob sat down in it, and when he had thought a little and\ncomposed himself, he kissed the little face. He was reconciled to what\nhad happened, and went down again quite happy.\n\nThey drew about the fire, and talked, the girls and mother working\nstill. Bob told them of the extraordinary kindness of Mr. Scrooge\'s\nnephew, whom he had scarcely seen but once, and who, meeting him in the\nstreet that day, and seeing that he looked a little--\'just a little\ndown, you know,\' said Bob, inquired what had happened to distress him.\n\'On which,\' said Bob, \'for he is the pleasantest-spoken gentleman you\never heard, I told him. "I am heartily sorry for it, Mr. Cratchit," he\nsaid, "and heartily sorry for your good wife." By-the-bye, how he ever\nknew _that_ I don\'t know.\'\n\n\'Knew what, my dear?\'\n\n\'Why, that you were a good wife,\' replied Bob.\n\n\'Everybody knows that,\' said Peter.\n\n\'Very well observed, my boy!\' cried Bob. \'I hope they do. "Heartily\nsorry," he said, "for your good wife. If I can be of service to you in\nany way," he said, giving me his card, "that\'s where I live. Pray come\nto me."'}
16:35:51,999 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:52,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 7.670633084002475. input_tokens=34, output_tokens=171
16:35:56,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:56,405 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:57,969 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:57,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 4.051290625000547. input_tokens=34, output_tokens=88
16:36:15,452 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:15,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 9.047385834001034. input_tokens=34, output_tokens=152
16:36:19,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6462517920008395. input_tokens=171, output_tokens=80
16:36:19,558 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.0771052919990325. input_tokens=161, output_tokens=64
16:36:19,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.111007583000173. input_tokens=175, output_tokens=112
16:36:19,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.218211667001015. input_tokens=186, output_tokens=102
16:36:19,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.422390625000844. input_tokens=189, output_tokens=85
16:36:20,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.587705082998582. input_tokens=163, output_tokens=90
16:36:20,342 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.851693790998979. input_tokens=160, output_tokens=119
16:36:20,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.066203166999912. input_tokens=178, output_tokens=104
16:36:20,637 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.130069082999398. input_tokens=174, output_tokens=103
16:36:20,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.4808825830004935. input_tokens=186, output_tokens=78
16:36:21,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.990342624998448. input_tokens=183, output_tokens=85
16:36:21,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.006378292000591. input_tokens=164, output_tokens=88
16:36:21,652 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.171888500000932. input_tokens=173, output_tokens=93
16:36:21,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.324265999999625. input_tokens=178, output_tokens=117
16:36:22,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:22,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.412575166999886. input_tokens=170, output_tokens=124
16:36:24,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:24,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.84964279199994. input_tokens=191, output_tokens=143
16:36:32,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:32,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.488486958001886. input_tokens=380, output_tokens=339
16:37:01,271 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
16:37:01,272 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,275 datashaper.workflow.workflow INFO executing verb create_final_entities
16:37:01,280 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
16:37:01,409 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
16:37:01,409 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,413 datashaper.workflow.workflow INFO executing verb create_final_nodes
16:37:01,427 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
16:37:01,546 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
16:37:01,546 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,551 datashaper.workflow.workflow INFO executing verb create_final_communities
16:37:01,564 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
16:37:01,682 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
16:37:01,682 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,683 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
16:37:01,708 datashaper.workflow.workflow INFO executing verb create_final_relationships
16:37:01,714 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
16:37:01,830 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
16:37:01,830 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:37:01,833 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,833 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:37:01,840 datashaper.workflow.workflow INFO executing verb create_final_text_units
16:37:01,847 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
16:37:01,969 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_communities', 'create_final_nodes', 'create_final_entities']
16:37:01,970 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:37:01,972 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
16:37:01,974 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
16:37:01,976 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:37:01,983 datashaper.workflow.workflow INFO executing verb create_final_community_reports
16:37:01,987 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 59
16:37:02,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,936 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:02,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,940 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:02,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,981 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:02,985 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,986 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:04,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:04,457 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:04,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:04,561 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:04,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:04,825 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:05,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:05,107 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,519 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,590 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,592 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,777 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,778 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,912 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:12,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:12,735 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:12,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:12,898 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:13,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:13,298 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:22,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:22,632 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:22,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:22,919 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:23,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:23,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.941597334000107. input_tokens=2298, output_tokens=708
16:37:26,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:26,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.38557283300179. input_tokens=2485, output_tokens=786
16:37:26,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:26,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.69984545799889. input_tokens=2371, output_tokens=692
16:37:28,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:28,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.114666207999107. input_tokens=2058, output_tokens=500
16:37:37,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:37,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 25.071480375001556. input_tokens=2355, output_tokens=708
16:37:40,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:40,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.98843712500093. input_tokens=2478, output_tokens=781
16:37:42,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:42,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.32822229099838. input_tokens=4195, output_tokens=889
16:37:43,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:43,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 22.591396915999212. input_tokens=2217, output_tokens=459
16:37:57,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:57,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 24.40175087500029. input_tokens=2116, output_tokens=618
16:38:01,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:38:02,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 29.078852458002075. input_tokens=2368, output_tokens=678
16:38:02,14 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
16:38:02,148 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_documents', 'create_final_text_units', 'create_final_community_reports', 'create_final_relationships', 'create_final_entities']
16:38:02,150 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
16:38:02,153 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
16:38:02,155 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
16:38:02,157 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:38:02,159 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:38:02,167 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
16:38:02,168 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
16:38:02,168 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
16:38:02,170 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
16:38:02,187 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
16:38:02,187 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
16:38:02,191 graphrag.index.operations.embed_text.strategies.openai INFO embedding 59 inputs via 59 snippets using 4 batches. max_batch_size=16, max_tokens=8191
16:38:03,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,643 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,667 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5639834579997114. input_tokens=241, output_tokens=0
16:38:04,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9467990000011923. input_tokens=664, output_tokens=0
16:38:04,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0545979579983396. input_tokens=938, output_tokens=0
16:38:04,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1938268750018324. input_tokens=602, output_tokens=0
16:38:04,449 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
16:38:04,501 graphrag.index.operations.embed_text.strategies.openai INFO embedding 42 inputs via 42 snippets using 7 batches. max_batch_size=16, max_tokens=8191
16:38:05,158 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:05,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:05,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:05,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9701062919994001. input_tokens=7200, output_tokens=0
16:38:05,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1150982079998357. input_tokens=7200, output_tokens=0
16:38:05,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.26774637500057. input_tokens=7200, output_tokens=0
16:38:05,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5538726249978936. input_tokens=7055, output_tokens=0
16:38:06,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7801383749974775. input_tokens=7200, output_tokens=0
16:38:06,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9572205420008686. input_tokens=7200, output_tokens=0
16:38:06,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1077710409990686. input_tokens=7200, output_tokens=0
16:38:06,657 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
16:38:06,666 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 10 snippets using 1 batches. max_batch_size=16, max_tokens=8191
16:38:07,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:07,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7823621249990538. input_tokens=5526, output_tokens=0
16:38:07,505 graphrag.cli.index INFO All workflows completed successfully.
17:36:27,759 graphrag.cli.index INFO Logging enabled at /Users/apple/Documents/project/KG-RAG/ragtest/logs/indexing-engine.log
17:36:27,761 graphrag.cli.index INFO Starting pipeline run for: 20241127-173627, dry_run=False
17:36:27,761 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "deepseek-chat",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "https://api.deepseek.com/v1",
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/Users/apple/Documents/project/KG-RAG/ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "/Users/apple/Documents/project/KG-RAG/ragtest/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/Users/apple/Documents/project/KG-RAG/ragtest/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "deepseek-chat",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.deepseek.com/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "deepseek-chat",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.deepseek.com/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "deepseek-chat",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.deepseek.com/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "deepseek-chat",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "https://api.deepseek.com/v1",
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 3,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0.0,
        "local_search_top_p": 1.0,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 2000
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
17:36:27,762 graphrag.index.create_pipeline_config INFO skipping workflows 
17:36:27,762 graphrag.index.run.run INFO Running pipeline
17:36:27,762 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /Users/apple/Documents/project/KG-RAG/ragtest/output
17:36:27,763 graphrag.index.input.load_input INFO loading input from root_dir=input
17:36:27,763 graphrag.index.input.load_input INFO using file storage for input
17:36:27,763 graphrag.index.storage.file_pipeline_storage INFO search /Users/apple/Documents/project/KG-RAG/ragtest/input for files matching .*\.txt$
17:36:27,763 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
17:36:27,765 graphrag.index.input.text INFO Found 1 files, loading 1
17:36:27,766 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_final_documents', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'generate_text_embeddings']
17:36:27,766 graphrag.index.run.run INFO Final # of rows loaded: 1
17:36:27,806 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
17:36:27,808 datashaper.workflow.workflow INFO executing verb create_base_text_units
17:36:28,830 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_text_units']
17:36:28,830 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:36:28,833 datashaper.workflow.workflow INFO executing verb create_final_documents
17:36:28,838 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
17:36:28,914 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
17:36:28,914 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:36:28,918 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
17:36:28,921 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=https://api.deepseek.com/v1
17:36:28,945 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for deepseek-chat: TPM=0, RPM=0
17:36:28,945 graphrag.index.llm.load_llm INFO create concurrency limiter for deepseek-chat: 25
17:36:30,166 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,167 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,167 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,167 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,168 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,168 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,241 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,262 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,356 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,357 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,358 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,358 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,359 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,360 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,360 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,361 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,361 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,363 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,364 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,370 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,380 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,392 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,516 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:30,693 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:31,249 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:41,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.810336250000546. input_tokens=2936, output_tokens=209
17:36:42,294 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:42,599 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.603821665998112. input_tokens=2936, output_tokens=234
17:36:43,113 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:47,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.176131374999386. input_tokens=2936, output_tokens=346
17:36:47,614 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:50,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.550794916001905. input_tokens=2936, output_tokens=383
17:36:50,999 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:51,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.129905124998913. input_tokens=2936, output_tokens=393
17:36:51,713 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:52,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.90373270800046. input_tokens=2935, output_tokens=437
17:36:53,454 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:53,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.651886999999988. input_tokens=2936, output_tokens=454
17:36:54,172 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:55,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.0825518749989. input_tokens=2936, output_tokens=491
17:36:55,607 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:57,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.652290499998344. input_tokens=2937, output_tokens=500
17:36:57,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.754226249999192. input_tokens=2936, output_tokens=515
17:36:57,837 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.830304208000598. input_tokens=2936, output_tokens=533
17:36:58,68 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:58,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.106873832999554. input_tokens=2935, output_tokens=515
17:36:58,224 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:58,316 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:58,535 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:36:59,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.280072625002504. input_tokens=2935, output_tokens=584
17:36:59,709 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:00,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.347134791001736. input_tokens=2936, output_tokens=587
17:37:00,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.390887500001554. input_tokens=2936, output_tokens=576
17:37:00,845 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:00,847 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:00,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.890027458000986. input_tokens=2936, output_tokens=594
17:37:01,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.03223083299963. input_tokens=2936, output_tokens=586
17:37:01,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.13252450000073. input_tokens=2936, output_tokens=596
17:37:01,360 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:01,464 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:01,644 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:04,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.814999083999282. input_tokens=2936, output_tokens=415
17:37:04,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.93420962500022. input_tokens=2936, output_tokens=689
17:37:05,95 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:05,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.19216750000123. input_tokens=2936, output_tokens=702
17:37:05,376 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:05,714 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:06,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.15529025000069. input_tokens=2936, output_tokens=702
17:37:06,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.242218791001505. input_tokens=2935, output_tokens=724
17:37:06,616 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:06,729 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:08,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.260161124999286. input_tokens=34, output_tokens=128
17:37:08,588 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.604377874999045. input_tokens=2936, output_tokens=754
17:37:09,23 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:09,129 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:13,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.05727683299847. input_tokens=2935, output_tokens=322
17:37:13,769 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:13,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.83540066599744. input_tokens=2935, output_tokens=856
17:37:14,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 16.285944125000242. input_tokens=2936, output_tokens=308
17:37:14,325 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.79946583399942. input_tokens=2934, output_tokens=463
17:37:14,380 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:14,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.8113120420021. input_tokens=2935, output_tokens=584
17:37:14,790 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:14,815 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:15,16 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:15,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.477054417002364. input_tokens=2936, output_tokens=540
17:37:16,177 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:16,764 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.80777641700115. input_tokens=2935, output_tokens=924
17:37:17,301 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:18,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.712373500002286. input_tokens=2934, output_tokens=532
17:37:19,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.774799040998914. input_tokens=2936, output_tokens=359
17:37:19,390 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:19,680 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:20,368 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.01665095899807. input_tokens=2937, output_tokens=377
17:37:21,3 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.60225687500133. input_tokens=34, output_tokens=216
17:37:21,4 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:21,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.235589832998812. input_tokens=2936, output_tokens=424
17:37:21,504 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:21,937 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:23,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 9.258949416998803. input_tokens=34, output_tokens=162
17:37:23,888 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:24,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.102398999999423. input_tokens=2935, output_tokens=506
17:37:25,301 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:25,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 20.317182624999987. input_tokens=34, output_tokens=371
17:37:25,919 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.91577391700048. input_tokens=2790, output_tokens=453
17:37:26,130 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:26,532 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:27,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.96881787500024. input_tokens=34, output_tokens=358
17:37:28,66 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:29,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 12.357096666997677. input_tokens=34, output_tokens=228
17:37:29,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.49403287500172. input_tokens=2937, output_tokens=589
17:37:29,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.445084332997794. input_tokens=2935, output_tokens=533
17:37:29,663 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:29,727 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:29,798 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:31,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.76746616699893. input_tokens=34, output_tokens=331
17:37:32,561 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:34,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 13.525491292002698. input_tokens=34, output_tokens=222
17:37:34,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.49105300000156. input_tokens=2936, output_tokens=671
17:37:35,17 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:35,327 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:36,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.891818749998492. input_tokens=34, output_tokens=289
17:37:36,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.113188291998085. input_tokens=34, output_tokens=548
17:37:36,836 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:36,847 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:39,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.40969762500026. input_tokens=2936, output_tokens=890
17:37:39,639 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:40,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.64132833400072. input_tokens=34, output_tokens=695
17:37:41,171 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:41,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.36371304199929. input_tokens=34, output_tokens=698
17:37:42,108 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:42,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 19.126541000001453. input_tokens=34, output_tokens=345
17:37:42,794 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:46,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.492549582999345. input_tokens=34, output_tokens=477
17:37:47,421 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:48,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 22.726113458000327. input_tokens=34, output_tokens=406
17:37:48,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 7.256910457999766. input_tokens=34, output_tokens=122
17:37:48,854 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:49,468 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:49,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 8.97445629199865. input_tokens=34, output_tokens=150
17:37:50,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 15.523717249998299. input_tokens=34, output_tokens=273
17:37:50,182 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:51,315 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.419891749999806. input_tokens=2936, output_tokens=1094
17:37:51,904 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:37:52,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.62773133299925. input_tokens=34, output_tokens=917
17:37:53,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.544462791000115. input_tokens=34, output_tokens=457
17:37:54,697 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.876166957998066. input_tokens=34, output_tokens=684
17:37:54,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 18.449940167000022. input_tokens=34, output_tokens=331
17:37:54,840 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.279551042000094. input_tokens=34, output_tokens=512
17:37:54,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.612604458001442. input_tokens=34, output_tokens=491
17:37:55,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.89327979199879. input_tokens=34, output_tokens=750
17:38:02,676 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.55143083299845. input_tokens=34, output_tokens=642
17:38:03,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.893484750002244. input_tokens=34, output_tokens=720
17:38:05,770 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.57809529199949. input_tokens=34, output_tokens=430
17:38:09,442 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.32006029100012. input_tokens=34, output_tokens=940
17:38:10,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 23.22748008400231. input_tokens=34, output_tokens=446
17:38:10,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.77732570900116. input_tokens=34, output_tokens=1083
17:38:10,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.41440862499803. input_tokens=34, output_tokens=651
17:38:14,820 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:38:16,178 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:38:17,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.639070791999984. input_tokens=34, output_tokens=786
17:38:18,42 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:38:18,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.20310687499659. input_tokens=34, output_tokens=852
17:38:18,655 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:38:19,434 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 54.713059958001395. input_tokens=34, output_tokens=1051
17:38:27,217 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.186691165999946. input_tokens=34, output_tokens=935
17:38:27,769 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.52380808400267. input_tokens=34, output_tokens=743
17:38:38,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.37568899999678. input_tokens=34, output_tokens=963
17:38:51,906 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:38:52,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 63.44218137500138. input_tokens=34, output_tokens=1241
17:38:53,980 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:39:11,698 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 54.88784170800136. input_tokens=34, output_tokens=1035
17:39:18,641 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:39:22,463 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:39:53,171 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 59.74873624999964. input_tokens=34, output_tokens=1094
17:40:22,454 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:40:28,279 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:42:28,292 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:42:38,388 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:44:38,438 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:44:49,717 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:45:49,718 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:46:01,150 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:47:01,151 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:47:12,816 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:40,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 89.58825162500216. input_tokens=34, output_tokens=1709
17:48:41,237 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,747 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,755 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,781 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,788 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,880 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,880 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,880 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,881 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,881 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,882 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,946 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,953 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,965 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,967 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,969 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,971 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,972 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,977 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:41,978 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:42,75 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:42,76 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:42,77 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:42,78 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:42,94 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:43,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.9602816669976164. input_tokens=162, output_tokens=21
17:48:44,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.199795290998736. input_tokens=160, output_tokens=31
17:48:44,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.356982124998467. input_tokens=218, output_tokens=34
17:48:44,171 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,394 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7164644580007007. input_tokens=182, output_tokens=43
17:48:44,542 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.784760708000249. input_tokens=162, output_tokens=36
17:48:44,888 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.206552042000112. input_tokens=160, output_tokens=46
17:48:45,25 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,227 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.416423667000345. input_tokens=178, output_tokens=54
17:48:45,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.438169584002026. input_tokens=158, output_tokens=52
17:48:45,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.527507458002219. input_tokens=191, output_tokens=66
17:48:45,403 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.757311417000892. input_tokens=161, output_tokens=56
17:48:45,659 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,665 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,738 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:45,955 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.306342166997638. input_tokens=179, output_tokens=69
17:48:46,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.5189484160000575. input_tokens=232, output_tokens=69
17:48:46,598 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,712 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:46,864 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.038904041000933. input_tokens=214, output_tokens=75
17:48:47,277 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6655462919989077. input_tokens=156, output_tokens=25
17:48:47,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.8431742089996987. input_tokens=159, output_tokens=34
17:48:47,478 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.672041833000549. input_tokens=198, output_tokens=104
17:48:47,667 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.839113541998813. input_tokens=243, output_tokens=99
17:48:47,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.880154083999514. input_tokens=202, output_tokens=99
17:48:47,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.472855874999368. input_tokens=155, output_tokens=27
17:48:47,729 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.2143723750014033. input_tokens=150, output_tokens=21
17:48:47,848 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:47,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.6059918329992797. input_tokens=161, output_tokens=31
17:48:48,79 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,166 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,167 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,260 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.443206958996598. input_tokens=226, output_tokens=114
17:48:48,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.096911333999742. input_tokens=154, output_tokens=33
17:48:48,476 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,695 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.933755666999787. input_tokens=223, output_tokens=119
17:48:48,827 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:48,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.5089174579989049. input_tokens=138, output_tokens=14
17:48:49,219 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,392 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:49,930 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.095808499998384. input_tokens=212, output_tokens=138
17:48:49,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.910559790998377. input_tokens=206, output_tokens=69
17:48:50,297 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:50,326 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:50,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.386064083002566. input_tokens=178, output_tokens=104
17:48:50,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.694161000003078. input_tokens=284, output_tokens=149
17:48:50,792 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:50,941 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.7720632910022687. input_tokens=163, output_tokens=46
17:48:51,92 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.818007709000085. input_tokens=156, output_tokens=32
17:48:51,129 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.957355832997564. input_tokens=206, output_tokens=112
17:48:51,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5651779580002767. input_tokens=171, output_tokens=54
17:48:51,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.503825624997262. input_tokens=171, output_tokens=45
17:48:51,429 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,490 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,512 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.818972749999375. input_tokens=190, output_tokens=126
17:48:51,746 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,841 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:51,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.023692124999798. input_tokens=225, output_tokens=175
17:48:51,969 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.265137999998842. input_tokens=206, output_tokens=176
17:48:52,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.430446834001486. input_tokens=178, output_tokens=59
17:48:52,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.988120209000044. input_tokens=240, output_tokens=88
17:48:52,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.529308584002138. input_tokens=183, output_tokens=60
17:48:52,266 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.0555173749999085. input_tokens=198, output_tokens=95
17:48:52,492 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,520 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,522 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,599 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:52,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.93012316700333. input_tokens=184, output_tokens=76
17:48:52,766 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.05021774999841. input_tokens=196, output_tokens=58
17:48:53,77 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,99 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.33119479099696. input_tokens=182, output_tokens=63
17:48:53,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.769421125001827. input_tokens=190, output_tokens=94
17:48:53,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.484051957999327. input_tokens=349, output_tokens=208
17:48:53,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4280962079974415. input_tokens=174, output_tokens=44
17:48:53,411 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,501 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,726 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,755 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.7052282500008005. input_tokens=214, output_tokens=81
17:48:54,134 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.209488583001075. input_tokens=188, output_tokens=54
17:48:54,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.32367133400112. input_tokens=364, output_tokens=208
17:48:54,537 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,539 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,544 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:54,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.801937832999101. input_tokens=284, output_tokens=129
17:48:54,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.696713875000569. input_tokens=168, output_tokens=27
17:48:54,912 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.8182868330004567. input_tokens=182, output_tokens=54
17:48:55,80 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:55,240 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:55,282 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:55,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.1505004999999073. input_tokens=163, output_tokens=19
17:48:55,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.2597214999987045. input_tokens=166, output_tokens=65
17:48:55,768 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:55,779 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:56,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.6636324579994834. input_tokens=173, output_tokens=83
17:48:56,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.133046209000895. input_tokens=159, output_tokens=77
17:48:56,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.930652833001659. input_tokens=163, output_tokens=70
17:48:56,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.122040791000472. input_tokens=163, output_tokens=58
17:48:56,549 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.915800875001878. input_tokens=192, output_tokens=53
17:48:56,678 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:56,685 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:56,858 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:56,877 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:56,968 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:56,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.611460583000735. input_tokens=161, output_tokens=47
17:48:57,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.650251084000047. input_tokens=181, output_tokens=83
17:48:57,88 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.675821166998503. input_tokens=185, output_tokens=102
17:48:57,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.067242791999888. input_tokens=169, output_tokens=70
17:48:57,359 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:48:57,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.631940125000256. input_tokens=164, output_tokens=70
17:48:57,985 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.376211666000017. input_tokens=207, output_tokens=104
17:48:58,109 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.4400249999998778. input_tokens=160, output_tokens=46
17:48:58,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5198646249991725. input_tokens=184, output_tokens=40
17:48:58,692 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.300743792002322. input_tokens=169, output_tokens=42
17:48:58,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.65085666700179. input_tokens=217, output_tokens=108
17:48:58,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5740087919984944. input_tokens=171, output_tokens=53
17:48:58,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.891783499999292. input_tokens=217, output_tokens=96
17:48:58,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.394779334001214. input_tokens=150, output_tokens=20
17:48:58,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.702245832999324. input_tokens=177, output_tokens=24
17:48:59,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.311552708000818. input_tokens=177, output_tokens=83
17:48:59,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.376844708000135. input_tokens=258, output_tokens=119
17:48:59,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.048536665999563. input_tokens=163, output_tokens=70
17:49:00,23 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.175865458000771. input_tokens=243, output_tokens=130
17:49:00,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.89203683300002. input_tokens=172, output_tokens=84
17:49:00,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.5009482499990554. input_tokens=173, output_tokens=45
17:49:00,694 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.179732458000217. input_tokens=176, output_tokens=57
17:49:01,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.229354040999169. input_tokens=171, output_tokens=63
17:49:01,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.424147625002661. input_tokens=191, output_tokens=83
17:49:02,222 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.07471520800027. input_tokens=170, output_tokens=129
17:49:02,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 21.53942462499981. input_tokens=944, output_tokens=376
17:49:04,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.970329959000082. input_tokens=255, output_tokens=186
17:49:07,578 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:49:07,579 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:07,583 datashaper.workflow.workflow INFO executing verb create_final_entities
17:49:07,613 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:49:07,717 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:49:07,717 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:07,721 datashaper.workflow.workflow INFO executing verb create_final_nodes
17:49:07,851 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:49:07,955 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:49:07,956 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:07,960 datashaper.workflow.workflow INFO executing verb create_final_communities
17:49:08,25 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:49:08,144 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
17:49:08,144 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:08,144 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:49:08,172 datashaper.workflow.workflow INFO executing verb create_final_relationships
17:49:08,201 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:49:08,324 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_final_relationships', 'create_base_text_units']
17:49:08,339 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:49:08,345 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:49:08,353 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
17:49:08,359 datashaper.workflow.workflow INFO executing verb create_final_text_units
17:49:08,370 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:49:08,494 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_communities', 'create_final_nodes', 'create_final_entities']
17:49:08,495 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:49:08,497 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
17:49:08,499 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
17:49:08,501 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:49:08,509 datashaper.workflow.workflow INFO executing verb create_final_community_reports
17:49:08,519 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=2 => 69
17:49:08,535 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=1 => 249
17:49:08,585 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 290
17:49:09,121 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:49:09,596 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:49:09,660 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:49:10,26 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:49:34,622 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.000997082999675. input_tokens=2124, output_tokens=426
17:49:45,84 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.45130091699684. input_tokens=2507, output_tokens=604
17:49:50,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.394168749997334. input_tokens=2548, output_tokens=710
17:50:10,24 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n33,SCROOGE,"Ebenezer Scrooge is the central character in the story, initially portrayed as a miserly and grumpy old man, particularly in his attitude towards Christmas. He is described as a squeezing, wrenching, grasping, scraping, clutching, covetous old sinner, who is secretive, self-contained, and solitary, carrying his own low temperature with him even in the coldest weather. Scrooge is known for his harsh and unfriendly demeanor, especially during the festive season. He is a wealthy man who dislikes Christmas and is visited by Marley\'s Ghost, which begins his journey of transformation.\n\nScrooge\'s initial skepticism about ghosts is dispelled as he encounters the Ghost of Christmas Past, the Ghost of Christmas Present, and the Ghost of Christmas Yet to Come. These spirits guide him through various scenes of Christmas celebrations, his past life, and potential future, prompting him to reflect deeply on his choices, wealth, and relationships. He witnesses interactions with characters like Old Joe and reflects on the significance of Christmas, his time as an apprentice under Old Fezziwig, and the impact of his actions on others, particularly Tiny Tim.\n\nThroughout his encounters, Scrooge undergoes a profound transformation from a cold and unfeeling miser to a generous and joyful individual. He becomes a good friend and master to Bob Cratchit and Tiny Tim, celebrating Christmas with great enthusiasm and reflecting on his own life. Scrooge\'s change is marked by his participation in festive activities, such as playing games and watching Christmas balls, and his newfound cheerfulness and light-heartedness.\n\nIn summary, Ebenezer Scrooge is a character who, after being visited by three spirits, transforms from a miserly and grumpy old man into a generous and joyful individual, embracing the true meaning of Christmas and changing his ways to become a better person.",111\n16,GHOST OF CHRISTMAS PAST,"The Ghost of Christmas Past is a spirit and apparition featured in Charles Dickens\' novella ""A Christmas Carol."" This ethereal being visits Ebenezer Scrooge to guide him through scenes from his past, specifically focusing on his experiences during Christmas. The Ghost of Christmas Past aims to aid Scrooge in his reclamation and transformation by revealing moments that have shaped his character and values. Through these poignant recollections, the spirit seeks to inspire Scrooge to embrace the spirit of Christmas and change his ways.",4\n66,CAMDEN TOWN,Camden Town is a location where Scrooge intends to send the turkey and is also the place where Scrooge\'s clerk runs home to play blind man\'s-buff.,2\n100,CHURCH,"The CHURCH is a significant location in the narrative, where Mr. Scrooge frequently goes to listen to the chimes, which serve as a reminder of the passage of time. This church not only provides Scrooge with a sense of temporal awareness but also becomes a place where he goes to reflect and find pleasure during the holiday season. The chimes heard at the church play a crucial role in Scrooge\'s journey, symbolizing the importance of time and the joy he discovers in the festive period.",2\n18,GHOST OF CHRISTMAS YET TO COME,"The Ghost of Christmas Yet to Come is a supernatural entity and an apparition in ""A Christmas Carol"" who shows Scrooge visions of the future, specifically focusing on his own grave and other possible future events.",2\n141,POVERTY,,2\n40,TWO PORTLY GENTLEMEN,Two portly gentlemen are characters who visit Scrooge\'s office to discuss charitable donations during Christmas,2\n93,TOOTHPICK,"The toothpick is an object mentioned in the story, used by Scrooge to divert Marley\'s Ghost\'s attention",2\n257,SPIRIT OF CHRISTMAS YET TO COME,The Spirit of Christmas Yet to Come is one of the three spirits that visit Scrooge to show him a bleak future if he does not change his ways,2\n151,ADJOINING ROOM,The adjoining room is where Scrooge suspects the source of the ghostly light might be,1\n105,ALI BABA,Ali Baba is a character from a story who appears to Scrooge during his reflection,1\n70,BANKER\'S BOOK,Scrooge beguiles the rest of the evening with his banker\'s book,1\n101,BED,Scrooge\'s bed where he awakens and later tries to stay awake to avoid Marley\'s Ghost,1\n41,BEDLAM,,1\n210,BOARDING-SCHOOL,"The boarding-school is where Scrooge was sent as a child, as reminded by the Ghost of Christmas Past",1\n67,CHAMBERS,"Chambers are the gloomy rooms where Scrooge lives, once belonging to his deceased partner Marley",1\n104,CHIMES,The sound of the church chimes that Scrooge uses to determine the time,1\n266,CHRISTMAS BOWL OF SMOKING BISHOP,A Christmas event where Scrooge and Bob Cratchit discuss Bob\'s affairs,1\n121,CHRISTMAS CAROL,,1\n142,CHRISTMAS CELEBRATION,A family is celebrating Christmas with joy and excitement,1\n26,CHRISTMAS TOYS AND PRESENTS,"Christmas toys and presents are items typically brought by a man, often associated with the figure of Santa Claus, to families during a Christmas celebration. These items are frequently described as being laden with joy and excitement, symbolizing the spirit of giving and the festive season.",1\n102,CLOCK,A clock that Scrooge checks to ensure it is not malfunctioning,1\n108,DAMASCUS,Damascus is a city mentioned in a story involving a character put down in his drawers,1\n73,DUTCH MERCHANT,The Dutch merchant is the person who built the old fireplace in Scrooge\'s house,1\n85,FISH BASKETS,Fish baskets are mentioned as part of the lumber-room in Scrooge\'s house,1\n113,FRIDAY,"Friday is a character mentioned by Scrooge, running for his life",1\n138,GAIN,"Gain refers to the pursuit of wealth and material success, which becomes the master passion for Scrooge and alters his nature",1\n71,GATEWAY,"The gateway is the entrance to Scrooge\'s house, where the fog and frost hang",1\n110,GENII,"Genii is a character mentioned by Scrooge, responsible for turning the Sultan\'s Groom upside down",1\n259,JOE MILLER,Joe Miller is a character known for making jokes,1\n213,LONDON,London is the location where the animal in the game Yes and No lives,1\n190,OGRE,The Ogre is a metaphor used to describe Scrooge\'s negative impact on the Cratchit family,1\n83,OLD FIRE-GUARD,Old fire-guard is mentioned as part of the lumber-room in Scrooge\'s house,1\n84,OLD SHOES,Old shoes are mentioned as part of the lumber-room in Scrooge\'s house,1\n107,ORSON,"Orson is a character mentioned by Scrooge, possibly Valentine\'s brother",1\n111,PARROT,"The Parrot is a character mentioned by Scrooge, associated with Robin Crusoe",1\n87,POKER,Poker is mentioned as part of the lumber-room in Scrooge\'s house,1\n258,POULTERER\'S,The poulterer\'s is a shop where Scrooge buys a turkey,1\n221,PRISONS,Prisons are places where criminals are incarcerated,1\n112,ROBIN CRUSOE,"Robin Crusoe is a character mentioned by Scrooge, associated with the Parrot",1\n74,SCRIPTURES,The Scriptures are mentioned as being illustrated on the Dutch tiles around the fireplace,1\n39,SCROOGE AND MARLEY\'S,"Scrooge and Marley\'s is the name of the business where Scrooge works, which is mentioned as having a deceased partner named Marley.",1\n69,TAVERN,Scrooge takes his melancholy dinner in the tavern,1\n82,WINE-MERCHANT\'S CELLAR,The wine-merchant\'s cellar is mentioned as the location where the clanking noise is heard,1\n86,WASHING-STAND,Washing-stand is mentioned as part of the lumber-room in Scrooge\'s house,1\n103,WINDOW,Scrooge\'s window that he tries to see through after rubbing off frost,1\n106,VALENTINE,"Valentine is a character mentioned by Scrooge, possibly from a story",1\n109,SULTAN\'S GROOM,"The Sultan\'s Groom is a character mentioned by Scrooge, turned upside down by the Genii",1\n137,WEALTH,"Wealth is the state of being rich, which is a goal and a source of conflict in the text as characters pursue it at the expense of relationships",1\n150,SPONTANEOUS COMBUSTION,Spontaneous combustion is a phenomenon Scrooge fears might be happening to him due to the strange light,1\n211,SEXTON\'S SPADE,"The sexton\'s spade is an event or tool mentioned in the context of burying Jacob Marley, which Scrooge reflects on",1\n212,WHITECHAPEL,Whitechapel is a location mentioned in the text where the sharpest needle is from,1\n220,WORKHOUSES,Workhouses are places where the poor and needy were sent for labor and support,1\n235,UNNAMED MAN,The unnamed man is a deceased individual whose belongings are being sold by the characters,1\n255,SPIRITS,"The Spirits represent the past, present, and future Christmas spirits that visit Scrooge to show him the error of his ways",1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,combined_degree\n5,PROJECT GUTENBERG,SCROOGE,Project Gutenberg distributes the story of Scrooge,123\n42,GHOST OF CHRISTMAS PAST,SCROOGE,"During the event of Christmas Past, Scrooge interacts with the Ghost of Christmas Past, who shows him the Christmas ball. The Ghost of Christmas Past then proceeds to reveal to Scrooge his own past, utilizing the Christmas ball as a focal point for these memories.",115\n108,SCROOGE,THREE SPIRITS,Scrooge is visited by the Three Spirits to show him the consequences of his actions,115\n153,SCROOGE,CHRISTMAS PAST,"Scrooge is visited by the Ghost of Christmas Past to show him memories of his past, which are triggered by the music played by his niece during Christmas. This experience allows Scrooge to reflect deeply on his past and the events that have shaped his life.",114\n160,SCROOGE,CHRISTMAS YET TO COME,Scrooge is visited by the Ghost of Christmas Yet to Come to show him his future,114\n88,SCROOGE,CAMDEN TOWN,"Scrooge, known for his generous intentions, plans to send a turkey to Camden Town, a place where his clerk also resides. On Christmas Eve, Scrooge\'s clerk returns to Camden Town to engage in the festive activity of playing blind man\'s-buff with friends and family.",113\n109,SCROOGE,CHURCH,Scrooge listens to the chimes of a nearby or neighboring church to determine the time.,113\n52,GHOST OF CHRISTMAS YET TO COME,SCROOGE,"Scrooge interacts with the Ghost of Christmas Yet to Come, who shows him visions of his future grave",113\n136,SCROOGE,POVERTY,Scrooge reflects on his past poverty and how it influenced his choices and relationships,113\n83,SCROOGE,TWO PORTLY GENTLEMEN,Scrooge interacts with the two portly gentlemen who come to solicit charitable donations,113\n105,SCROOGE,TOOTHPICK,Scrooge uses the toothpick to divert Marley\'s Ghost\'s attention,113\n176,SCROOGE,SPIRIT OF CHRISTMAS YET TO COME,Scrooge is shown a bleak future if he does not change his ways by the Spirit of Christmas Yet to Come,113\n144,SCROOGE,ADJOINING ROOM,Scrooge investigates the adjoining room to find the source of the ghostly light,112\n114,SCROOGE,ALI BABA,"Scrooge reminisces about Ali Baba, a character from a story",112\n91,SCROOGE,BANKER\'S BOOK,Scrooge beguiles the rest of the evening with his banker\'s book,112\n110,SCROOGE,BED,Scrooge awakens in his bed and later tries to stay awake in it,112\n84,SCROOGE,BEDLAM,"Scrooge mentions Bedlam as a place he might retire to, indicating his frustration with the Christmas spirit",112\n154,SCROOGE,BOARDING-SCHOOL,"Scrooge was sent to the boarding-school as a child, as reminded by the Ghost of Christmas Past",112\n86,SCROOGE,CHAMBERS,Scrooge lives in the gloomy chambers that once belonged to Marley,112\n113,SCROOGE,CHIMES,Scrooge uses the sound of the church chimes to determine the time,112\n179,SCROOGE,CHRISTMAS BOWL OF SMOKING BISHOP,Scrooge and Bob Cratchit discuss Bob\'s affairs over a Christmas bowl of smoking bishop,112\n128,SCROOGE,CHRISTMAS CAROL,Scrooge experiences a Christmas carol sung by a boy at his door,112\n141,SCROOGE,CHRISTMAS CELEBRATION,Scrooge is reflecting on a Christmas celebration he is observing,112\n74,CHRISTMAS TOYS AND PRESENTS,SCROOGE,Scrooge observes a man bringing Christmas toys and presents to a family,112\n111,SCROOGE,CLOCK,Scrooge checks his clock to ensure it is not malfunctioning,112\n117,SCROOGE,DAMASCUS,Scrooge mentions Damascus in relation to a story character,112\n94,SCROOGE,DUTCH MERCHANT,Scrooge\'s house has a fireplace built by a Dutch merchant,112\n99,SCROOGE,FISH BASKETS,Scrooge\'s lumber-room contains fish baskets,112\n122,SCROOGE,FRIDAY,"Scrooge mentions Friday, a character from a story",112\n138,SCROOGE,GAIN,"Scrooge\'s focus on Gain, or the pursuit of wealth, alters his spirit and affects his relationships",112\n92,SCROOGE,GATEWAY,Scrooge\'s house has a gateway where the fog and frost hang,112\n119,SCROOGE,GENII,"Scrooge mentions the Genii, a character from a story",112\n178,SCROOGE,JOE MILLER,Scrooge references Joe Miller\'s jokes in his thoughts,112\n157,SCROOGE,LONDON,Scrooge participates in a game where the animal lives in London,112\n151,SCROOGE,OGRE,The Ogre metaphor describes Scrooge\'s negative impact on the Cratchit family,112\n97,SCROOGE,OLD FIRE-GUARD,Scrooge\'s lumber-room contains an old fire-guard,112\n98,SCROOGE,OLD SHOES,Scrooge\'s lumber-room contains old shoes,112\n116,SCROOGE,ORSON,"Scrooge mentions Orson, a character from a story",112\n120,SCROOGE,PARROT,"Scrooge mentions the Parrot, a character from a story",112\n101,SCROOGE,POKER,Scrooge\'s lumber-room contains a poker,112\n177,SCROOGE,POULTERER\'S,Scrooge buys a turkey from the poulterer\'s shop,112\n162,SCROOGE,PRISONS,"The Spirit questions Scrooge about the existence of prisons, a place for criminals",112\n121,SCROOGE,ROBIN CRUSOE,"Scrooge mentions Robin Crusoe, a character from a story",112\n95,SCROOGE,SCRIPTURES,Scrooge\'s fireplace is decorated with illustrations of the Scriptures,112\n82,SCROOGE,SCROOGE AND MARLEY\'S,"Scrooge is a surviving partner in the business of Scrooge and Marley\'s, where he works at the counting-house.",112\n90,SCROOGE,TAVERN,Scrooge takes his melancholy dinner in the tavern,112\n96,SCROOGE,WINE-MERCHANT\'S CELLAR,Scrooge hears noises coming from the wine-merchant\'s cellar,112\n100,SCROOGE,WASHING-STAND,Scrooge\'s lumber-room contains a washing-stand,112\n112,SCROOGE,WINDOW,Scrooge tries to see through his window after rubbing off frost,112\n115,SCROOGE,VALENTINE,"Scrooge mentions Valentine, a character from a story",112\n118,SCROOGE,SULTAN\'S GROOM,"Scrooge mentions the Sultan\'s Groom, a character from a story",112\n137,SCROOGE,WEALTH,"Scrooge\'s pursuit of wealth becomes his master passion, leading to changes in his nature and relationships",112\n143,SCROOGE,SPONTANEOUS COMBUSTION,Scrooge fears he might be experiencing spontaneous combustion due to the strange light,112\n155,SCROOGE,SEXTON\'S SPADE,Scrooge reflects on the sexton\'s spade that buried Jacob Marley,112\n156,SCROOGE,WHITECHAPEL,Scrooge is compared to the sharpest needle from Whitechapel,112\n161,SCROOGE,WORKHOUSES,"The Spirit questions Scrooge about the existence of workhouses, a place for the poor",112\n173,SCROOGE,UNNAMED MAN,Scrooge witnesses the selling of the unnamed man\'s belongings,112\n174,SCROOGE,SPIRITS,Scrooge\'s interactions with the spirits lead to his transformation and renewed appreciation for life and Christmas,112\n43,GHOST OF CHRISTMAS PAST,CHRISTMAS PAST,The Ghost of Christmas Past is the spirit associated with the event of Christmas Past,7\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
17:50:12,769 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:09,51 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 57.88070249999873. input_tokens=5921, output_tokens=950
17:51:09,662 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,184 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,185 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,186 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,186 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,187 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,187 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,221 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,221 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,226 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,236 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,258 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,261 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,271 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,289 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,293 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,327 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,353 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,517 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,519 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,520 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,521 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,522 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,719 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:10,765 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:36,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.013915207997343. input_tokens=2150, output_tokens=420
17:51:36,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 27.15808662500058. input_tokens=2123, output_tokens=448
17:51:36,599 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:36,785 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:39,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.96151579200159. input_tokens=2189, output_tokens=472
17:51:39,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 29.952705083000183. input_tokens=2165, output_tokens=460
17:51:39,744 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:39,932 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:43,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 34.804123542002344. input_tokens=2143, output_tokens=569
17:51:44,471 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:45,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.24317562500073. input_tokens=2123, output_tokens=572
17:51:45,795 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:45,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.73822825000025. input_tokens=2480, output_tokens=611
17:51:46,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 36.930631707997236. input_tokens=2347, output_tokens=601
17:51:46,432 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:46,516 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:48,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.85271612499855. input_tokens=2840, output_tokens=642
17:51:48,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.297281457998906. input_tokens=2159, output_tokens=638
17:51:48,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.38640058300007. input_tokens=2284, output_tokens=644
17:51:48,500 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:51:49,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.30254995900032. input_tokens=2219, output_tokens=677
17:51:49,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.797569291997206. input_tokens=2950, output_tokens=688
17:51:50,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.716909457998554. input_tokens=2351, output_tokens=650
17:51:51,7 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.909870542000135. input_tokens=2487, output_tokens=700
17:51:51,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 42.33327504200133. input_tokens=2718, output_tokens=687
17:51:54,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 44.96717704099865. input_tokens=2442, output_tokens=727
17:51:55,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.95981825000126. input_tokens=2571, output_tokens=736
17:51:55,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.19446095799867. input_tokens=2974, output_tokens=784
17:51:57,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 48.195291874999384. input_tokens=3119, output_tokens=774
17:51:57,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 48.20822354200209. input_tokens=2756, output_tokens=806
17:52:01,198 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 52.07604979200187. input_tokens=3025, output_tokens=816
17:52:02,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 53.13691004199791. input_tokens=3083, output_tokens=875
17:52:05,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 56.51727824999762. input_tokens=2959, output_tokens=919
17:52:08,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 59.42098837500089. input_tokens=3998, output_tokens=974
17:52:16,441 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 30.383568000001105. input_tokens=2141, output_tokens=484
17:52:18,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 42.411432375000004. input_tokens=2791, output_tokens=705
17:52:18,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 42.54061816600006. input_tokens=2617, output_tokens=701
17:52:21,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 41.981573625002056. input_tokens=2492, output_tokens=702
17:52:23,993 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.64747841699864. input_tokens=2247, output_tokens=649
17:52:26,216 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 38.20517841699984. input_tokens=2532, output_tokens=642
17:52:29,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.52507079199859. input_tokens=2765, output_tokens=756
17:52:35,10 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 55.91355345800184. input_tokens=9523, output_tokens=973
17:52:46,435 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n94,JACOB MARLEY,"Jacob Marley is a deceased business partner of Scrooge, who, after his death, appears as a ghost to warn Scrooge about his past, present, and future. Marley was buried by the sexton\'s spade, a fact that Scrooge reflects upon. Condemned to wander the earth as a ghost due to his neglect of charity and benevolence in life, Marley sends spirits to visit Scrooge in an effort to guide him towards redemption.",8\n95,GHOST,"The Ghost is a supernatural entity that appears to Scrooge, representing his deceased business partner Jacob Marley. This spectral figure serves as a warning to Scrooge about the consequences of his worldly behavior. Throughout the night, the Ghost accompanies Scrooge on a transformative journey, during which it grows older, symbolizing the passage of time. On this journey, the Ghost takes Scrooge to witness various Christmas scenes, illustrating the impact of his actions and choices. Through these experiences, the Ghost aims to show Scrooge the dire consequences of his current path and the potential for redemption.",7\n68,CLERK,The clerk works in the counting-house with Scrooge and celebrates Christmas Eve,5\n174,SPIRIT OF CHRISTMAS PRESENT,The Spirit of Christmas Present is a supernatural being who shows Scrooge the joys and struggles of Christmas.,4\n64,COUNTING-HOUSE,"The COUNTING-HOUSE is the central location where Scrooge and Marley conducted their business operations. It is the place where Scrooge works and interacts with his clerk on a daily basis. Notably, it is also the site where Marley\'s spirit was confined due to his neglect of charity and benevolence during his lifetime.",3\n65,CORNHILL,Cornhill is a location where Scrooge\'s clerk slides down a slide on Christmas Eve,2\n256,SPIRIT OF CHRISTMAS PAST,The Spirit of Christmas Past is one of the three spirits that visit Scrooge to show him his past and the missed opportunities for joy and kindness,2\n219,OLD JACOB MARLEY,Old Jacob Marley refers to the prediction made by Jacob Marley\'s ghost about the future visitations of spirits,1\n178,SUBURBS OF THE TOWN,The suburbs of the town is where the Ghost and Scrooge visit after leaving the baker\'s shops,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,combined_degree\n107,SCROOGE,JACOB MARLEY,"In the narrative, Scrooge, a character deeply entrenched in his miserly ways, is profoundly impacted by a series of supernatural visits. The pivotal moment occurs when the ghost of his deceased business partner, Jacob Marley, appears to Scrooge. Marley\'s ghost serves as a harbinger of warning, alerting Scrooge to the dire consequences of his current path. This visit is not isolated; it is part of a series of spiritual interventions orchestrated by Marley to guide Scrooge towards redemption. Scrooge, initially shaken and moved by Marley\'s ghostly presence, begins to reflect deeply on his past relationship with Marley, the lessons imparted by the Ghost, and even the somber memory of Marley\'s burial, marked by the sexton\'s spade. These experiences collectively prompt Scrooge to vow a significant change in his ways, marking a turning point in his character arc.",119\n106,SCROOGE,GHOST,"Scrooge, accompanied by the Ghost, embarks on a transformative journey where the Ghost, revealed to be his deceased business partner Jacob Marley, confronts him about his neglect of charity and benevolence. Throughout their travels, the Ghost teaches Scrooge his precepts and shows him the consequences of his past actions and choices by taking him to witness various Christmas scenes. This experience serves as a profound lesson for Scrooge, highlighting the importance of compassion and generosity.",118\n89,SCROOGE,CLERK,Scrooge interacts with his clerk in the counting-house,116\n146,SCROOGE,SPIRIT OF CHRISTMAS PRESENT,"In the timeless tale, Scrooge, a character known for his miserly ways, encounters the Spirit of Christmas Present. Through this interaction, Scrooge is guided to learn about the true meaning of Christmas. The Spirit of Christmas Present reveals to Scrooge the joys and warmth that Christmas brings to people in the present, helping him to understand and appreciate the festive spirit in a way he had never before experienced.",115\n85,SCROOGE,COUNTING-HOUSE,"Scrooge, a prominent figure in the counting-house, reflects on his time working alongside Jacob Marley and the profound lessons he learned from the Ghost. During his tenure in the counting-house, Scrooge not only managed his own responsibilities but also interacted closely with his clerk, fostering a dynamic work environment that shaped his perspectives and future actions.",114\n87,SCROOGE,CORNHILL,Scrooge\'s clerk slides down Cornhill on Christmas Eve,113\n175,SCROOGE,SPIRIT OF CHRISTMAS PAST,Scrooge is shown his past and the missed opportunities for joy and kindness by the Spirit of Christmas Past,113\n24,BOB CRATCHIT,SPIRIT OF CHRISTMAS PRESENT,The Spirit of Christmas Present blesses Bob Cratchit\'s home with his torch,21\n239,JACOB MARLEY,GHOST,"The Ghost is the spirit of Jacob Marley, who appears to Scrooge to convey a message about the consequences of his worldly behavior",15\n235,MARLEY\'S GHOST,JACOB MARLEY,Jacob Marley\'s spirit appears as Marley\'s Ghost to Scrooge,15\n186,CHRISTMAS EVE,CLERK,The clerk celebrates Christmas Eve,14\n246,GHOST,LIGHT HOUSE,The Ghost takes Scrooge to a lighthouse where two men celebrate Christmas,12\n242,JACOB MARLEY,SPIRIT OF CHRISTMAS PRESENT,Jacob Marley\'s ghost sets the stage for the visit of the Spirit of Christmas Present,12\n230,COUNTING-HOUSE,JACOB MARLEY,Jacob Marley\'s spirit was confined to the counting-house due to his neglect of charity and benevolence in life,11\n244,GHOST,MINERS,The Ghost shows Scrooge a scene where miners are celebrating Christmas,11\n245,GHOST,MOOR,The Ghost takes Scrooge to a bleak moor where miners live,10\n247,GHOST,SHIP,The Ghost takes Scrooge to a ship at sea where the crew celebrates Christmas,10\n248,GHOST,TWELFTH-NIGHT PARTY,The Ghost and Scrooge attend a children\'s Twelfth-Night party,10\n241,JACOB MARLEY,SPIRIT OF CHRISTMAS PAST,Jacob Marley\'s ghost sets the stage for the visit of the Spirit of Christmas Past,10\n243,JACOB MARLEY,SPIRIT OF CHRISTMAS YET TO COME,Jacob Marley\'s ghost sets the stage for the visit of the Spirit of Christmas Yet to Come,10\n240,JACOB MARLEY,OLD JACOB MARLEY,Jacob Marley\'s ghost makes a prediction about the future visitations of spirits,9\n229,COUNTING-HOUSE,CLERK,The clerk works in the counting-house with Scrooge,8\n231,CORNHILL,CLERK,The clerk slides down Cornhill on Christmas Eve,7\n232,CAMDEN TOWN,CLERK,The clerk runs home to Camden Town to play blind man\'s-buff on Christmas Eve,7\n309,SPIRIT OF CHRISTMAS PRESENT,SUBURBS OF THE TOWN,The Spirit of Christmas Present and Scrooge visit the suburbs of the town after leaving the baker\'s shops,5\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
17:52:48,987 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:46,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 1 retries took 58.72769920800056. input_tokens=3582, output_tokens=1000
17:53:46,970 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,419 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,577 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,617 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,623 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,631 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,639 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,791 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,819 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:53:47,857 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:54:24,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.61853466699904. input_tokens=2513, output_tokens=612
17:54:26,324 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.89776500000153. input_tokens=2978, output_tokens=680
17:54:26,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 39.99085495899999. input_tokens=4143, output_tokens=655
17:54:31,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 45.396996667001076. input_tokens=3842, output_tokens=759
17:54:32,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 46.03600212500169. input_tokens=5148, output_tokens=772
17:54:35,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 48.994927125000686. input_tokens=3421, output_tokens=805
17:54:35,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 49.130141708003066. input_tokens=4608, output_tokens=756
17:54:43,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 56.85812358399926. input_tokens=6864, output_tokens=966
17:54:46,793 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 60.34920591699847. input_tokens=4167, output_tokens=906
17:54:47,634 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n156,CHRISTMAS MORNING,Christmas Morning is the time when Scrooge and the Ghost witness the activities and atmosphere of the city,23\n17,GHOST OF CHRISTMAS PRESENT,"The Ghost of Christmas Present is a jolly and generous spirit featured in Charles Dickens\' classic novella, ""A Christmas Carol."" This benevolent apparition visits Ebenezer Scrooge to take him on a journey through the current state of Christmas, showcasing the joys and festivities that the season brings. Throughout their encounter, the Ghost of Christmas Present embodies the essence of generosity and merriment, serving as a catalyst for Scrooge\'s transformation and awakening to the true spirit of Christmas.",8\n98,CHRISTMAS PRESENT,"The Christmas Present, also known as the Ghost of Christmas Present, is a pivotal figure in Charles Dickens\' classic tale, ""A Christmas Carol."" This benevolent spirit is one of the three spirits who visit Ebenezer Scrooge during the Christmas season to teach him the importance of generosity, joy, and the spirit of giving. The Ghost of Christmas Present is the second spirit to appear to Scrooge, following the Ghost of Christmas Past and preceding the Ghost of Christmas Yet to Come. Through a series of vivid and enlightening visions, the Ghost of Christmas Present shows Scrooge the current state of others during the festive season, highlighting the happiness and goodwill that are prevalent among people, in stark contrast to Scrooge\'s own miserly and isolated existence. This encounter is instrumental in Scrooge\'s transformation from a cold-hearted businessman to a compassionate and caring individual.",3\n12,GREAT BRITAIN,"Great Britain serves as both the location where the book ""A Christmas Carol"" is printed and the setting for the story itself. The narrative is characterized by its depiction of chimneys and snowy weather, immersing readers in the atmospheric environment typical of Great Britain during the holiday season.",2\n153,HOLLY,"Holly is a plant that is prominently featured in Scrooge\'s room as part of the decorations, symbolizing the Christmas spirit.",2\n154,MISTLETOE,"Mistletoe is a plant that is prominently featured in Scrooge\'s room as part of the decorations, symbolizing the festive spirit of Christmas.",2\n155,IVY,"Ivy is a plant that is prominently featured in Scrooge\'s room as part of the decorations, and it symbolizes the spirit of Christmas in the text.",2\n165,BRAWN,"Brawn is mentioned in the text, symbolizing Christmas food",1\n157,POULTERERS\' SHOPS,"Poulterers\' shops are establishments that sell poultry and other festive foods, described as being half open and radiant",1\n158,FRUITERERS\' SHOPS,"Fruiterers\' shops are establishments that sell fruits, described as being radiant and full of various fruits",1\n159,GROCERS\' SHOPS,"Grocers\' shops are establishments that sell a variety of goods, described as having glimpses of various items through the shutters",1\n160,RED BERRIES,"Red berries are mentioned in the text, symbolizing Christmas",1\n161,TURKEYS,"Turkeys are mentioned in the text, symbolizing Christmas food",1\n162,GEESE,"Geese are mentioned in the text, symbolizing Christmas food",1\n163,GAME,"Game is mentioned in the text, symbolizing Christmas food",1\n164,POULTRY,"Poultry is mentioned in the text, symbolizing Christmas food",1\n166,MEAT,"Meat is mentioned in the text, symbolizing Christmas food",1\n167,PIGS,"Pigs are mentioned in the text, symbolizing Christmas food",1\n168,SAUSAGES,"Sausages are mentioned in the text, symbolizing Christmas food",1\n169,OYSTERS,"Oysters are mentioned in the text, symbolizing Christmas food",1\n170,PIES,"Pies are mentioned in the text, symbolizing Christmas food",1\n171,PUDDINGS,"Puddings are mentioned in the text, symbolizing Christmas food",1\n172,FRUIT,"Fruit is mentioned in the text, symbolizing Christmas food",1\n173,PUNCH,"Punch is mentioned in the text, symbolizing Christmas drink",1\n1,A CHRISTMAS CAROL,"A Christmas Carol is a literary event represented by the publication of Charles Dickens\'s book, first published in 1915 by J. B. Lippincott Company",10\n4,J. B. LIPPINCOTT COMPANY,"J. B. Lippincott Company is the publisher of ""A Christmas Carol,"" based in Philadelphia and New York",3\n15,1843,"1843 is the year when ""A Christmas Carol"" was first published by Charles Dickens",1\n14,1915,"1915 is the year when ""A Christmas Carol"" was originally published by J. B. Lippincott Company",1\n2,CHARLES DICKENS,"Charles Dickens is the author of ""A Christmas Carol,"" a renowned English writer known for his contributions to literature",1\n3,ARTHUR RACKHAM,"Arthur Rackham is the illustrator of ""A Christmas Carol,"" known for his artistic contributions to the book",1\n152,THRONE,The throne is a symbolic seat formed by various festive foods and items in Scrooge\'s room,1\n19,PHILADELPHIA,,1\n11,NEW YORK,New York is a city in the United States where J. B. Lippincott Company is based,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,combined_degree\n145,SCROOGE,CHRISTMAS MORNING,Scrooge experiences Christmas Morning with the Ghost of Christmas Present,134\n45,GHOST OF CHRISTMAS PRESENT,SCROOGE,"During his transformative journey, Scrooge is visited by the Ghost of Christmas Present, who serves as a guide to help him learn about the joys and significance of Christmas. This interaction plays a crucial role in Scrooge\'s eventual transformation from a miserly and isolated figure to one who embraces the spirit of generosity and community.",119\n163,SCROOGE,CHRISTMAS PRESENT,Scrooge is visited by the Ghost of Christmas Present to show him the current state of others during Christmas,114\n51,GHOST OF CHRISTMAS PRESENT,CHRISTMAS MORNING,The Ghost of Christmas Present takes Scrooge to witness Christmas Morning,31\n41,GREAT BRITAIN,CHRISTMAS MORNING,Great Britain is the location where Christmas Morning is experienced,25\n289,HOLLY,CHRISTMAS MORNING,Holly is mentioned as part of the Christmas Morning decorations,25\n290,MISTLETOE,CHRISTMAS MORNING,Mistletoe is mentioned as part of the Christmas Morning decorations,25\n291,IVY,CHRISTMAS MORNING,Ivy is mentioned as part of the Christmas Morning decorations,25\n300,CHRISTMAS MORNING,BRAWN,Brawn is mentioned as part of the Christmas Morning food,24\n292,CHRISTMAS MORNING,POULTERERS\' SHOPS,Poulterers\' shops are described as being active during Christmas Morning,24\n293,CHRISTMAS MORNING,FRUITERERS\' SHOPS,Fruiterers\' shops are described as being active during Christmas Morning,24\n294,CHRISTMAS MORNING,GROCERS\' SHOPS,Grocers\' shops are described as being active during Christmas Morning,24\n295,CHRISTMAS MORNING,RED BERRIES,Red berries are mentioned as part of the Christmas Morning decorations,24\n296,CHRISTMAS MORNING,TURKEYS,Turkeys are mentioned as part of the Christmas Morning food,24\n297,CHRISTMAS MORNING,GEESE,Geese are mentioned as part of the Christmas Morning food,24\n298,CHRISTMAS MORNING,GAME,Game is mentioned as part of the Christmas Morning food,24\n299,CHRISTMAS MORNING,POULTRY,Poultry is mentioned as part of the Christmas Morning food,24\n301,CHRISTMAS MORNING,MEAT,Meat is mentioned as part of the Christmas Morning food,24\n302,CHRISTMAS MORNING,PIGS,Pigs are mentioned as part of the Christmas Morning food,24\n303,CHRISTMAS MORNING,SAUSAGES,Sausages are mentioned as part of the Christmas Morning food,24\n304,CHRISTMAS MORNING,OYSTERS,Oysters are mentioned as part of the Christmas Morning food,24\n305,CHRISTMAS MORNING,PIES,Pies are mentioned as part of the Christmas Morning food,24\n306,CHRISTMAS MORNING,PUDDINGS,Puddings are mentioned as part of the Christmas Morning food,24\n307,CHRISTMAS MORNING,FRUIT,Fruit is mentioned as part of the Christmas Morning food,24\n308,CHRISTMAS MORNING,PUNCH,Punch is mentioned as part of the Christmas Morning drink,24\n19,A CHRISTMAS CAROL,GHOST OF CHRISTMAS PRESENT,"The Ghost of Christmas Present is an apparition in ""A Christmas Carol""",18\n18,A CHRISTMAS CAROL,GHOST OF CHRISTMAS PAST,"The Ghost of Christmas Past is an apparition in ""A Christmas Carol""",14\n14,A CHRISTMAS CAROL,J. B. LIPPINCOTT COMPANY,"J. B. Lippincott Company is the publisher of ""A Christmas Carol""",13\n15,A CHRISTMAS CAROL,GREAT BRITAIN,"Great Britain is the location where ""A Christmas Carol"" is printed",12\n20,A CHRISTMAS CAROL,GHOST OF CHRISTMAS YET TO COME,"The Ghost of Christmas Yet to Come is an apparition in ""A Christmas Carol""",12\n17,A CHRISTMAS CAROL,1843,"1843 is the year when ""A Christmas Carol"" was first published by Charles Dickens",11\n16,A CHRISTMAS CAROL,1915,"1915 is the year when ""A Christmas Carol"" was originally published by J. B. Lippincott Company",11\n12,A CHRISTMAS CAROL,CHARLES DICKENS,"Charles Dickens is the author of ""A Christmas Carol""",11\n13,A CHRISTMAS CAROL,ARTHUR RACKHAM,"Arthur Rackham is the illustrator of ""A Christmas Carol""",11\n46,GHOST OF CHRISTMAS PRESENT,CHRISTMAS PRESENT,"The Ghost of Christmas Present embodies the spirit of Christmas Present, showing Scrooge the joys of the season",11\n48,GHOST OF CHRISTMAS PRESENT,HOLLY,Holly is part of the decorations worn by the Ghost of Christmas Present,10\n49,GHOST OF CHRISTMAS PRESENT,MISTLETOE,Mistletoe is part of the decorations worn by the Ghost of Christmas Present,10\n50,GHOST OF CHRISTMAS PRESENT,IVY,Ivy is part of the decorations worn by the Ghost of Christmas Present,10\n47,GHOST OF CHRISTMAS PRESENT,THRONE,The Ghost of Christmas Present sits on a throne made of festive foods and items,9\n250,THREE SPIRITS,CHRISTMAS PRESENT,The Ghost of Christmas Present is one of the Three Spirits who visits Scrooge,7\n21,J. B. LIPPINCOTT COMPANY,PHILADELPHIA,Philadelphia is the location where J. B. Lippincott Company is based,4\n22,J. B. LIPPINCOTT COMPANY,NEW YORK,New York is the location where J. B. Lippincott Company is based,4\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
17:54:49,983 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:55:50,76 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n156,CHRISTMAS MORNING,Christmas Morning is the time when Scrooge and the Ghost witness the activities and atmosphere of the city,23\n17,GHOST OF CHRISTMAS PRESENT,"The Ghost of Christmas Present is a jolly and generous spirit featured in Charles Dickens\' classic novella, ""A Christmas Carol."" This benevolent apparition visits Ebenezer Scrooge to take him on a journey through the current state of Christmas, showcasing the joys and festivities that the season brings. Throughout their encounter, the Ghost of Christmas Present embodies the essence of generosity and merriment, serving as a catalyst for Scrooge\'s transformation and awakening to the true spirit of Christmas.",8\n98,CHRISTMAS PRESENT,"The Christmas Present, also known as the Ghost of Christmas Present, is a pivotal figure in Charles Dickens\' classic tale, ""A Christmas Carol."" This benevolent spirit is one of the three spirits who visit Ebenezer Scrooge during the Christmas season to teach him the importance of generosity, joy, and the spirit of giving. The Ghost of Christmas Present is the second spirit to appear to Scrooge, following the Ghost of Christmas Past and preceding the Ghost of Christmas Yet to Come. Through a series of vivid and enlightening visions, the Ghost of Christmas Present shows Scrooge the current state of others during the festive season, highlighting the happiness and goodwill that are prevalent among people, in stark contrast to Scrooge\'s own miserly and isolated existence. This encounter is instrumental in Scrooge\'s transformation from a cold-hearted businessman to a compassionate and caring individual.",3\n12,GREAT BRITAIN,"Great Britain serves as both the location where the book ""A Christmas Carol"" is printed and the setting for the story itself. The narrative is characterized by its depiction of chimneys and snowy weather, immersing readers in the atmospheric environment typical of Great Britain during the holiday season.",2\n153,HOLLY,"Holly is a plant that is prominently featured in Scrooge\'s room as part of the decorations, symbolizing the Christmas spirit.",2\n154,MISTLETOE,"Mistletoe is a plant that is prominently featured in Scrooge\'s room as part of the decorations, symbolizing the festive spirit of Christmas.",2\n155,IVY,"Ivy is a plant that is prominently featured in Scrooge\'s room as part of the decorations, and it symbolizes the spirit of Christmas in the text.",2\n165,BRAWN,"Brawn is mentioned in the text, symbolizing Christmas food",1\n157,POULTERERS\' SHOPS,"Poulterers\' shops are establishments that sell poultry and other festive foods, described as being half open and radiant",1\n158,FRUITERERS\' SHOPS,"Fruiterers\' shops are establishments that sell fruits, described as being radiant and full of various fruits",1\n159,GROCERS\' SHOPS,"Grocers\' shops are establishments that sell a variety of goods, described as having glimpses of various items through the shutters",1\n160,RED BERRIES,"Red berries are mentioned in the text, symbolizing Christmas",1\n161,TURKEYS,"Turkeys are mentioned in the text, symbolizing Christmas food",1\n162,GEESE,"Geese are mentioned in the text, symbolizing Christmas food",1\n163,GAME,"Game is mentioned in the text, symbolizing Christmas food",1\n164,POULTRY,"Poultry is mentioned in the text, symbolizing Christmas food",1\n166,MEAT,"Meat is mentioned in the text, symbolizing Christmas food",1\n167,PIGS,"Pigs are mentioned in the text, symbolizing Christmas food",1\n168,SAUSAGES,"Sausages are mentioned in the text, symbolizing Christmas food",1\n169,OYSTERS,"Oysters are mentioned in the text, symbolizing Christmas food",1\n170,PIES,"Pies are mentioned in the text, symbolizing Christmas food",1\n171,PUDDINGS,"Puddings are mentioned in the text, symbolizing Christmas food",1\n172,FRUIT,"Fruit is mentioned in the text, symbolizing Christmas food",1\n173,PUNCH,"Punch is mentioned in the text, symbolizing Christmas drink",1\n1,A CHRISTMAS CAROL,"A Christmas Carol is a literary event represented by the publication of Charles Dickens\'s book, first published in 1915 by J. B. Lippincott Company",10\n4,J. B. LIPPINCOTT COMPANY,"J. B. Lippincott Company is the publisher of ""A Christmas Carol,"" based in Philadelphia and New York",3\n15,1843,"1843 is the year when ""A Christmas Carol"" was first published by Charles Dickens",1\n14,1915,"1915 is the year when ""A Christmas Carol"" was originally published by J. B. Lippincott Company",1\n2,CHARLES DICKENS,"Charles Dickens is the author of ""A Christmas Carol,"" a renowned English writer known for his contributions to literature",1\n3,ARTHUR RACKHAM,"Arthur Rackham is the illustrator of ""A Christmas Carol,"" known for his artistic contributions to the book",1\n152,THRONE,The throne is a symbolic seat formed by various festive foods and items in Scrooge\'s room,1\n19,PHILADELPHIA,,1\n11,NEW YORK,New York is a city in the United States where J. B. Lippincott Company is based,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,combined_degree\n145,SCROOGE,CHRISTMAS MORNING,Scrooge experiences Christmas Morning with the Ghost of Christmas Present,134\n45,GHOST OF CHRISTMAS PRESENT,SCROOGE,"During his transformative journey, Scrooge is visited by the Ghost of Christmas Present, who serves as a guide to help him learn about the joys and significance of Christmas. This interaction plays a crucial role in Scrooge\'s eventual transformation from a miserly and isolated figure to one who embraces the spirit of generosity and community.",119\n163,SCROOGE,CHRISTMAS PRESENT,Scrooge is visited by the Ghost of Christmas Present to show him the current state of others during Christmas,114\n51,GHOST OF CHRISTMAS PRESENT,CHRISTMAS MORNING,The Ghost of Christmas Present takes Scrooge to witness Christmas Morning,31\n41,GREAT BRITAIN,CHRISTMAS MORNING,Great Britain is the location where Christmas Morning is experienced,25\n289,HOLLY,CHRISTMAS MORNING,Holly is mentioned as part of the Christmas Morning decorations,25\n290,MISTLETOE,CHRISTMAS MORNING,Mistletoe is mentioned as part of the Christmas Morning decorations,25\n291,IVY,CHRISTMAS MORNING,Ivy is mentioned as part of the Christmas Morning decorations,25\n300,CHRISTMAS MORNING,BRAWN,Brawn is mentioned as part of the Christmas Morning food,24\n292,CHRISTMAS MORNING,POULTERERS\' SHOPS,Poulterers\' shops are described as being active during Christmas Morning,24\n293,CHRISTMAS MORNING,FRUITERERS\' SHOPS,Fruiterers\' shops are described as being active during Christmas Morning,24\n294,CHRISTMAS MORNING,GROCERS\' SHOPS,Grocers\' shops are described as being active during Christmas Morning,24\n295,CHRISTMAS MORNING,RED BERRIES,Red berries are mentioned as part of the Christmas Morning decorations,24\n296,CHRISTMAS MORNING,TURKEYS,Turkeys are mentioned as part of the Christmas Morning food,24\n297,CHRISTMAS MORNING,GEESE,Geese are mentioned as part of the Christmas Morning food,24\n298,CHRISTMAS MORNING,GAME,Game is mentioned as part of the Christmas Morning food,24\n299,CHRISTMAS MORNING,POULTRY,Poultry is mentioned as part of the Christmas Morning food,24\n301,CHRISTMAS MORNING,MEAT,Meat is mentioned as part of the Christmas Morning food,24\n302,CHRISTMAS MORNING,PIGS,Pigs are mentioned as part of the Christmas Morning food,24\n303,CHRISTMAS MORNING,SAUSAGES,Sausages are mentioned as part of the Christmas Morning food,24\n304,CHRISTMAS MORNING,OYSTERS,Oysters are mentioned as part of the Christmas Morning food,24\n305,CHRISTMAS MORNING,PIES,Pies are mentioned as part of the Christmas Morning food,24\n306,CHRISTMAS MORNING,PUDDINGS,Puddings are mentioned as part of the Christmas Morning food,24\n307,CHRISTMAS MORNING,FRUIT,Fruit is mentioned as part of the Christmas Morning food,24\n308,CHRISTMAS MORNING,PUNCH,Punch is mentioned as part of the Christmas Morning drink,24\n19,A CHRISTMAS CAROL,GHOST OF CHRISTMAS PRESENT,"The Ghost of Christmas Present is an apparition in ""A Christmas Carol""",18\n18,A CHRISTMAS CAROL,GHOST OF CHRISTMAS PAST,"The Ghost of Christmas Past is an apparition in ""A Christmas Carol""",14\n14,A CHRISTMAS CAROL,J. B. LIPPINCOTT COMPANY,"J. B. Lippincott Company is the publisher of ""A Christmas Carol""",13\n15,A CHRISTMAS CAROL,GREAT BRITAIN,"Great Britain is the location where ""A Christmas Carol"" is printed",12\n20,A CHRISTMAS CAROL,GHOST OF CHRISTMAS YET TO COME,"The Ghost of Christmas Yet to Come is an apparition in ""A Christmas Carol""",12\n17,A CHRISTMAS CAROL,1843,"1843 is the year when ""A Christmas Carol"" was first published by Charles Dickens",11\n16,A CHRISTMAS CAROL,1915,"1915 is the year when ""A Christmas Carol"" was originally published by J. B. Lippincott Company",11\n12,A CHRISTMAS CAROL,CHARLES DICKENS,"Charles Dickens is the author of ""A Christmas Carol""",11\n13,A CHRISTMAS CAROL,ARTHUR RACKHAM,"Arthur Rackham is the illustrator of ""A Christmas Carol""",11\n46,GHOST OF CHRISTMAS PRESENT,CHRISTMAS PRESENT,"The Ghost of Christmas Present embodies the spirit of Christmas Present, showing Scrooge the joys of the season",11\n48,GHOST OF CHRISTMAS PRESENT,HOLLY,Holly is part of the decorations worn by the Ghost of Christmas Present,10\n49,GHOST OF CHRISTMAS PRESENT,MISTLETOE,Mistletoe is part of the decorations worn by the Ghost of Christmas Present,10\n50,GHOST OF CHRISTMAS PRESENT,IVY,Ivy is part of the decorations worn by the Ghost of Christmas Present,10\n47,GHOST OF CHRISTMAS PRESENT,THRONE,The Ghost of Christmas Present sits on a throne made of festive foods and items,9\n250,THREE SPIRITS,CHRISTMAS PRESENT,The Ghost of Christmas Present is one of the Three Spirits who visits Scrooge,7\n21,J. B. LIPPINCOTT COMPANY,PHILADELPHIA,Philadelphia is the location where J. B. Lippincott Company is based,4\n22,J. B. LIPPINCOTT COMPANY,NEW YORK,New York is the location where J. B. Lippincott Company is based,4\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
17:55:54,372 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:56:54,299 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community\'s key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n\n# Example Input\n-----------\nText:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at Verdant Oasis Plaza\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n{{\n    "title": "Verdant Oasis Plaza and Unity March",\n    "summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of which are associated with the march event.",\n    "rating": 5.0,\n    "rating_explanation": "The impact severity rating is moderate due to the potential for unrest or conflict during the Unity March.",\n    "findings": [\n        {{\n            "summary": "Verdant Oasis Plaza as the central location",\n            "explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for the Unity March. This plaza is the common link between all other entities, suggesting its significance in the community. The plaza\'s association with the march could potentially lead to issues such as public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data: Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"\n        }},\n        {{\n            "summary": "Harmony Assembly\'s role in the community",\n            "explanation": "Harmony Assembly is another key entity in this community, being the organizer of the march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of threat, depending on their objectives and the reactions they provoke. The relationship between Harmony Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6), Relationships (38, 43)]"\n        }},\n        {{\n            "summary": "Unity March as a significant event",\n            "explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event is a key factor in the community\'s dynamics and could be a potential source of threat, depending on the nature of the march and the reactions it provokes. The relationship between the march and the plaza is crucial in understanding the dynamics of this community. [Data: Relationships (39)]"\n        }},\n        {{\n            "summary": "Role of Tribune Spotlight",\n            "explanation": "Tribune Spotlight is reporting on the Unity March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention, which could amplify its impact on the community. The role of Tribune Spotlight could be significant in shaping public perception of the event and the entities involved. [Data: Relationships (40)]"\n        }}\n    ]\n}}\n\n\n# Real Data\n\nUse the following text for your answer. Do not make anything up in your answer.\n\nText:\n-----Entities-----\nhuman_readable_id,title,description,degree\n156,CHRISTMAS MORNING,Christmas Morning is the time when Scrooge and the Ghost witness the activities and atmosphere of the city,23\n17,GHOST OF CHRISTMAS PRESENT,"The Ghost of Christmas Present is a jolly and generous spirit featured in Charles Dickens\' classic novella, ""A Christmas Carol."" This benevolent apparition visits Ebenezer Scrooge to take him on a journey through the current state of Christmas, showcasing the joys and festivities that the season brings. Throughout their encounter, the Ghost of Christmas Present embodies the essence of generosity and merriment, serving as a catalyst for Scrooge\'s transformation and awakening to the true spirit of Christmas.",8\n98,CHRISTMAS PRESENT,"The Christmas Present, also known as the Ghost of Christmas Present, is a pivotal figure in Charles Dickens\' classic tale, ""A Christmas Carol."" This benevolent spirit is one of the three spirits who visit Ebenezer Scrooge during the Christmas season to teach him the importance of generosity, joy, and the spirit of giving. The Ghost of Christmas Present is the second spirit to appear to Scrooge, following the Ghost of Christmas Past and preceding the Ghost of Christmas Yet to Come. Through a series of vivid and enlightening visions, the Ghost of Christmas Present shows Scrooge the current state of others during the festive season, highlighting the happiness and goodwill that are prevalent among people, in stark contrast to Scrooge\'s own miserly and isolated existence. This encounter is instrumental in Scrooge\'s transformation from a cold-hearted businessman to a compassionate and caring individual.",3\n12,GREAT BRITAIN,"Great Britain serves as both the location where the book ""A Christmas Carol"" is printed and the setting for the story itself. The narrative is characterized by its depiction of chimneys and snowy weather, immersing readers in the atmospheric environment typical of Great Britain during the holiday season.",2\n153,HOLLY,"Holly is a plant that is prominently featured in Scrooge\'s room as part of the decorations, symbolizing the Christmas spirit.",2\n154,MISTLETOE,"Mistletoe is a plant that is prominently featured in Scrooge\'s room as part of the decorations, symbolizing the festive spirit of Christmas.",2\n155,IVY,"Ivy is a plant that is prominently featured in Scrooge\'s room as part of the decorations, and it symbolizes the spirit of Christmas in the text.",2\n165,BRAWN,"Brawn is mentioned in the text, symbolizing Christmas food",1\n157,POULTERERS\' SHOPS,"Poulterers\' shops are establishments that sell poultry and other festive foods, described as being half open and radiant",1\n158,FRUITERERS\' SHOPS,"Fruiterers\' shops are establishments that sell fruits, described as being radiant and full of various fruits",1\n159,GROCERS\' SHOPS,"Grocers\' shops are establishments that sell a variety of goods, described as having glimpses of various items through the shutters",1\n160,RED BERRIES,"Red berries are mentioned in the text, symbolizing Christmas",1\n161,TURKEYS,"Turkeys are mentioned in the text, symbolizing Christmas food",1\n162,GEESE,"Geese are mentioned in the text, symbolizing Christmas food",1\n163,GAME,"Game is mentioned in the text, symbolizing Christmas food",1\n164,POULTRY,"Poultry is mentioned in the text, symbolizing Christmas food",1\n166,MEAT,"Meat is mentioned in the text, symbolizing Christmas food",1\n167,PIGS,"Pigs are mentioned in the text, symbolizing Christmas food",1\n168,SAUSAGES,"Sausages are mentioned in the text, symbolizing Christmas food",1\n169,OYSTERS,"Oysters are mentioned in the text, symbolizing Christmas food",1\n170,PIES,"Pies are mentioned in the text, symbolizing Christmas food",1\n171,PUDDINGS,"Puddings are mentioned in the text, symbolizing Christmas food",1\n172,FRUIT,"Fruit is mentioned in the text, symbolizing Christmas food",1\n173,PUNCH,"Punch is mentioned in the text, symbolizing Christmas drink",1\n1,A CHRISTMAS CAROL,"A Christmas Carol is a literary event represented by the publication of Charles Dickens\'s book, first published in 1915 by J. B. Lippincott Company",10\n4,J. B. LIPPINCOTT COMPANY,"J. B. Lippincott Company is the publisher of ""A Christmas Carol,"" based in Philadelphia and New York",3\n15,1843,"1843 is the year when ""A Christmas Carol"" was first published by Charles Dickens",1\n14,1915,"1915 is the year when ""A Christmas Carol"" was originally published by J. B. Lippincott Company",1\n2,CHARLES DICKENS,"Charles Dickens is the author of ""A Christmas Carol,"" a renowned English writer known for his contributions to literature",1\n3,ARTHUR RACKHAM,"Arthur Rackham is the illustrator of ""A Christmas Carol,"" known for his artistic contributions to the book",1\n152,THRONE,The throne is a symbolic seat formed by various festive foods and items in Scrooge\'s room,1\n19,PHILADELPHIA,,1\n11,NEW YORK,New York is a city in the United States where J. B. Lippincott Company is based,1\n\n\n-----Relationships-----\nhuman_readable_id,source,target,description,combined_degree\n145,SCROOGE,CHRISTMAS MORNING,Scrooge experiences Christmas Morning with the Ghost of Christmas Present,134\n45,GHOST OF CHRISTMAS PRESENT,SCROOGE,"During his transformative journey, Scrooge is visited by the Ghost of Christmas Present, who serves as a guide to help him learn about the joys and significance of Christmas. This interaction plays a crucial role in Scrooge\'s eventual transformation from a miserly and isolated figure to one who embraces the spirit of generosity and community.",119\n163,SCROOGE,CHRISTMAS PRESENT,Scrooge is visited by the Ghost of Christmas Present to show him the current state of others during Christmas,114\n51,GHOST OF CHRISTMAS PRESENT,CHRISTMAS MORNING,The Ghost of Christmas Present takes Scrooge to witness Christmas Morning,31\n41,GREAT BRITAIN,CHRISTMAS MORNING,Great Britain is the location where Christmas Morning is experienced,25\n289,HOLLY,CHRISTMAS MORNING,Holly is mentioned as part of the Christmas Morning decorations,25\n290,MISTLETOE,CHRISTMAS MORNING,Mistletoe is mentioned as part of the Christmas Morning decorations,25\n291,IVY,CHRISTMAS MORNING,Ivy is mentioned as part of the Christmas Morning decorations,25\n300,CHRISTMAS MORNING,BRAWN,Brawn is mentioned as part of the Christmas Morning food,24\n292,CHRISTMAS MORNING,POULTERERS\' SHOPS,Poulterers\' shops are described as being active during Christmas Morning,24\n293,CHRISTMAS MORNING,FRUITERERS\' SHOPS,Fruiterers\' shops are described as being active during Christmas Morning,24\n294,CHRISTMAS MORNING,GROCERS\' SHOPS,Grocers\' shops are described as being active during Christmas Morning,24\n295,CHRISTMAS MORNING,RED BERRIES,Red berries are mentioned as part of the Christmas Morning decorations,24\n296,CHRISTMAS MORNING,TURKEYS,Turkeys are mentioned as part of the Christmas Morning food,24\n297,CHRISTMAS MORNING,GEESE,Geese are mentioned as part of the Christmas Morning food,24\n298,CHRISTMAS MORNING,GAME,Game is mentioned as part of the Christmas Morning food,24\n299,CHRISTMAS MORNING,POULTRY,Poultry is mentioned as part of the Christmas Morning food,24\n301,CHRISTMAS MORNING,MEAT,Meat is mentioned as part of the Christmas Morning food,24\n302,CHRISTMAS MORNING,PIGS,Pigs are mentioned as part of the Christmas Morning food,24\n303,CHRISTMAS MORNING,SAUSAGES,Sausages are mentioned as part of the Christmas Morning food,24\n304,CHRISTMAS MORNING,OYSTERS,Oysters are mentioned as part of the Christmas Morning food,24\n305,CHRISTMAS MORNING,PIES,Pies are mentioned as part of the Christmas Morning food,24\n306,CHRISTMAS MORNING,PUDDINGS,Puddings are mentioned as part of the Christmas Morning food,24\n307,CHRISTMAS MORNING,FRUIT,Fruit is mentioned as part of the Christmas Morning food,24\n308,CHRISTMAS MORNING,PUNCH,Punch is mentioned as part of the Christmas Morning drink,24\n19,A CHRISTMAS CAROL,GHOST OF CHRISTMAS PRESENT,"The Ghost of Christmas Present is an apparition in ""A Christmas Carol""",18\n18,A CHRISTMAS CAROL,GHOST OF CHRISTMAS PAST,"The Ghost of Christmas Past is an apparition in ""A Christmas Carol""",14\n14,A CHRISTMAS CAROL,J. B. LIPPINCOTT COMPANY,"J. B. Lippincott Company is the publisher of ""A Christmas Carol""",13\n15,A CHRISTMAS CAROL,GREAT BRITAIN,"Great Britain is the location where ""A Christmas Carol"" is printed",12\n20,A CHRISTMAS CAROL,GHOST OF CHRISTMAS YET TO COME,"The Ghost of Christmas Yet to Come is an apparition in ""A Christmas Carol""",12\n17,A CHRISTMAS CAROL,1843,"1843 is the year when ""A Christmas Carol"" was first published by Charles Dickens",11\n16,A CHRISTMAS CAROL,1915,"1915 is the year when ""A Christmas Carol"" was originally published by J. B. Lippincott Company",11\n12,A CHRISTMAS CAROL,CHARLES DICKENS,"Charles Dickens is the author of ""A Christmas Carol""",11\n13,A CHRISTMAS CAROL,ARTHUR RACKHAM,"Arthur Rackham is the illustrator of ""A Christmas Carol""",11\n46,GHOST OF CHRISTMAS PRESENT,CHRISTMAS PRESENT,"The Ghost of Christmas Present embodies the spirit of Christmas Present, showing Scrooge the joys of the season",11\n48,GHOST OF CHRISTMAS PRESENT,HOLLY,Holly is part of the decorations worn by the Ghost of Christmas Present,10\n49,GHOST OF CHRISTMAS PRESENT,MISTLETOE,Mistletoe is part of the decorations worn by the Ghost of Christmas Present,10\n50,GHOST OF CHRISTMAS PRESENT,IVY,Ivy is part of the decorations worn by the Ghost of Christmas Present,10\n47,GHOST OF CHRISTMAS PRESENT,THRONE,The Ghost of Christmas Present sits on a throne made of festive foods and items,9\n250,THREE SPIRITS,CHRISTMAS PRESENT,The Ghost of Christmas Present is one of the Three Spirits who visits Scrooge,7\n21,J. B. LIPPINCOTT COMPANY,PHILADELPHIA,Philadelphia is the location where J. B. Lippincott Company is based,4\n22,J. B. LIPPINCOTT COMPANY,NEW YORK,New York is the location where J. B. Lippincott Company is based,4\n\n\nThe report should include the following sections:\n\n- TITLE: community\'s name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\'s overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n    {{\n        "title": <report_title>,\n        "summary": <executive_summary>,\n        "rating": <impact_severity_rating>,\n        "rating_explanation": <rating_explanation>,\n        "findings": [\n            {{\n                "summary":<insight_1_summary>,\n                "explanation": <insight_1_explanation>\n            }},\n            {{\n                "summary":<insight_2_summary>,\n                "explanation": <insight_2_explanation>\n            }}\n        ]\n    }}\n\n# Grounding Rules\n\nPoints supported by data should list their data references as follows:\n\n"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids); <dataset name> (record ids)]."\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add "+more" to indicate that there are more.\n\nFor example:\n"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1), Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\nOutput:'}
17:56:59,837 httpx INFO HTTP Request: POST https://api.deepseek.com/v1/chat/completions "HTTP/1.1 200 OK"
17:57:44,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 46.21486933399865. input_tokens=4370, output_tokens=772
17:57:44,888 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:57:45,103 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_community_reports', 'create_final_documents', 'create_final_relationships', 'create_final_entities', 'create_final_text_units']
17:57:45,103 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
17:57:45,107 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
17:57:45,109 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
17:57:45,111 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
17:57:45,113 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
17:57:45,120 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
17:57:45,122 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
17:57:45,122 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
17:57:45,124 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
17:57:45,142 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
17:57:45,142 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
17:57:45,174 graphrag.index.operations.embed_text.strategies.openai INFO embedding 48 inputs via 48 snippets using 4 batches. max_batch_size=16, max_tokens=8191
17:57:46,706 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:46,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7769481249997625. input_tokens=3839, output_tokens=0
17:57:47,26 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:47,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:47,207 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:47,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3105487920001906. input_tokens=7902, output_tokens=0
17:57:47,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.500797249998868. input_tokens=7666, output_tokens=0
17:57:47,800 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.621744916999887. input_tokens=8181, output_tokens=0
17:57:47,880 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
17:57:47,925 graphrag.index.operations.embed_text.strategies.openai INFO embedding 42 inputs via 42 snippets using 7 batches. max_batch_size=16, max_tokens=8191
17:57:47,951 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
17:57:47,962 graphrag.index.operations.embed_text.strategies.openai INFO embedding 290 inputs via 290 snippets using 19 batches. max_batch_size=16, max_tokens=8191
17:57:48,518 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:48,758 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:48,846 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8768186250017607. input_tokens=625, output_tokens=0
17:57:48,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,13 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,42 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0720354589975614. input_tokens=1284, output_tokens=0
17:57:49,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.258512124997651. input_tokens=750, output_tokens=0
17:57:49,245 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,263 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,365 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,408 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4481062919985561. input_tokens=724, output_tokens=0
17:57:49,430 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4510148749977816. input_tokens=33, output_tokens=0
17:57:49,436 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.475714042000618. input_tokens=828, output_tokens=0
17:57:49,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4795018339973467. input_tokens=414, output_tokens=0
17:57:49,469 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,470 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,579 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,596 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,620 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:49,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6572382919985102. input_tokens=448, output_tokens=0
17:57:49,652 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6845310420030728. input_tokens=404, output_tokens=0
17:57:49,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.764080875000218. input_tokens=656, output_tokens=0
17:57:49,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8105504999984987. input_tokens=332, output_tokens=0
17:57:49,815 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8331057499999588. input_tokens=489, output_tokens=0
17:57:49,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.8491149590008717. input_tokens=429, output_tokens=0
17:57:50,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0897210000002815. input_tokens=431, output_tokens=0
17:57:50,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.136231833999773. input_tokens=372, output_tokens=0
17:57:50,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.178041374998429. input_tokens=464, output_tokens=0
17:57:50,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.2822269999996934. input_tokens=407, output_tokens=0
17:57:50,323 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.3490415000014764. input_tokens=335, output_tokens=0
17:57:52,749 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:57:53,379 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 5.407196583997575. input_tokens=469, output_tokens=0
17:57:53,454 graphrag.cli.index INFO All workflows completed successfully.
