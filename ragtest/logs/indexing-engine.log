16:32:47,293 graphrag.cli.index INFO Logging enabled at /Users/apple/Documents/project/KG-RAG/ragtest/logs/indexing-engine.log
16:32:47,294 graphrag.cli.index INFO Starting pipeline run for: 20241127-163247, dry_run=False
16:32:47,295 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "model": "gpt-4-turbo-preview",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "/Users/apple/Documents/project/KG-RAG/ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "/Users/apple/Documents/project/KG-RAG/ragtest/logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "/Users/apple/Documents/project/KG-RAG/ragtest/output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output/lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 3,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0.0,
        "local_search_top_p": 1.0,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 2000
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
16:32:47,296 graphrag.index.create_pipeline_config INFO skipping workflows 
16:32:47,296 graphrag.index.run.run INFO Running pipeline
16:32:47,296 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at /Users/apple/Documents/project/KG-RAG/ragtest/output
16:32:47,296 graphrag.index.input.load_input INFO loading input from root_dir=input
16:32:47,296 graphrag.index.input.load_input INFO using file storage for input
16:32:47,297 graphrag.index.storage.file_pipeline_storage INFO search /Users/apple/Documents/project/KG-RAG/ragtest/input for files matching .*\.txt$
16:32:47,297 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
16:32:47,298 graphrag.index.input.text INFO Found 1 files, loading 1
16:32:47,300 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_final_documents', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'create_final_relationships', 'create_final_text_units', 'create_final_community_reports', 'generate_text_embeddings']
16:32:47,300 graphrag.index.run.run INFO Final # of rows loaded: 1
16:32:47,343 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
16:32:47,345 datashaper.workflow.workflow INFO executing verb create_base_text_units
16:33:18,639 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_text_units']
16:33:18,639 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:33:18,642 datashaper.workflow.workflow INFO executing verb create_final_documents
16:33:18,648 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
16:33:18,724 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
16:33:18,724 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:33:18,728 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
16:33:18,731 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
16:33:18,768 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for gpt-4-turbo-preview: TPM=0, RPM=0
16:33:18,768 graphrag.index.llm.load_llm INFO create concurrency limiter for gpt-4-turbo-preview: 25
16:33:20,245 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,248 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,253 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,259 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,259 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,261 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,262 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,271 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,273 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,274 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,281 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,282 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,289 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,289 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,290 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,302 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,303 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,311 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,312 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,334 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,350 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,354 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,366 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,368 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,381 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,391 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,417 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,419 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,423 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,425 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:20,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:20,788 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,734 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,736 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,769 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,772 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,843 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,844 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,888 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,889 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,935 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,936 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,940 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:21,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:21,980 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,21 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,79 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,80 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,93 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,94 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,130 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,251 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,252 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,302 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,304 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,304 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,306 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,306 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,353 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,378 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,466 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,469 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,472 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,577 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:22,638 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:22,641 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,308 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,435 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,440 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,605 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,606 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,704 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,707 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,768 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,770 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,778 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,780 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,837 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,840 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,886 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,889 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,917 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,920 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,922 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,924 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,939 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,941 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:24,977 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:24,979 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,61 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,139 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,141 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,159 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,248 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,251 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,405 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,456 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:25,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.666573833001166. input_tokens=2936, output_tokens=153
16:33:25,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,791 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:25,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:25,840 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:26,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:26,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.71959216599862. input_tokens=2936, output_tokens=186
16:33:26,844 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:26,846 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:28,640 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:28,642 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,210 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,212 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,299 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,303 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,305 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,306 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,437 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,439 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,455 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,457 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,657 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,658 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,811 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,897 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,899 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,937 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,941 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:29,977 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:29,979 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,28 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,30 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,47 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,148 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,150 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,152 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,153 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,173 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,175 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,671 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,674 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,676 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,685 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:30,696 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:30,697 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:31,610 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:31,613 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:36,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:36,323 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:37,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:37,609 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,20 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,336 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,338 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,542 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,636 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,855 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,912 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,914 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,934 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:38,935 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:38,936 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,63 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,72 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,73 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,73 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:39,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 12.171458417000395. input_tokens=2936, output_tokens=388
16:33:39,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,411 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,424 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,425 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,448 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,450 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,461 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,488 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,583 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,585 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,587 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:39,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 14.22006516699912. input_tokens=2936, output_tokens=258
16:33:39,668 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,671 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,724 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,726 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:39,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:39,913 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:40,683 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:40,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.89539666699784. input_tokens=2935, output_tokens=391
16:33:41,31 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:41,33 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:41,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:41,417 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:41,552 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:41,555 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.741565917000116. input_tokens=2935, output_tokens=400
16:33:41,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:41,574 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:43,264 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:43,266 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:44,9 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:44,13 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:44,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:44,882 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:45,432 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:45,435 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:46,457 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:46,459 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:47,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:47,996 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:48,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:48,496 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:48,898 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:48,900 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,131 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,133 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,290 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,291 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,425 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,427 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,617 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,619 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,626 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,628 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,700 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,701 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,717 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,718 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,771 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,773 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,787 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,788 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:49,856 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:49,858 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,20 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,22 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,30 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,31 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,53 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,55 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,68 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,70 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,270 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,427 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,429 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:50,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:50,517 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:51,167 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:51,170 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:53,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:53,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.49813712499963. input_tokens=2936, output_tokens=303
16:33:55,877 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:55,880 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:58,13 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:58,14 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:58,496 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:33:58,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 19.708580082999106. input_tokens=2936, output_tokens=470
16:33:58,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:58,806 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:58,872 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:58,874 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,274 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,276 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,471 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,471 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,502 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,503 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,680 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,680 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:33:59,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:33:59,953 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,95 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,95 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,159 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,161 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,219 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,221 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,365 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,367 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,631 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,711 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,712 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,719 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,768 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,769 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,835 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,836 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,848 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,849 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,850 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,852 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,875 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,877 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:00,944 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:00,945 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:01,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:01,18 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:03,269 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:03,272 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:06,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:06,254 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:06,787 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:06,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 27.87185904099897. input_tokens=2935, output_tokens=496
16:34:08,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:08,377 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:08,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:08,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.603592332998232. input_tokens=2936, output_tokens=266
16:34:08,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:08,995 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:09,108 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 4 retries took 10.862156040999253. input_tokens=2936, output_tokens=351
16:34:09,176 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,177 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,574 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,631 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:09,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:09,860 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,17 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,19 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,291 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,293 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,315 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,316 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,438 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,439 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,544 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,738 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,739 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:10,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:10,897 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,14 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,15 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,48 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,49 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,128 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,129 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,140 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,285 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,287 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,404 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,522 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,524 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,646 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,648 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,777 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,778 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,795 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,796 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:11,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:11,819 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:14,280 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:14,282 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:16,621 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:16,623 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:18,743 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:18,745 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:19,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:19,201 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:19,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:19,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 3 retries took 11.322165582998423. input_tokens=2935, output_tokens=354
16:34:19,953 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:19,955 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:19,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:19,999 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,376 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,378 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,629 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,764 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,765 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:20,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:20,805 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,70 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,72 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,87 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,88 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,374 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,375 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,413 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,415 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,460 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,463 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,464 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,510 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:21,773 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:21,775 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,147 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,149 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,157 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,159 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,160 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,161 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,272 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,274 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:22,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:22,467 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:24,958 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:24,962 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.170364416000666. input_tokens=2935, output_tokens=517
16:34:25,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:25,370 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:27,465 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:27,469 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:28,61 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:28,63 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:29,107 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:29,109 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,321 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,323 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,323 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 28114, Requested 6818. Please try again in 9.864s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:30,332 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "with my eyes turned down,\nand never raise them to that blessed Star which led the Wise Men to a\npoor abode? Were there no poor homes to which its light would have\nconducted _me_?'\n\nScrooge was very much dismayed to hear the spectre going on at this\nrate, and began to quake exceedingly.\n\n'Hear me!' cried the Ghost. 'My time is nearly gone.'\n\n'I will,' said Scrooge. 'But don't be hard upon me! Don't be flowery,\nJacob! Pray!'\n\n'How it is that I appear before you in a shape that you can see, I may\nnot tell. I have sat invisible beside you many and many a day.'\n\nIt was not an agreeable idea. Scrooge shivered, and wiped the\nperspiration from his brow.\n\n'That is no light part of my penance,' pursued the Ghost. 'I am here\nto-night to warn you that you have yet a chance and hope of escaping my\nfate. A chance and hope of my procuring, Ebenezer.'\n\n'You were always a good friend to me,' said Scrooge. 'Thankee!'\n\n'You will be haunted,' resumed the Ghost, 'by Three Spirits.'\n\nScrooge's countenance fell almost as low as the Ghost's had done.\n\n'Is that the chance and hope you mentioned, Jacob?' he demanded in a\nfaltering voice.\n\n'It is.'\n\n'I--I think I'd rather not,' said Scrooge.\n\n'Without their visits,' said the Ghost, 'you cannot hope to shun the\npath I tread. Expect the first to-morrow when the bell tolls One.'\n\n'Couldn't I take 'em all at once, and have it over, Jacob?' hinted\nScrooge.\n\n'Expect the second on the next night at the same hour. The third, upon\nthe next night when the last stroke of Twelve has ceased to vibrate.\nLook to see me no more; and look that, for your own sake, you remember\nwhat has passed between us!'\n\nWhen it had said these words, the spectre took its wrapper from the\ntable, and bound it round its head as before. Scrooge knew this by the\nsmart sound its teeth made when the jaws were brought together by the\nbandage. He ventured to raise his eyes again, and found his supernatural\nvisitor confronting him in an erect attitude, with its chain wound over\nand about its arm.\n\n[Illustration: _The air was filled with phantoms, wandering hither and\nthither in restless haste and moaning as they went_]\n\nThe apparition walked backward from him; and, at every step it took, the\nwindow raised itself a little, so that, when the spectre reached it, it\nwas wide open. It beckoned Scrooge to approach, which he did. When they\nwere within two paces of each other, Marley's Ghost held up its hand,\nwarning him to come no nearer. Scrooge stopped.\n\nNot so much in obedience as in surprise and fear; for, on the raising of\nthe hand, he became sensible of confused noises in the air; incoherent\nsounds of lamentation and regret; wailings inexpressibly sorrowful and\nself-accusatory. The spectre, after listening for a moment, joined in\nthe mournful dirge; and floated out upon the bleak, dark night.\n\nScrooge followed to the window: desperate in his curiosity. He looked\nout.\n\nThe air was filled with phantoms, wandering hither and thither in\nrestless haste, and moaning as they went. Every one of them wore chains\nlike Marley's Ghost; some few (they might be guilty governments) were\nlinked together; none were free. Many had been personally known to\nScrooge in their lives. He had been quite familiar with one old ghost in\na white waistcoat, with a monstrous iron safe attached to its ankle, who\ncried piteously at being unable to assist a wretched woman with an\ninfant, whom it saw below upon a doorstep. The misery with them all was\nclearly, that they sought to interfere, for good, in human matters, and\nhad lost the power for ever.\n\nWhether these creatures faded into mist, or mist enshrouded them, he\ncould not tell. But they and their spirit voices faded together; and\nthe night became as it had been when he walked home.\n\nScrooge closed the window, and examined the door by which the Ghost had\nentered. It was double locked, as he had locked it with his own hands,\nand the bolts were undisturbed. He tried to say 'Humbug!' but stopped at\nthe first syllable. And being, from the emotions he had undergone, or\nthe fatigues of the day, or his glimpse of the Invisible World, or the\ndull conversation of the Ghost, or the lateness of the hour, much in\nneed of repose, went straight to bed without undressing, and fell asleep\nupon the instant.\n\n[Illustration]\n\n\nSTAVE TWO\n\n[Illustration]\n\n\n\n\nTHE FIRST OF THE THREE SPIRITS\n\n\nWhen Scrooge awoke it was so dark, that, looking out of bed, he could\nscarcely distinguish the transparent window from the opaque walls of his\nchamber. He was endeavouring to pierce the darkness with his ferret\neyes, when the chimes of a neighbouring church struck the four quarters.\nSo he listened for the hour.\n\nTo his great astonishment, the heavy bell went on from six to seven, and\nfrom seven to eight, and"}
16:34:30,705 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,707 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,774 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,775 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:30,776 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27877, Requested 6785. Please try again in 9.324s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:30,777 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '\'Bah!\' again; and followed it up with \'Humbug!\'\n\n\'Don\'t be cross, uncle!\' said the nephew.\n\n\'What else can I be,\' returned the uncle, \'when I live in such a world\nof fools as this? Merry Christmas! Out upon merry Christmas! What\'s\nChristmas-time to you but a time for paying bills without money; a time\nfor finding yourself a year older, and not an hour richer; a time for\nbalancing your books, and having every item in \'em through a round dozen\nof months presented dead against you? If I could work my will,\' said\nScrooge indignantly, \'every idiot who goes about with "Merry Christmas"\non his lips should be boiled with his own pudding, and buried with a\nstake of holly through his heart. He should!\'\n\n\'Uncle!\' pleaded the nephew.\n\n\'Nephew!\' returned the uncle sternly, \'keep Christmas in your own way,\nand let me keep it in mine.\'\n\n\'Keep it!\' repeated Scrooge\'s nephew. \'But you don\'t keep it.\'\n\n\'Let me leave it alone, then,\' said Scrooge. \'Much good may it do you!\nMuch good it has ever done you!\'\n\n\'There are many things from which I might have derived good, by which I\nhave not profited, I dare say,\' returned the nephew; \'Christmas among\nthe rest. But I am sure I have always thought of Christmas-time, when\nit has come round--apart from the veneration due to its sacred name and\norigin, if anything belonging to it can be apart from that--as a good\ntime; a kind, forgiving, charitable, pleasant time; the only time I know\nof, in the long calendar of the year, when men and women seem by one\nconsent to open their shut-up hearts freely, and to think of people\nbelow them as if they really were fellow-passengers to the grave, and\nnot another race of creatures bound on other journeys. And therefore,\nuncle, though it has never put a scrap of gold or silver in my pocket, I\nbelieve that it _has_ done me good and _will_ do me good; and I say, God\nbless it!\'\n\nThe clerk in the tank involuntarily applauded. Becoming immediately\nsensible of the impropriety, he poked the fire, and extinguished the\nlast frail spark for ever.\n\n\'Let me hear another sound from _you_,\' said Scrooge, \'and you\'ll keep\nyour Christmas by losing your situation! You\'re quite a powerful\nspeaker, sir,\' he added, turning to his nephew. \'I wonder you don\'t go\ninto Parliament.\'\n\n\'Don\'t be angry, uncle. Come! Dine with us to-morrow.\'\n\nScrooge said that he would see him----Yes, indeed he did. He went the\nwhole length of the expression, and said that he would see him in that\nextremity first.\n\n\'But why?\' cried Scrooge\'s nephew. \'Why?\'\n\n\'Why did you get married?\' said Scrooge.\n\n\'Because I fell in love.\'\n\n\'Because you fell in love!\' growled Scrooge, as if that were the only\none thing in the world more ridiculous than a merry Christmas. \'Good\nafternoon!\'\n\n\'Nay, uncle, but you never came to see me before that happened. Why give\nit as a reason for not coming now?\'\n\n\'Good afternoon,\' said Scrooge.\n\n\'I want nothing from you; I ask nothing of you; why cannot we be\nfriends?\'\n\n\'Good afternoon!\' said Scrooge.\n\n\'I am sorry, with all my heart, to find you so resolute. We have never\nhad any quarrel to which I have been a party. But I have made the trial\nin homage to Christmas, and I\'ll keep my Christmas humour to the last.\nSo A Merry Christmas, uncle!\'\n\n\'Good afternoon,\' said Scrooge.\n\n\'And A Happy New Year!\'\n\n\'Good afternoon!\' said Scrooge.\n\nHis nephew left the room without an angry word, notwithstanding. He\nstopped at the outer door to bestow the greetings of the season on the\nclerk, who, cold as he was, was warmer than Scrooge; for he returned\nthem cordially.\n\n\'There\'s another fellow,\' muttered Scrooge, who overheard him: \'my\nclerk, with fifteen shillings a week, and a wife and family, talking\nabout a merry Christmas. I\'ll retire to Bedlam.\'\n\nThis lunatic, in letting Scrooge\'s nephew out, had let two other people\nin. They were portly gentlemen, pleasant to behold, and now stood, with\ntheir hats off, in Scrooge\'s office. They had books and papers in their\nhands, and bowed to him.\n\n\'Scrooge and Marley\'s, I believe,\' said one of the gentlemen, referring\nto his list. \'Have I the pleasure of addressing Mr. Scrooge, or Mr.\nMarley?\'\n\n\'Mr. Marley has been dead these seven years,\' Scrooge replied. \'He died\nseven years ago, this very night.\'\n\n\'We have no doubt his liberality is well represented by his surviving\npartner,\' said the gentleman, presenting his credentials.\n\n[Illustration: THEY WERE PORTLY GENTLEMEN, PLEASANT TO BEHOLD]\n\nIt certainly was; for they had been two kindred spirits. At the ominous\nword \'liberality\' Scrooge frowned, and shook his head, and handed the\ncredentials back.\n\n\'At this festive season of the year, Mr. Scro'}
16:34:30,983 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:30,985 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,130 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,130 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27707, Requested 6806. Please try again in 9.026s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,131 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'that the crisp air laughed to hear it.\n\n\'These are but shadows of the things that have been,\' said the Ghost.\n\'They have no consciousness of us.\'\n\nThe jocund travellers came on; and as they came, Scrooge knew and named\nthem every one. Why was he rejoiced beyond all bounds to see them? Why\ndid his cold eye glisten, and his heart leap up as they went past? Why\nwas he filled with gladness when he heard them give each other Merry\nChristmas, as they parted at cross-roads and by-ways for their several\nhomes? What was merry Christmas to Scrooge? Out upon merry Christmas!\nWhat good had it ever done to him?\n\n\'The school is not quite deserted,\' said the Ghost. \'A solitary child,\nneglected by his friends, is left there still.\'\n\nScrooge said he knew it. And he sobbed.\n\nThey left the high-road by a well-remembered lane and soon approached a\nmansion of dull red brick, with a little weather-cock surmounted cupola\non the roof, and a bell hanging in it. It was a large house, but one of\nbroken fortunes; for the spacious offices were little used, their walls\nwere damp and mossy, their windows broken, and their gates decayed.\nFowls clucked and strutted in the stables; and the coach-houses and\nsheds were overrun with grass. Nor was it more retentive of its ancient\nstate within; for, entering the dreary hall, and glancing through the\nopen doors of many rooms, they found them poorly furnished, cold, and\nvast. There was an earthy savour in the air, a chilly bareness in the\nplace, which associated itself somehow with too much getting up by\ncandle light and not too much to eat.\n\nThey went, the Ghost and Scrooge, across the hall, to a door at the back\nof the house. It opened before them, and disclosed a long, bare,\nmelancholy room, made barer still by lines of plain deal forms and\ndesks. At one of these a lonely boy was reading near a feeble fire; and\nScrooge sat down upon a form, and wept to see his poor forgotten self as\nhe had used to be.\n\nNot a latent echo in the house, not a squeak and scuffle from the mice\nbehind the panelling, not a drip from the half-thawed waterspout in the\ndull yard behind, not a sigh among the leafless boughs of one despondent\npoplar, not the idle swinging of an empty storehouse door, no, not a\nclicking in the fire, but fell upon the heart of Scrooge with softening\ninfluence, and gave a freer passage to his tears.\n\nThe Spirit touched him on the arm, and pointed to his younger self,\nintent upon his reading. Suddenly a man in foreign garments, wonderfully\nreal and distinct to look at, stood outside the window, with an axe\nstuck in his belt, and leading by the bridle an ass laden with wood.\n\n\'Why, it\'s Ali Baba!\' Scrooge exclaimed in ecstasy. \'It\'s dear old\nhonest Ali Baba! Yes, yes, I know. One Christmas-time, when yonder\nsolitary child was left here all alone, he _did_ come, for the first\ntime, just like that. Poor boy! And Valentine,\' said Scrooge, \'and his\nwild brother, Orson; there they go! And what\'s his name, who was put\ndown in his drawers, asleep, at the gate of Damascus; don\'t you see him?\nAnd the Sultan\'s Groom turned upside down by the Genii; there he is upon\nhis head! Serve him right! I\'m glad of it. What business had he to be\nmarried to the Princess?\'\n\nTo hear Scrooge expending all the earnestness of his nature on such\nsubjects, in a most extraordinary voice between laughing and crying; and\nto see his heightened and excited face; would have been a surprise to\nhis business friends in the City, indeed.\n\n\'There\'s the Parrot!\' cried Scrooge. \'Green body and yellow tail, with a\nthing like a lettuce growing out of the top of his head; there he is!\nPoor Robin Crusoe he called him, when he came home again after sailing\nround the island. "Poor Robin Crusoe, where have you been, Robin\nCrusoe?" The man thought he was dreaming, but he wasn\'t. It was the\nParrot, you know. There goes Friday, running for his life to the little\ncreek! Halloa! Hoop! Halloo!\'\n\nThen, with a rapidity of transition very foreign to his usual character,\nhe said, in pity for his former self, \'Poor boy!\' and cried again.\n\n\'I wish,\' Scrooge muttered, putting his hand in his pocket, and looking\nabout him, after drying his eyes with his cuff; \'but it\'s too late now.\'\n\n\'What is the matter?\' asked the Spirit.\n\n\'Nothing,\' said Scrooge. \'Nothing. There was a boy singing a Christmas\ncarol at my door last night. I should like to have given him something:\nthat\'s all.\'\n\nThe Ghost smiled thoughtfully, and waved its hand, saying as it did so,\n\'Let us see another Christmas!\'\n\nScrooge\'s former self grew larger at the words, and the room became a\nlittle darker and more dirty. The panels shrunk, the windows cracked;\nfragments of plaster fell out of the ceiling, and the naked l'}
16:34:31,138 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,142 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,273 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,274 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,274 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27634, Requested 6837. Please try again in 8.942s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,275 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "ITS\n\n\nWhen Scrooge awoke it was so dark, that, looking out of bed, he could\nscarcely distinguish the transparent window from the opaque walls of his\nchamber. He was endeavouring to pierce the darkness with his ferret\neyes, when the chimes of a neighbouring church struck the four quarters.\nSo he listened for the hour.\n\nTo his great astonishment, the heavy bell went on from six to seven, and\nfrom seven to eight, and regularly up to twelve; then stopped. Twelve!\nIt was past two when he went to bed. The clock was wrong. An icicle must\nhave got into the works. Twelve!\n\nHe touched the spring of his repeater, to correct this most preposterous\nclock. Its rapid little pulse beat twelve, and stopped.\n\n'Why, it isn't possible,' said Scrooge, 'that I can have slept through a\nwhole day and far into another night. It isn't possible that anything\nhas happened to the sun, and this is twelve at noon!'\n\nThe idea being an alarming one, he scrambled out of bed, and groped his\nway to the window. He was obliged to rub the frost off with the sleeve\nof his dressing-gown before he could see anything; and could see very\nlittle then. All he could make out was, that it was still very foggy and\nextremely cold, and that there was no noise of people running to and\nfro, and making a great stir, as there unquestionably would have been if\nnight had beaten off bright day, and taken possession of the world. This\nwas a great relief, because 'Three days after sight of this First of\nExchange pay to Mr. Ebenezer Scrooge or his order,' and so forth, would\nhave become a mere United States security if there were no days to count\nby.\n\nScrooge went to bed again, and thought, and thought, and thought it over\nand over, and could make nothing of it. The more he thought, the more\nperplexed he was; and, the more he endeavoured not to think, the more he\nthought.\n\nMarley's Ghost bothered him exceedingly. Every time he resolved within\nhimself, after mature inquiry that it was all a dream, his mind flew\nback again, like a strong spring released, to its first position, and\npresented the same problem to be worked all through, 'Was it a dream or\nnot?'\n\nScrooge lay in this state until the chime had gone three-quarters more,\nwhen he remembered, on a sudden, that the Ghost had warned him of a\nvisitation when the bell tolled one. He resolved to lie awake until the\nhour was passed; and, considering that he could no more go to sleep than\ngo to heaven, this was, perhaps, the wisest resolution in his power.\n\nThe quarter was so long, that he was more than once convinced he must\nhave sunk into a doze unconsciously, and missed the clock. At length it\nbroke upon his listening ear.\n\n'Ding, dong!'\n\n'A quarter past,' said Scrooge, counting.\n\n'Ding, dong!'\n\n'Half past,' said Scrooge.\n\n'Ding, dong!'\n\n'A quarter to it.' said Scrooge.\n\n'Ding, dong!'\n\n'The hour itself,' said Scrooge triumphantly, 'and nothing else!'\n\nHe spoke before the hour bell sounded, which it now did with a deep,\ndull, hollow, melancholy ONE. Light flashed up in the room upon the\ninstant, and the curtains of his bed were drawn.\n\nThe curtains of his bed were drawn aside, I tell you, by a hand. Not\nthe curtains at his feet, nor the curtains at his back, but those to\nwhich his face was addressed. The curtains of his bed were drawn aside;\nand Scrooge, starting up into a half-recumbent attitude, found himself\nface to face with the unearthly visitor who drew them: as close to it as\nI am now to you, and I am standing in the spirit at your elbow.\n\nIt was a strange figure--like a child; yet not so like a child as like\nan old man, viewed through some supernatural medium, which gave him the\nappearance of having receded from the view, and being diminished to a\nchild's proportions. Its hair, which hung about its neck and down its\nback, was white, as if with age; and yet the face had not a wrinkle in\nit, and the tenderest bloom was on the skin. The arms were very long and\nmuscular; the hands the same, as if its hold were of uncommon strength.\nIts legs and feet, most delicately formed, were, like those upper\nmembers, bare. It wore a tunic of the purest white; and round its waist\nwas bound a lustrous belt, the sheen of which was beautiful. It held a\nbranch of fresh green holly in its hand; and, in singular contradiction\nof that wintry emblem, had its dress trimmed with summer flowers. But\nthe strangest thing about it was, that from the crown of its head there\nsprang a bright clear jet of light, by which all this was visible; and\nwhich was doubtless the occasion of its using, in its duller moments, a\ngreat extinguisher for a cap, which it now held under its arm.\n\nEven this, though, when Scrooge looked at it with increasing steadiness,\nwas _not_ its strangest quality. For, as its belt sparkled and\nglittered, now in one part and now in another, and what was light one\ninstant"}
16:34:31,399 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,401 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,401 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27564, Requested 6968. Please try again in 9.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,402 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'and thither in\n    restless haste and moaning as\n    they went                                                         32\n\n  Then old Fezziwig stood out to\n    dance with Mrs. Fezziwig                                          54\n\n  A flushed and boisterous group                                      62\n\n  Laden with Christmas toys and\n    presents                                                          64\n\n  The way he went after that plump\n    sister in the lace tucker!                                       100\n\n  "How are you?" said one.\n    "How are you?" returned the other.\n   "Well!" said the first. "Old\n    Scratch has got his own at last,\n    hey?"                                                            114\n\n  "What do you call this?" said Joe.\n    "Bed-curtains!" "Ah!" returned\n    the woman, laughing....\n    "Bed-curtains!"\n\n  "You don\'t mean to say you took\n    \'em down, rings and all, with him\n    lying there?" said Joe.\n\n  "Yes, I do," replied the woman.\n    "Why not?"                                                       120\n\n  "It\'s I, your uncle Scrooge. I have\n    come to dinner. Will you let\n    me in, Fred?"                                                    144\n\n  "Now, I\'ll tell you what, my friend,"\n    said Scrooge. "I am not going\n    to stand this sort of thing any\n    longer."                                                         146\n\n[Illustration]\n\n_IN BLACK AND WHITE_\n\n\n  Tailpiece                                                           vi\n  Tailpiece to List of Coloured Illustrations                          x\n  Tailpiece to List of Black and White Illustrations                  xi\n  Heading to Stave One                                                 3\n  They were portly gentlemen, pleasant to behold                      12\n  On the wings of the wind                                         28-29\n  Tailpiece to Stave One                                              34\n  Heading to Stave Two                                                37\n  He produced a decanter of curiously\n  light wine and a block of curiously heavy cake                      50\n  She left him, and they parted                                       60\n  Tailpiece to Stave Two                                              65\n  Heading to Stave Three                                              69\n  There was nothing very cheerful in the climate                      75\n  He had been Tim\'s blood-horse all the way from church            84-85\n  With the pudding                                                    88\n  Heading to Stave Four                                              111\n  Heading to Stave Five                                              137\n  Tailpiece to Stave Five                                            147\n\n[Illustration]\n\n\nSTAVE ONE\n\n\n[Illustration]\n\n\n\n\nMARLEY\'S GHOST\n\n\nMarley was dead, to begin with. There is no doubt whatever about that.\nThe register of his burial was signed by the clergyman, the clerk, the\nundertaker, and the chief mourner. Scrooge signed it. And Scrooge\'s name\nwas good upon \'Change for anything he chose to put his hand to. Old\nMarley was as dead as a door-nail.\n\nMind! I don\'t mean to say that I know of my own knowledge, what there is\nparticularly dead about a door-nail. I might have been inclined, myself,\nto regard a coffin-nail as the deadest piece of ironmongery in the\ntrade. But the wisdom of our ancestors is in the simile; and my\nunhallowed hands shall not disturb it, or the country\'s done for. You\nwill, therefore, permit me to repeat, emphatically, that Marley was as\ndead as a door-nail.\n\nScrooge knew he was dead? Of course he did. How could it be otherwise?\nScrooge and he were partners for I don\'t know how many years. Scrooge\nwas his sole executor, his sole administrator, his sole assign, his sole\nresiduary legatee, his sole friend, and sole mourner. And even Scrooge\nwas not so dreadfully cut up by the sad event but that he was an\nexcellent man of business on the very day of the funeral, and solemnised\nit with an undoubted bargain.\n\nThe mention of Marley\'s funeral brings me back to the point I started\nfrom. There is no doubt that Marley was dead. This must be distinctly\nunderstood, or nothing wonderful can come of the story I am going to\nrelate. If we were not perfectly convinced that Hamlet\'s father died\nbefore the play began, there would be nothing more remarkable in his\ntaking a stroll at night, in an easterly wind, upon his own ramparts,\nthan there would be in any other middle-aged gentleman rashly turning\nout after dark in a breezy spot--say St. Paul\'s Churchyard, for\ninstance--literally to astonish his son\'s weak mind.\n\nScrooge never painted out Old Marley\'s name. There it stood, years\nafterwards, above the warehouse door: Scrooge and Marley. The firm was\nknown as Scrooge and Marley. Sometimes people new to the business called\nScrooge Scrooge, and sometimes Marley, but he answered to both names. It\nwas all the same to him.\n\nOh! but he was a tight-fisted hand at the grindstone, Scrooge! a\nsqueezing, wrenching, grasping, scraping, clutching, covetous old\nsinner! Hard and sharp as flint, from which no steel had ever struck out\ngenerous fire; secret, and self-contained, and solitary as an oyster.\nThe cold within him froze his old features, nipped his pointed nose,\nshrivelled his cheek, stiffened his gait; made his'}
16:34:31,409 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,414 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,415 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27559, Requested 6779. Please try again in 8.676s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,416 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "-fisted hand at the grindstone, Scrooge! a\nsqueezing, wrenching, grasping, scraping, clutching, covetous old\nsinner! Hard and sharp as flint, from which no steel had ever struck out\ngenerous fire; secret, and self-contained, and solitary as an oyster.\nThe cold within him froze his old features, nipped his pointed nose,\nshrivelled his cheek, stiffened his gait; made his eyes red, his thin\nlips blue; and spoke out shrewdly in his grating voice. A frosty rime\nwas on his head, and on his eyebrows, and his wiry chin. He carried his\nown low temperature always about with him; he iced his office in the\ndog-days, and didn't thaw it one degree at Christmas.\n\nExternal heat and cold had little influence on Scrooge. No warmth could\nwarm, no wintry weather chill him. No wind that blew was bitterer than\nhe, no falling snow was more intent upon its purpose, no pelting rain\nless open to entreaty. Foul weather didn't know where to have him. The\nheaviest rain, and snow, and hail, and sleet could boast of the\nadvantage over him in only one respect. They often 'came down'\nhandsomely, and Scrooge never did.\n\nNobody ever stopped him in the street to say, with gladsome looks, 'My\ndear Scrooge, how are you? When will you come to see me?' No beggars\nimplored him to bestow a trifle, no children asked him what it was\no'clock, no man or woman ever once in all his life inquired the way to\nsuch and such a place, of Scrooge. Even the blind men's dogs appeared to\nknow him; and, when they saw him coming on, would tug their owners into\ndoorways and up courts; and then would wag their tails as though they\nsaid, 'No eye at all is better than an evil eye, dark master!'\n\nBut what did Scrooge care? It was the very thing he liked. To edge his\nway along the crowded paths of life, warning all human sympathy to keep\nits distance, was what the knowing ones call 'nuts' to Scrooge.\n\nOnce upon a time--of all the good days in the year, on Christmas\nEve--old Scrooge sat busy in his counting-house. It was cold, bleak,\nbiting weather; foggy withal; and he could hear the people in the court\noutside go wheezing up and down, beating their hands upon their breasts,\nand stamping their feet upon the pavement stones to warm them. The City\nclocks had only just gone three, but it was quite dark already--it had\nnot been light all day--and candles were flaring in the windows of the\nneighbouring offices, like ruddy smears upon the palpable brown air. The\nfog came pouring in at every chink and keyhole, and was so dense\nwithout, that, although the court was of the narrowest, the houses\nopposite were mere phantoms. To see the dingy cloud come drooping down,\nobscuring everything, one might have thought that nature lived hard by,\nand was brewing on a large scale.\n\nThe door of Scrooge's counting-house was open, that he might keep his\neye upon his clerk, who in a dismal little cell beyond, a sort of tank,\nwas copying letters. Scrooge had a very small fire, but the clerk's fire\nwas so very much smaller that it looked like one coal. But he couldn't\nreplenish it, for Scrooge kept the coal-box in his own room; and so\nsurely as the clerk came in with the shovel, the master predicted that\nit would be necessary for them to part. Wherefore the clerk put on his\nwhite comforter, and tried to warm himself at the candle; in which\neffort, not being a man of strong imagination, he failed.\n\n'A merry Christmas, uncle! God save you!' cried a cheerful voice. It was\nthe voice of Scrooge's nephew, who came upon him so quickly that this\nwas the first intimation he had of his approach.\n\n'Bah!' said Scrooge. 'Humbug!'\n\nHe had so heated himself with rapid walking in the fog and frost, this\nnephew of Scrooge's, that he was all in a glow; his face was ruddy and\nhandsome; his eyes sparkled, and his breath smoked again.\n\n'Christmas a humbug, uncle!' said Scrooge's nephew. 'You don't mean\nthat, I am sure?'\n\n'I do,' said Scrooge. 'Merry Christmas! What right have you to be merry?\nWhat reason have you to be merry? You're poor enough.'\n\n'Come, then,' returned the nephew gaily. 'What right have you to be\ndismal? What reason have you to be morose? You're rich enough.'\n\nScrooge, having no better answer ready on the spur of the moment, said,\n'Bah!' again; and followed it up with 'Humbug!'\n\n'Don't be cross, uncle!' said the nephew.\n\n'What else can I be,' returned the uncle, 'when I live in such a world\nof fools as this? Merry Christmas! Out upon merry Christmas! What's\nChristmas-time to you but a time for paying bills without money; a time\nfor finding yourself a year older, and not an hour richer; a time for\nbalancing your books"}
16:34:31,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,500 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,569 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,570 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,571 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27475, Requested 6810. Please try again in 8.57s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,573 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "nothing on which it is so hard as poverty; and there is nothing it\nprofesses to condemn with such severity as the pursuit of wealth!'\n\n'You fear the world too much,' she answered gently. 'All your other\nhopes have merged into the hope of being beyond the chance of its sordid\nreproach. I have seen your nobler aspirations fall off one by one, until\nthe master passion, Gain, engrosses you. Have I not?'\n\n'What then?' he retorted. 'Even if I have grown so much wiser, what\nthen? I am not changed towards you.'\n\nShe shook her head.\n\n'Am I?'\n\n'Our contract is an old one. It was made when we were both poor, and\ncontent to be so, until, in good season, we could improve our worldly\nfortune by our patient industry. You _are_ changed. When it was made you\nwere another man.'\n\n'I was a boy,' he said impatiently.\n\n'Your own feeling tells you that you were not what you are,' she\nreturned. 'I am. That which promised happiness when we were one in heart\nis fraught with misery now that we are two. How often and how keenly I\nhave thought of this I will not say. It is enough that I _have_ thought\nof it, and can release you.'\n\n'Have I ever sought release?'\n\n'In words. No. Never.'\n\n'In what, then?'\n\n'In a changed nature; in an altered spirit; in another atmosphere of\nlife; another Hope as its great end. In everything that made my love of\nany worth or value in your sight. If this had never been between us,'\nsaid the girl, looking mildly, but with steadiness, upon him; 'tell me,\nwould you seek me out and try to win me now? Ah, no!'\n\nHe seemed to yield to the justice of this supposition in spite of\nhimself. But he said, with a struggle, 'You think not.'\n\n'I would gladly think otherwise if I could,' she answered. 'Heaven\nknows! When _I_ have learned a Truth like this, I know how strong and\nirresistible it must be. But if you were free to-day, to-morrow,\nyesterday, can even I believe that you would choose a dowerless\ngirl--you who, in your very confidence with her, weigh everything by\nGain: or, choosing her, if for a moment you were false enough to your\none guiding principle to do so, do I not know that your repentance and\nregret would surely follow? I do; and I release you. With a full heart,\nfor the love of him you once were.'\n\n[Illustration: SHE LEFT HIM, AND THEY PARTED]\n\nHe was about to speak; but, with her head turned from him, she resumed:\n\n'You may--the memory of what is past half makes me hope you will--have\npain in this. A very, very brief time, and you will dismiss the\nrecollection of it gladly, as an unprofitable dream, from which it\nhappened well that you awoke. May you be happy in the life you have\nchosen!'\n\nShe left him, and they parted.\n\n'Spirit!' said Scrooge, 'show me no more! Conduct me home. Why do you\ndelight to torture me?'\n\n'One shadow more!' exclaimed the Ghost.\n\n'No more!' cried Scrooge. 'No more! I don't wish to see it. Show me no\nmore!'\n\nBut the relentless Ghost pinioned him in both his arms, and forced him\nto observe what happened next.\n\nThey were in another scene and place; a room, not very large or\nhandsome, but full of comfort. Near to the winter fire sat a beautiful\nyoung girl, so like that last that Scrooge believed it was the same,\nuntil he saw _her_, now a comely matron, sitting opposite her daughter.\nThe noise in this room was perfectly tumultuous, for there were more\nchildren there than Scrooge in his agitated state of mind could count;\nand, unlike the celebrated herd in the poem, they were not forty\nchildren conducting themselves like one, but every child was conducting\nitself like forty. The consequences were uproarious beyond belief; but\nno one seemed to care; on the contrary, the mother and daughter laughed\nheartily, and enjoyed it very much; and the latter, soon beginning to\nmingle in the sports, got pillaged by the young brigands most\nruthlessly. What would I not have given to be one of them! Though I\nnever could have been so rude, no, no! I wouldn't for the wealth of all\nthe world have crushed that braided hair, and torn it down; and for the\nprecious little shoe, I wouldn't have plucked it off, God bless my soul!\nto save my life. As to measuring her waist in sport, as they did, bold\nyoung brood, I couldn't have done it; I should have expected my arm to\nhave grown round it for a punishment, and never come straight again. And\nyet I should have dearly liked, I own, to have touched her lips; to have\nquestioned her, that she might have opened them; to have looked upon the\nlashes of her downcast eyes, and never raised a blush; to have let loose\nwaves of hair, an inch of which would be a keepsake beyond price: in\nshort, I should have liked, I do confess, to have had the lightest\nlicense of a child, and yet to have been man enough to know its"}
16:34:31,607 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,608 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,745 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,747 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,747 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27402, Requested 6779. Please try again in 8.362s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,748 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '\ufeffThe Project Gutenberg eBook of A Christmas Carol\n    \nThis ebook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this ebook or online\nat www.gutenberg.org. If you are not located in the United States,\nyou will have to check the laws of the country where you are located\nbefore using this eBook.\n\nTitle: A Christmas Carol\n\nAuthor: Charles Dickens\n\nIllustrator: Arthur Rackham\n\nRelease date: December 24, 2007 [eBook #24022]\n\nLanguage: English\n\nOriginal publication: Philadelphia and New York: J. B. Lippincott Company,, 1915\n\nCredits: Produced by Suzanne Shell, Janet Blenkinship and the Online\n        Distributed Proofreading Team at http://www.pgdp.net\n\n\n*** START OF THE PROJECT GUTENBERG EBOOK A CHRISTMAS CAROL ***\n\n\n\n\nProduced by Suzanne Shell, Janet Blenkinship and the Online\nDistributed Proofreading Team at http://www.pgdp.net\n\n\n\n\n\n\n\n\n\n\n\n  A CHRISTMAS CAROL\n\n  [Illustration: _"How now?" said Scrooge, caustic and cold as ever.\n  "What do you want with me?"_]\n\n\n  A CHRISTMAS CAROL\n\n  [Illustration]\n\n  BY\n\n  CHARLES DICKENS\n\n  [Illustration]\n\n  ILLUSTRATED BY ARTHUR RACKHAM\n\n  [Illustration]\n\n  J. B. LIPPINCOTT COMPANY PHILADELPHIA AND NEW YORK\n\n  FIRST PUBLISHED 1915\n\n  REPRINTED 1923, 1927, 1932, 1933, 1934, 1935, 1947, 1948, 1952, 1958,\n  1962, 1964, 1966, 1967, 1969, 1971, 1972, 1973\n\n  ISBN: 0-397-00033-2\n\n  PRINTED IN GREAT BRITAIN\n\n\n\n\n  PREFACE\n\n  I have endeavoured in this Ghostly little book to raise the Ghost of an\n  Idea which shall not put my readers out of humour with themselves, with\n  each other, with the season, or with me. May it haunt their house\n  pleasantly, and no one wish to lay it.\n\n  Their faithful Friend and Servant,\n\n  C. D.\n\n  _December, 1843._\n\n\n\n\n  CHARACTERS\n\n  Bob Cratchit, clerk to Ebenezer Scrooge.\n  Peter Cratchit, a son of the preceding.\n  Tim Cratchit ("Tiny Tim"), a cripple, youngest son of Bob Cratchit.\n  Mr. Fezziwig, a kind-hearted, jovial old merchant.\n  Fred, Scrooge\'s nephew.\n  Ghost of Christmas Past, a phantom showing things past.\n  Ghost of Christmas Present, a spirit of a kind, generous,\n    and hearty nature.\n  Ghost of Christmas Yet to Come, an apparition showing the shadows\n    of things which yet may happen.\n  Ghost of Jacob Marley, a spectre of Scrooge\'s former partner in business.\n  Joe, a marine-store dealer and receiver of stolen goods.\n  Ebenezer Scrooge, a grasping, covetous old man, the surviving partner\n    of the firm of Scrooge and Marley.\n  Mr. Topper, a bachelor.\n  Dick Wilkins, a fellow apprentice of Scrooge\'s.\n\n  Belle, a comely matron, an old sweetheart of Scrooge\'s.\n  Caroline, wife of one of Scrooge\'s debtors.\n  Mrs. Cratchit, wife of Bob Cratchit.\n  Belinda and Martha Cratchit, daughters of the preceding.\n\n  Mrs. Dilber, a laundress.\n  Fan, the sister of Scrooge.\n  Mrs. Fezziwig, the worthy partner of Mr. Fezziwig.\n\n\n\n\n  CONTENTS\n\n  STAVE ONE--MARLEY\'S GHOST                                             3\n  STAVE TWO--THE FIRST OF THE THREE SPIRITS                            37\n  STAVE THREE--THE SECOND OF THE THREE SPIRITS                         69\n  STAVE FOUR--THE LAST OF THE SPIRITS                                 111\n  STAVE FIVE--THE END OF IT                                           137\n\n\n  LIST OF ILLUSTRATIONS\n\n  _IN COLOUR_\n\n\n  "How now?" said Scrooge, caustic\n    and cold as ever. "What do you\n    want with me?"                                           _Frontispiece_\n\n  Bob Cratchit went down a slide on\n    Cornhill, at the end of a lane of\n    boys, twenty times, in honour of\n    its being Christmas Eve                                           16\n\n  Nobody under the bed; nobody in\n    the closet; nobody in his dressing-gown,\n    which was hanging up\n    in a suspicious attitude against\n    the wall                                                          20\n\n  The air was filled with phantoms,\n   wandering hither and thither in\n    restless haste and moaning as\n    they went                                                         32\n\n  Then old Fezziwig stood out to\n    dance with Mrs. Fezziwig                                          54\n\n  A flushed and boisterous group                                      62\n\n  Laden with Christmas toys and\n    presents                                                          64\n\n  The way he went after that plump\n    sister in the lace tucker!                                       100\n\n  "How are you?" said one.'}
16:34:31,760 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,762 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,762 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,868 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,869 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,870 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27339, Requested 6834. Please try again in 8.346s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,871 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "would\nbe untrue. But he put his hand upon the key he had relinquished, turned\nit sturdily, walked in, and lighted his candle.\n\nHe _did_ pause, with a moment's irresolution, before he shut the door;\nand he _did_ look cautiously behind it first, as if he half expected to\nbe terrified with the sight of Marley's pigtail sticking out into the\nhall. But there was nothing on the back of the door, except the screws\nand nuts that held the knocker on, so he said, 'Pooh, pooh!' and closed\nit with a bang.\n\nThe sound resounded through the house like thunder. Every room above,\nand every cask in the wine-merchant's cellars below, appeared to have a\nseparate peal of echoes of its own. Scrooge was not a man to be\nfrightened by echoes. He fastened the door, and walked across the hall,\nand up the stairs: slowly, too: trimming his candle as he went.\n\nYou may talk vaguely about driving a coach and six up a good old flight\nof stairs, or through a bad young Act of Parliament; but I mean to say\nyou might have got a hearse up that staircase, and taken it broadwise,\nwith the splinter-bar towards the wall, and the door towards the\nbalustrades: and done it easy. There was plenty of width for that, and\nroom to spare; which is perhaps the reason why Scrooge thought he saw a\nlocomotive hearse going on before him in the gloom. Half-a-dozen\ngas-lamps out of the street wouldn't have lighted the entry too well, so\nyou may suppose that it was pretty dark with Scrooge's dip.\n\nUp Scrooge went, not caring a button for that. Darkness is cheap, and\nScrooge liked it. But, before he shut his heavy door, he walked through\nhis rooms to see that all was right. He had just enough recollection of\nthe face to desire to do that.\n\nSitting-room, bedroom, lumber-room. All as they should be. Nobody under\nthe table, nobody under the sofa; a small fire in the grate; spoon and\nbasin ready; and the little saucepan of gruel (Scrooge had a cold in his\nhead) upon the hob. Nobody under the bed; nobody in the closet; nobody\nin his dressing-gown, which was hanging up in a suspicious attitude\nagainst the wall. Lumber-room as usual. Old fire-guard, old shoes, two\nfish baskets, washing-stand on three legs, and a poker.\n\n[Illustration: _Nobody under the bed; nobody in the closet; nobody in\nhis dressing-gown, which was hanging up in a suspicious attitude against\nthe wall_]\n\nQuite satisfied, he closed his door, and locked himself in; double\nlocked himself in, which was not his custom. Thus secured against\nsurprise, he took off his cravat; put on his dressing-gown and slippers,\nand his nightcap; and sat down before the fire to take his gruel.\n\nIt was a very low fire indeed; nothing on such a bitter night. He was\nobliged to sit close to it, and brood over it, before he could extract\nthe least sensation of warmth from such a handful of fuel. The fireplace\nwas an old one, built by some Dutch merchant long ago, and paved all\nround with quaint Dutch tiles, designed to illustrate the Scriptures.\nThere were Cains and Abels, Pharaoh's daughters, Queens of Sheba,\nAngelic messengers descending through the air on clouds like\nfeather-beds, Abrahams, Belshazzars, Apostles putting off to sea in\nbutter-boats, hundreds of figures to attract his thoughts; and yet that\nface of Marley, seven years dead, came like the ancient Prophet's rod,\nand swallowed up the whole. If each smooth tile had been a blank at\nfirst, with power to shape some picture on its surface from the\ndisjointed fragments of his thoughts, there would have been a copy of\nold Marley's head on every one.\n\n'Humbug!' said Scrooge; and walked across the room.\n\nAfter several turns he sat down again. As he threw his head back in the\nchair, his glance happened to rest upon a bell, a disused bell, that\nhung in the room, and communicated, for some purpose now forgotten, with\na chamber in the highest storey of the building. It was with great\nastonishment, and with a strange, inexplicable dread, that, as he\nlooked, he saw this bell begin to swing. It swung so softly in the\noutset that it scarcely made a sound; but soon it rang out loudly, and\nso did every bell in the house.\n\nThis might have lasted half a minute, or a minute, but it seemed an\nhour. The bells ceased, as they had begun, together. They were succeeded\nby a clanking noise deep down below as if some person were dragging a\nheavy chain over the casks in the wine-merchant's cellar. Scrooge then\nremembered to have heard that ghosts in haunted houses were described as\ndragging chains.\n\nThe cellar door flew open with a booming sound, and then he heard the\nnoise much louder on the floors below; then coming up the stairs; then\ncoming straight towards his door.\n\n'It's humbug still!' said Scrooge. 'I won't believe it.'\n\nHis colour changed, though, when, without a pause, it came on through\nthe heavy door and passed into the"}
16:34:31,881 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,882 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:31,882 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27322, Requested 6849. Please try again in 8.342s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:31,883 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "scales descending on the counter made a merry sound, or that\nthe twine and roller parted company so briskly, or that the canisters\nwere rattled up and down like juggling tricks, or even that the blended\nscents of tea and coffee were so grateful to the nose, or even that the\nraisins were so plentiful and rare, the almonds so extremely white, the\nsticks of cinnamon so long and straight, the other spices so delicious,\nthe candied fruits so caked and spotted with molten sugar as to make the\ncoldest lookers-on feel faint, and subsequently bilious. Nor was it that\nthe figs were moist and pulpy, or that the French plums blushed in\nmodest tartness from their highly-decorated boxes, or that everything\nwas good to eat and in its Christmas dress; but the customers were all\nso hurried and so eager in the hopeful promise of the day, that they\ntumbled up against each other at the door, crashing their wicker baskets\nwildly, and left their purchases upon the counter, and came running\nback to fetch them, and committed hundreds of the like mistakes, in the\nbest humour possible; while the grocer and his people were so frank and\nfresh, that the polished hearts with which they fastened their aprons\nbehind might have been their own, worn outside for general inspection,\nand for Christmas daws to peck at if they chose.\n\nBut soon the steeples called good people all to church and chapel, and\naway they came, flocking through the streets in their best clothes and\nwith their gayest faces. And at the same time there emerged, from scores\nof by-streets, lanes, and nameless turnings, innumerable people,\ncarrying their dinners to the bakers' shops. The sight of these poor\nrevellers appeared to interest the Spirit very much, for he stood with\nScrooge beside him in a baker's doorway, and, taking off the covers as\ntheir bearers passed, sprinkled incense on their dinners from his torch.\nAnd it was a very uncommon kind of torch, for once or twice, when there\nwere angry words between some dinner-carriers who had jostled each\nother, he shed a few drops of water on them from it, and their\ngood-humour was restored directly. For they said, it was a shame to\nquarrel upon Christmas Day. And so it was! God love it, so it was!\n\nIn time the bells ceased, and the bakers were shut up; and yet there was\na genial shadowing forth of all these dinners, and the progress of their\ncooking, in the thawed blotch of wet above each baker's oven, where the\npavement smoked as if its stones were cooking too.\n\n'Is there a peculiar flavour in what you sprinkle from your torch?'\nasked Scrooge.\n\n'There is. My own.'\n\n'Would it apply to any kind of dinner on this day?' asked Scrooge.\n\n'To any kindly given. To a poor one most.'\n\n'Why to a poor one most?' asked Scrooge.\n\n'Because it needs it most.'\n\n'Spirit!' said Scrooge, after a moment's thought, 'I wonder you, of all\nthe beings in the many worlds about us, should desire to cramp these\npeople's opportunities of innocent enjoyment.\n\n'I!' cried the Spirit.\n\n'You would deprive them of their means of dining every seventh day,\noften the only day on which they can be said to dine at all,' said\nScrooge; 'wouldn't you?'\n\n'I!' cried the Spirit.\n\n'You seek to close these places on the Seventh Day,' said Scrooge. 'And\nit comes to the same thing.'\n\n'I seek!' exclaimed the Spirit.\n\n'Forgive me if I am wrong. It has been done in your name, or at least in\nthat of your family,' said Scrooge.\n\n'There are some upon this earth of yours,' returned the Spirit, 'who\nlay claim to know us, and who do their deeds of passion, pride,\nill-will, hatred, envy, bigotry, and selfishness in our name, who are as\nstrange to us, and all our kith and kin, as if they had never lived.\nRemember that, and charge their doings on themselves, not us.'\n\nScrooge promised that he would; and they went on, invisible, as they had\nbeen before, into the suburbs of the town. It was a remarkable quality\nof the Ghost (which Scrooge had observed at the baker's), that\nnotwithstanding his gigantic size, he could accommodate himself to any\nplace with ease; and that he stood beneath a low roof quite as\ngracefully and like a supernatural creature as it was possible he could\nhave done in any lofty hall.\n\nAnd perhaps it was the pleasure the good Spirit had in showing off this\npower of his, or else it was his own kind, generous, hearty nature, and\nhis sympathy with all poor men, that led him straight to Scrooge's\nclerk's; for there he went, and took Scrooge with him, holding to his\nrobe; and on the threshold of the door the Spirit smiled, and stopped to\nbless Bob Cratchit's dwelling with the sprinklings of his torch. Think\nof that! Bob had but fifteen 'Bob' a week himself; he pocketed on\nSaturdays but fifteen copies of his Christian name; and yet the Ghost of\nChristmas Present blessed his four-roomed house!\n\nThen up rose Mrs. Cratchit, Cratchit's wife, dressed out but poorly in a\ntw"}
16:34:31,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:31,907 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,102 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,102 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27207, Requested 6816. Please try again in 8.046s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,103 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "tell\nme if Tiny Tim will live.'\n\n'I see a vacant seat,' replied the Ghost, 'in the poor chimney corner,\nand a crutch without an owner, carefully preserved. If these shadows\nremain unaltered by the Future, the child will die.'\n\n'No, no,' said Scrooge. 'Oh no, kind Spirit! say he will be spared.'\n\n'If these shadows remain unaltered by the Future none other of my race,'\nreturned the Ghost, 'will find him here. What then? If he be like to\ndie, he had better do it, and decrease the surplus population.'\n\nScrooge hung his head to hear his own words quoted by the Spirit, and\nwas overcome with penitence and grief.\n\n'Man,' said the Ghost, 'if man you be in heart, not adamant, forbear\nthat wicked cant until you have discovered what the surplus is, and\nwhere it is. Will you decide what men shall live, what men shall die? It\nmay be that, in the sight of Heaven, you are more worthless and less fit\nto live than millions like this poor man's child. O God! to hear the\ninsect on the leaf pronouncing on the too much life among his hungry\nbrothers in the dust!'\n\nScrooge bent before the Ghost's rebuke, and, trembling, cast his eyes\nupon the ground. But he raised them speedily on hearing his own name.\n\n'Mr. Scrooge!' said Bob. 'I'll give you Mr. Scrooge, the Founder of the\nFeast!'\n\n'The Founder of the Feast, indeed!' cried Mrs. Cratchit, reddening. 'I\nwish I had him here. I'd give him a piece of my mind to feast upon, and\nI hope he'd have a good appetite for it.'\n\n'My dear,' said Bob, 'the children! Christmas Day.'\n\n'It should be Christmas Day, I am sure,' said she, 'on which one drinks\nthe health of such an odious, stingy, hard, unfeeling man as Mr.\nScrooge. You know he is, Robert! Nobody knows it better than you do,\npoor fellow!'\n\n'My dear!' was Bob's mild answer. 'Christmas Day.'\n\n'I'll drink his health for your sake and the Day's,' said Mrs. Cratchit,\n'not for his. Long life to him! A merry Christmas and a happy New Year!\nHe'll be very merry and very happy, I have no doubt!'\n\nThe children drank the toast after her. It was the first of their\nproceedings which had no heartiness in it. Tiny Tim drank it last of\nall, but he didn't care twopence for it. Scrooge was the Ogre of the\nfamily. The mention of his name cast a dark shadow on the party, which\nwas not dispelled for full five minutes.\n\nAfter it had passed away they were ten times merrier than before, from\nthe mere relief of Scrooge the Baleful being done with. Bob Cratchit\ntold them how he had a situation in his eye for Master Peter, which\nwould bring in, if obtained, full five-and-sixpence weekly. The two\nyoung Cratchits laughed tremendously at the idea of Peter's being a man\nof business; and Peter himself looked thoughtfully at the fire from\nbetween his collars, as if he were deliberating what particular\ninvestments he should favour when he came into the receipt of that\nbewildering income. Martha, who was a poor apprentice at a milliner's,\nthen told them what kind of work she had to do, and how many hours she\nworked at a stretch and how she meant to lie abed to-morrow morning for\na good long rest; to-morrow being a holiday she passed at home. Also how\nshe had seen a countess and a lord some days before, and how the lord\n'was much about as tall as Peter'; at which Peter pulled up his collar\nso high that you couldn't have seen his head if you had been there. All\nthis time the chestnuts and the jug went round and round; and by-and-by\nthey had a song, about a lost child travelling in the snow, from Tiny\nTim, who had a plaintive little voice, and sang it very well indeed.\n\nThere was nothing of high mark in this. They were not a handsome family;\nthey were not well dressed; their shoes were far from being waterproof;\ntheir clothes were scanty; and Peter might have known, and very likely\ndid, the inside of a pawnbroker's. But they were happy, grateful,\npleased with one another, and contented with the time; and when they\nfaded, and looked happier yet in the bright sprinklings of the Spirit's\ntorch at parting, Scrooge had his eye upon them, and especially on Tiny\nTim, until the last.\n\nBy this time it was getting dark, and snowing pretty heavily; and as\nScrooge and the Spirit went along the streets, the brightness of the\nroaring fires in kitchens, parlours, and all sorts of rooms was\nwonderful. Here, the flickering of the blaze showed preparations for a\ncosy dinner, with hot plates baking through and through before the fire,\nand deep red curtains, ready to be drawn to shut out cold and darkness.\nThere, all the children of the house were running out into the snow to\nmeet their married sisters, brothers, cousins, uncles, aunts, and be the\nfirst to greet them. Here, again, were shadows on the window-blinds of\nguests assembling; and there a group of"}
16:34:32,121 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,122 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,187 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,188 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,188 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27166, Requested 6753. Please try again in 7.837s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,188 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "in. At sight of an old gentleman in a Welsh wig, sitting\nbehind such a high desk, that if he had been two inches taller, he must\nhave knocked his head against the ceiling, Scrooge cried in great\nexcitement--\n\n'Why, it's old Fezziwig! Bless his heart, it's Fezziwig alive again!'\n\nOld Fezziwig laid down his pen, and looked up at the clock, which\npointed to the hour of seven. He rubbed his hands; adjusted his\ncapacious waistcoat; laughed all over himself, from his shoes to his\norgan of benevolence; and called out, in a comfortable, oily, rich, fat,\njovial voice--\n\n'Yo ho, there! Ebenezer! Dick!'\n\nScrooge's former self, now grown a young man, came briskly in,\naccompanied by his fellow-'prentice.\n\n'Dick Wilkins, to be sure!' said Scrooge to the Ghost. 'Bless me, yes.\nThere he is. He was very much attached to me, was Dick. Poor Dick! Dear,\ndear!'\n\n'Yo ho, my boys!' said Fezziwig. 'No more work to-night. Christmas Eve,\nDick. Christmas, Ebenezer! Let's have the shutters up,' cried old\nFezziwig, with a sharp clap of his hands, 'before a man can say Jack\nRobinson!'\n\nYou wouldn't believe how those two fellows went at it! They charged into\nthe street with the shutters--one, two, three--had 'em up in their\nplaces--four, five, six--barred 'em and pinned 'em--seven, eight,\nnine--and came back before you could have got to twelve, panting like\nracehorses.\n\n'Hilli-ho!' cried old Fezziwig, skipping down from the high desk with\nwonderful agility. 'Clear away, my lads, and let's have lots of room\nhere! Hilli-ho, Dick! Chirrup, Ebenezer!'\n\nClear away! There was nothing they wouldn't have cleared away, or\ncouldn't have cleared away, with old Fezziwig looking on. It was done in\na minute. Every movable was packed off, as if it were dismissed from\npublic life for evermore; the floor was swept and watered, the lamps\nwere trimmed, fuel was heaped upon the fire; and the warehouse was as\nsnug, and warm, and dry, and bright a ball-room as you would desire to\nsee upon a winter's night.\n\nIn came a fiddler with a music-book, and went up to the lofty desk, and\nmade an orchestra of it, and tuned like fifty stomach-aches. In came\nMrs. Fezziwig, one vast substantial smile. In came the three Miss\nFezziwigs, beaming and lovable. In came the six young followers whose\nhearts they broke. In came all the young men and women employed in the\nbusiness. In came the housemaid, with her cousin the baker. In came the\ncook with her brother's particular friend the milkman. In came the boy\nfrom over the way, who was suspected of not having board enough from his\nmaster; trying to hide himself behind the girl from next door but one,\nwho was proved to have had her ears pulled by her mistress. In they all\ncame, one after another; some shyly, some boldly, some gracefully, some\nawkwardly, some pushing, some pulling; in they all came, any how and\nevery how. Away they all went, twenty couple at once; hands half round\nand back again the other way; down the middle and up again; round and\nround in various stages of affectionate grouping; old top couple always\nturning up in the wrong place; new top couple starting off again as soon\nas they got there; all top couples at last, and not a bottom one to help\nthem! When this result was brought about, old Fezziwig, clapping his\nhands to stop the dance, cried out, 'Well done!' and the fiddler plunged\nhis hot face into a pot of porter, especially provided for that purpose.\nBut, scorning rest upon his reappearance, he instantly began again,\nthough there were no dancers yet, as if the other fiddler had been\ncarried home, exhausted, on a shutter, and he were a bran-new man\nresolved to beat him out of sight, or perish.\n\n[Illustration: _Then old Fezziwig stood out to dance with Mrs.\nFezziwig_]\n\nThere were more dances, and there were forfeits, and more dances, and\nthere was cake, and there was negus, and there was a great piece of Cold\nRoast, and there was a great piece of Cold Boiled, and there were\nmince-pies, and plenty of beer. But the great effect of the evening came\nafter the Roast and Boiled, when the fiddler (an artful dog, mind! The\nsort of man who knew his business better than you or I could have told\nit him!) struck up 'Sir Roger de Coverley.' Then old Fezziwig stood\nout to dance with Mrs. Fezziwig. Top couple, too; with a good stiff\npiece of work cut out for them; three or four and twenty pair of\npartners; people who were not to be trifled with; people who would\ndance, and had no notion of walking.\n\nBut if they had been twice as many--ah!"}
16:34:32,257 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,259 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,268 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,269 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,345 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,347 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,348 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27100, Requested 6842. Please try again in 7.883s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,349 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "that he turned\nuncomfortably cold when he began to wonder which of his curtains this\nnew spectre would draw back, he put them every one aside with his own\nhands, and, lying down again, established a sharp look-out all round the\nbed. For he wished to challenge the Spirit on the moment of its\nappearance, and did not wish to be taken by surprise and made nervous.\n\nGentlemen of the free-and-easy sort, who plume themselves on being\nacquainted with a move or two, and being usually equal to the time of\nday, express the wide range of their capacity for adventure by observing\nthat they are good for anything from pitch-and-toss to manslaughter;\nbetween which opposite extremes, no doubt, there lies a tolerably wide\nand comprehensive range of subjects. Without venturing for Scrooge quite\nas hardily as this, I don't mind calling on you to believe that he was\nready for a good broad field of strange appearances, and that nothing\nbetween a baby and a rhinoceros would have astonished him very much.\n\nNow, being prepared for almost anything, he was not by any means\nprepared for nothing; and consequently, when the bell struck One, and no\nshape appeared, he was taken with a violent fit of trembling. Five\nminutes, ten minutes, a quarter of an hour went by, yet nothing came.\nAll this time he lay upon his bed, the very core and centre of a blaze\nof ruddy light, which streamed upon it when the clock proclaimed the\nhour; and which, being only light, was more alarming than a dozen\nghosts, as he was powerless to make out what it meant, or would be at;\nand was sometimes apprehensive that he might be at that very moment an\ninteresting case of spontaneous combustion, without having the\nconsolation of knowing it. At last, however, he began to think--as you\nor I would have thought at first; for it is always the person not in the\npredicament who knows what ought to have been done in it, and would\nunquestionably have done it too--at last, I say, he began to think that\nthe source and secret of this ghostly light might be in the adjoining\nroom, from whence, on further tracing it, it seemed to shine. This idea\ntaking full possession of his mind, he got up softly, and shuffled in\nhis slippers to the door.\n\nThe moment Scrooge's hand was on the lock a strange voice called him by\nhis name, and bade him enter. He obeyed.\n\nIt was his own room. There was no doubt about that. But it had undergone\na surprising transformation. The walls and ceiling were so hung with\nliving green, that it looked a perfect grove; from every part of which\nbright gleaming berries glistened. The crisp leaves of holly, mistletoe,\nand ivy reflected back the light, as if so many little mirrors had been\nscattered there; and such a mighty blaze went roaring up the chimney as\nthat dull petrification of a hearth had never known in Scrooge's time,\nor Marley's, or for many and many a winter season gone. Heaped up on the\nfloor, to form a kind of throne, were turkeys, geese, game, poultry,\nbrawn, great joints of meat, sucking-pigs, long wreaths of sausages,\nmince-pies, plum-puddings, barrels of oysters, red-hot chestnuts,\ncherry-cheeked apples, juicy oranges, luscious pears, immense\ntwelfth-cakes, and seething bowls of punch, that made the chamber dim\nwith their delicious steam. In easy state upon this couch there sat a\njolly Giant, glorious to see; who bore a glowing torch, in shape not\nunlike Plenty's horn, and held it up, high up, to shed its light on\nScrooge as he came peeping round the door.\n\n'Come in!' exclaimed the Ghost. 'Come in! and know me better, man!'\n\nScrooge entered timidly, and hung his head before this Spirit. He was\nnot the dogged Scrooge he had been; and though the Spirit's eyes were\nclear and kind, he did not like to meet them.\n\n'I am the Ghost of Christmas Present,' said the Spirit. 'Look upon me!'\n\nScrooge reverently did so. It was clothed in one simple deep green robe,\nor mantle, bordered with white fur. This garment hung so loosely on the\nfigure, that its capacious breast was bare, as if disdaining to be\nwarded or concealed by any artifice. Its feet, observable beneath the\nample folds of the garment, were also bare; and on its head it wore no\nother covering than a holly wreath, set here and there with shining\nicicles. Its dark-brown curls were long and free; free as its genial\nface, its sparkling eye, its open hand, its cheery voice, its\nunconstrained demeanour, and its joyful air. Girded round its middle was\nan antique scabbard: but no sword was in it, and the ancient sheath was\neaten up with rust.\n\n'You have never seen the like of me before!' exclaimed the Spirit.\n\n'Never,' Scrooge made answer to it.\n\n'Have never walked forth with the younger members of my family; meaning\n(for I am very young) my elder brothers born in these later years?'\npursued the Phantom.\n\n'I don't think I have,' said Scrooge. 'I am afraid I have not. Have"}
16:34:32,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,382 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,382 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27083, Requested 6809. Please try again in 7.784s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,383 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "ation]\n\nSuch a bustle ensued that you might have thought a goose the rarest of\nall birds; a feathered phenomenon, to which a black swan was a matter of\ncourse--and, in truth, it was something very like it in that house. Mrs.\nCratchit made the gravy (ready beforehand in a little saucepan) hissing\nhot; Master Peter mashed the potatoes with incredible vigour; Miss\nBelinda sweetened up the apple sauce; Martha dusted the hot plates; Bob\ntook Tiny Tim beside him in a tiny corner at the table; the two young\nCratchits set chairs for everybody, not forgetting themselves, and,\nmounting guard upon their posts, crammed spoons into their mouths, lest\nthey should shriek for goose before their turn came to be helped. At\nlast the dishes were set on, and grace was said. It was succeeded by a\nbreathless pause, as Mrs. Cratchit, looking slowly all along the\ncarving-knife, prepared to plunge it in the breast; but when she did,\nand when the long-expected gush of stuffing issued forth, one murmur of\ndelight arose all round the board, and even Tiny Tim, excited by the two\nyoung Cratchits, beat on the table with the handle of his knife and\nfeebly cried Hurrah!\n\n[Illustration: HE HAD BEEN TIM'S BLOOD-HORSE ALL THE WAY FROM CHURCH]\n\nThere never was such a goose. Bob said he didn't believe there ever was\nsuch a goose cooked. Its tenderness and flavour, size and cheapness,\nwere the themes of universal admiration. Eked out by apple sauce and\nmashed potatoes, it was a sufficient dinner for the whole family;\nindeed, as Mrs. Cratchit said with great delight (surveying one small\natom of a bone upon the dish), they hadn't ate it all at last! Yet every\none had had enough, and the youngest Cratchits, in particular, were\nsteeped in sage and onion to the eyebrows! But now, the plates being\nchanged by Miss Belinda, Mrs. Cratchit left the room alone--too nervous\nto bear witnesses--to take the pudding up, and bring it in.\n\nSuppose it should not be done enough! Suppose it should break in turning\nout! Suppose somebody should have got over the wall of the back-yard and\nstolen it, while they were merry with the goose--a supposition at which\nthe two young Cratchits became livid! All sorts of horrors were\nsupposed.\n\nHallo! A great deal of steam! The pudding was out of the copper. A smell\nlike a washing-day! That was the cloth. A smell like an eating-house and\na pastry-cook's next door to each other, with a laundress's next door to\nthat! That was the pudding! In half a minute Mrs. Cratchit\nentered--flushed, but smiling proudly--with the pudding, like a speckled\ncannon-ball, so hard and firm, blazing in half of half-a-quartern of\nignited brandy, and bedight with Christmas holly stuck into the top.\n\nOh, a wonderful pudding! Bob Cratchit said, and calmly too, that he\nregarded it as the greatest success achieved by Mrs. Cratchit since\ntheir marriage. Mrs. Cratchit said that, now the weight was off her\nmind, she would confess she had her doubts about the quantity of flour.\nEverybody had something to say about it, but nobody said or thought it\nwas at all a small pudding for a large family. It would have been flat\nheresy to do so. Any Cratchit would have blushed to hint at such a\nthing.\n\n[Illustration: WITH THE PUDDING]\n\nAt last the dinner was all done, the cloth was cleared, the hearth\nswept, and the fire made up. The compound in the jug being tasted and\nconsidered perfect, apples and oranges were put upon the table, and a\nshovel full of chestnuts on the fire. Then all the Cratchit family\ndrew round the hearth in what Bob Cratchit called a circle, meaning half\na one; and at Bob Cratchit's elbow stood the family display of glass.\nTwo tumblers and a custard cup without a handle.\n\nThese held the hot stuff from the jug, however, as well as golden\ngoblets would have done; and Bob served it out with beaming looks, while\nthe chestnuts on the fire sputtered and cracked noisily. Then Bob\nproposed:\n\n'A merry Christmas to us all, my dears. God bless us!'\n\nWhich all the family re-echoed.\n\n'God bless us every one!' said Tiny Tim, the last of all.\n\nHe sat very close to his father's side, upon his little stool. Bob held\nhis withered little hand to his, as if he loved the child, and wished to\nkeep him by his side, and dreaded that he might be taken from him.\n\n'Spirit,' said Scrooge, with an interest he had never felt before, 'tell\nme if Tiny Tim will live.'\n\n'I see a vacant seat,' replied the Ghost, 'in the poor chimney corner,\nand a crutch without an owner, carefully preserved. If these shadows\nremain unaltered by the Future, the child will die.'\n\n'No, no,' said Scrooge. 'Oh no, kind Spirit! say he will be spared.'\n\n'If these shadows remain unaltered by the Future none other of my race,'\nreturned the Ghost, '"}
16:34:32,440 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,440 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,487 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,493 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,493 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27018, Requested 6843. Please try again in 7.722s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,494 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "have no doubt his liberality is well represented by his surviving\npartner,' said the gentleman, presenting his credentials.\n\n[Illustration: THEY WERE PORTLY GENTLEMEN, PLEASANT TO BEHOLD]\n\nIt certainly was; for they had been two kindred spirits. At the ominous\nword 'liberality' Scrooge frowned, and shook his head, and handed the\ncredentials back.\n\n'At this festive season of the year, Mr. Scrooge,' said the gentleman,\ntaking up a pen, 'it is more than usually desirable that we should make\nsome slight provision for the poor and destitute, who suffer greatly at\nthe present time. Many thousands are in want of common necessaries;\nhundreds of thousands are in want of common comforts, sir.'\n\n'Are there no prisons?' asked Scrooge.\n\n'Plenty of prisons,' said the gentleman, laying down the pen again.\n\n'And the Union workhouses?' demanded Scrooge. 'Are they still in\noperation?'\n\n'They are. Still,' returned the gentleman, 'I wish I could say they were\nnot.'\n\n'The Treadmill and the Poor Law are in full vigour, then?' said Scrooge.\n\n'Both very busy, sir.'\n\n'Oh! I was afraid, from what you said at first, that something had\noccurred to stop them in their useful course,' said Scrooge. 'I am very\nglad to hear it.'\n\n'Under the impression that they scarcely furnish Christian cheer of mind\nor body to the multitude,' returned the gentleman, 'a few of us are\nendeavouring to raise a fund to buy the Poor some meat and drink, and\nmeans of warmth. We choose this time, because it is a time, of all\nothers, when Want is keenly felt, and Abundance rejoices. What shall I\nput you down for?'\n\n'Nothing!' Scrooge replied.\n\n'You wish to be anonymous?'\n\n'I wish to be left alone,' said Scrooge. 'Since you ask me what I wish,\ngentlemen, that is my answer. I don't make merry myself at Christmas,\nand I can't afford to make idle people merry. I help to support the\nestablishments I have mentioned--they cost enough: and those who are\nbadly off must go there.'\n\n'Many can't go there; and many would rather die.'\n\n'If they would rather die,' said Scrooge, 'they had better do it, and\ndecrease the surplus population. Besides--excuse me--I don't know that.'\n\n'But you might know it,' observed the gentleman.\n\n'It's not my business,' Scrooge returned. 'It's enough for a man to\nunderstand his own business, and not to interfere with other people's.\nMine occupies me constantly. Good afternoon, gentlemen!'\n\nSeeing clearly that it would be useless to pursue their point, the\ngentlemen withdrew. Scrooge resumed his labours with an improved opinion\nof himself, and in a more facetious temper than was usual with him.\n\nMeanwhile the fog and darkness thickened so, that people ran about with\nflaring links, proffering their services to go before horses in\ncarriages, and conduct them on their way. The ancient tower of a church,\nwhose gruff old bell was always peeping slyly down at Scrooge out of a\nGothic window in the wall, became invisible, and struck the hours and\nquarters in the clouds, with tremulous vibrations afterwards, as if its\nteeth were chattering in its frozen head up there. The cold became\nintense. In the main street, at the corner of the court, some labourers\nwere repairing the gas-pipes, and had lighted a great fire in a brazier,\nround which a party of ragged men and boys were gathered: warming their\nhands and winking their eyes before the blaze in rapture. The water-plug\nbeing left in solitude, its overflowings suddenly congealed, and turned\nto misanthropic ice. The brightness of the shops, where holly sprigs and\nberries crackled in the lamp heat of the windows, made pale faces ruddy\nas they passed. Poulterers' and grocers' trades became a splendid joke:\na glorious pageant, with which it was next to impossible to believe that\nsuch dull principles as bargain and sale had anything to do. The Lord\nMayor, in the stronghold of the mighty Mansion House, gave orders to his\nfifty cooks and butlers to keep Christmas as a Lord Mayor's household\nshould; and even the little tailor, whom he had fined five shillings on\nthe previous Monday for being drunk and bloodthirsty in the streets,\nstirred up to-morrow's pudding in his garret, while his lean wife and\nthe baby sallied out to buy the beef.\n\nFoggier yet, and colder! Piercing, searching, biting cold. If the good\nSt. Dunstan had but nipped the Evil Spirit's nose with a touch of such\nweather as that, instead of using his familiar weapons, then indeed he\nwould have roared to lusty purpose. The owner of one scant young nose,\ngnawed and mumbled by the hungry cold as bones are gnawed by dogs,\nstooped down at Scrooge's keyhole to regale him with a Christmas carol;\nbut, at the first sound of\n\n  'God bless you, merry gentleman,\n  May nothing you dismay!'\n\nScrooge seized the ruler with such energy of action that the singer fled\nin terror, leaving the keyhole to the fog, and even more congenial\nfrost."}
16:34:32,505 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,505 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,505 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27014, Requested 6837. Please try again in 7.701s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,506 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "in it, and the ancient sheath was\neaten up with rust.\n\n'You have never seen the like of me before!' exclaimed the Spirit.\n\n'Never,' Scrooge made answer to it.\n\n'Have never walked forth with the younger members of my family; meaning\n(for I am very young) my elder brothers born in these later years?'\npursued the Phantom.\n\n'I don't think I have,' said Scrooge. 'I am afraid I have not. Have you\nhad many brothers, Spirit?'\n\n'More than eighteen hundred,' said the Ghost.\n\n'A tremendous family to provide for,' muttered Scrooge.\n\nThe Ghost of Christmas Present rose.\n\n'Spirit,' said Scrooge submissively, 'conduct me where you will. I went\nforth last night on compulsion, and I learned a lesson which is working\nnow. To-night if you have aught to teach me, let me profit by it.'\n\n'Touch my robe!'\n\nScrooge did as he was told, and held it fast.\n\nHolly, mistletoe, red berries, ivy, turkeys, geese, game, poultry,\nbrawn, meat, pigs, sausages, oysters, pies, puddings, fruit, and punch,\nall vanished instantly. So did the room, the fire, the ruddy glow, the\nhour of night, and they stood in the city streets on Christmas morning,\nwhere (for the weather was severe) the people made a rough, but brisk\nand not unpleasant kind of music, in scraping the snow from the pavement\nin front of their dwellings, and from the tops of their houses, whence\nit was mad delight to the boys to see it come plumping down into the\nroad below, and splitting into artificial little snowstorms.\n\nThe house-fronts looked black enough, and the windows blacker,\ncontrasting with the smooth white sheet of snow upon the roofs, and with\nthe dirtier snow upon the ground; which last deposit had been ploughed\nup in deep furrows by the heavy wheels of carts and waggons: furrows\nthat crossed and recrossed each other hundreds of times where the great\nstreets branched off; and made intricate channels, hard to trace in the\nthick yellow mud and icy water. The sky was gloomy, and the shortest\nstreets were choked up with a dingy mist, half thawed, half frozen,\nwhose heavier particles descended in a shower of sooty atoms, as if all\nthe chimneys in Great Britain had, by one consent, caught fire, and were\nblazing away to their dear heart's content. There was nothing very\ncheerful in the climate or the town, and yet was there an air of\ncheerfulness abroad that the clearest summer air and brightest summer\nsun might have endeavoured to diffuse in vain.\n\n[Illustration: THERE WAS NOTHING VERY CHEERFUL IN THE CLIMATE]\n\nFor the people who were shovelling away on the house-tops were jovial\nand full of glee; calling out to one another from the parapets, and now\nand then exchanging a facetious snowball--better-natured missile far\nthan many a wordy jest--laughing heartily if it went right, and not less\nheartily if it went wrong. The poulterers' shops were still half open,\nand the fruiterers' were radiant in their glory. There were great,\nround, pot-bellied baskets of chestnuts, shaped like the waistcoats of\njolly old gentlemen, lolling at the doors, and tumbling out into the\nstreet in their apoplectic opulence: There were ruddy, brown-faced,\nbroad-girthed Spanish onions, shining in the fatness of their growth\nlike Spanish friars, and winking from their shelves in wanton slyness at\nthe girls as they went by, and glanced demurely at the hung-up\nmistletoe. There were pears and apples clustered high in blooming\npyramids; there were bunches of grapes, made, in the shopkeepers'\nbenevolence, to dangle from conspicuous hooks that people's mouths might\nwater gratis as they passed; there were piles of filberts, mossy and\nbrown, recalling, in their fragrance, ancient walks among the woods, and\npleasant shufflings ankle deep through withered leaves; there were\nNorfolk Biffins, squab and swarthy, setting off the yellow of the\noranges and lemons, and, in the great compactness of their juicy\npersons, urgently entreating and beseeching to be carried home in paper\nbags and eaten after dinner. The very gold and silver fish, set forth\namong these choice fruits in a bowl, though members of a dull and\nstagnant-blooded race, appeared to know that there was something going\non; and, to a fish, went gasping round and round their little world in\nslow and passionless excitement.\n\nThe Grocers'! oh, the Grocers'! nearly closed, with perhaps two shutters\ndown, or one; but through those gaps such glimpses! It was not alone\nthat the scales descending on the counter made a merry sound, or that\nthe twine and roller parted company so briskly, or that the canisters\nwere rattled up and down like juggling tricks, or even that the blended\nscents of tea and coffee were so grateful to the nose, or even that the\nraisins were so plentiful and rare, the almonds so extremely white, the\nsticks of cinnamon so long and straight, the other spices so delicious,\nthe candied fruits so"}
16:34:32,512 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,513 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,513 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27013, Requested 6816. Please try again in 7.657s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,514 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "and on the threshold of the door the Spirit smiled, and stopped to\nbless Bob Cratchit's dwelling with the sprinklings of his torch. Think\nof that! Bob had but fifteen 'Bob' a week himself; he pocketed on\nSaturdays but fifteen copies of his Christian name; and yet the Ghost of\nChristmas Present blessed his four-roomed house!\n\nThen up rose Mrs. Cratchit, Cratchit's wife, dressed out but poorly in a\ntwice-turned gown, but brave in ribbons, which are cheap, and make a\ngoodly show for sixpence; and she laid the cloth, assisted by Belinda\nCratchit, second of her daughters, also brave in ribbons; while Master\nPeter Cratchit plunged a fork into the saucepan of potatoes, and getting\nthe corners of his monstrous shirt-collar (Bob's private property,\nconferred upon his son and heir in honour of the day,) into his mouth,\nrejoiced to find himself so gallantly attired, and yearned to show his\nlinen in the fashionable Parks. And now two smaller Cratchits, boy and\ngirl, came tearing in, screaming that outside the baker's they had smelt\nthe goose, and known it for their own; and basking in luxurious thoughts\nof sage and onion, these young Cratchits danced about the table, and\nexalted Master Peter Cratchit to the skies, while he (not proud,\nalthough his collars nearly choked him) blew the fire, until the slow\npotatoes, bubbling up, knocked loudly at the saucepan-lid to be let out\nand peeled.\n\n'What has ever got your precious father, then?' said Mrs. Cratchit. 'And\nyour brother, Tiny Tim? And Martha warn't as late last Christmas Day by\nhalf an hour!'\n\n'Here's Martha, mother!' said a girl, appearing as she spoke.\n\n'Here's Martha, mother!' cried the two young Cratchits. 'Hurrah! There's\n_such_ a goose, Martha!'\n\n'Why, bless your heart alive, my dear, how late you are!' said Mrs.\nCratchit, kissing her a dozen times, and taking off her shawl and bonnet\nfor her with officious zeal.\n\n'We'd a deal of work to finish up last night,' replied the girl, 'and\nhad to clear away this morning, mother!'\n\n'Well! never mind so long as you are come,' said Mrs. Cratchit. 'Sit ye\ndown before the fire, my dear, and have a warm, Lord bless ye!'\n\n'No, no! There's father coming,' cried the two young Cratchits, who were\neverywhere at once. 'Hide, Martha, hide!'\n\nSo Martha hid herself, and in came little Bob, the father, with at least\nthree feet of comforter, exclusive of the fringe, hanging down before\nhim, and his threadbare clothes darned up and brushed to look\nseasonable, and Tiny Tim upon his shoulder. Alas for Tiny Tim, he bore a\nlittle crutch, and had his limbs supported by an iron frame!\n\n'Why, where's our Martha?' cried Bob Cratchit, looking round.\n\n'Not coming,' said Mrs. Cratchit.\n\n'Not coming!' said Bob, with a sudden declension in his high spirits;\nfor he had been Tim's blood-horse all the way from church, and had come\nhome rampant. 'Not coming upon Christmas Day!'\n\nMartha didn't like to see him disappointed, if it were only in joke; so\nshe came out prematurely from behind the closet door, and ran into his\narms, while the two young Cratchits hustled Tiny Tim, and bore him off\ninto the wash-house, that he might hear the pudding singing in the\ncopper.\n\n'And how did little Tim behave?' asked Mrs. Cratchit when she had\nrallied Bob on his credulity, and Bob had hugged his daughter to his\nheart's content.\n\n'As good as gold,' said Bob, 'and better. Somehow, he gets thoughtful,\nsitting by himself so much, and thinks the strangest things you ever\nheard. He told me, coming home, that he hoped the people saw him in the\nchurch, because he was a cripple, and it might be pleasant to them to\nremember upon Christmas Day who made lame beggars walk and blind men\nsee.'\n\nBob's voice was tremulous when he told them this, and trembled more when\nhe said that Tiny Tim was growing strong and hearty.\n\nHis active little crutch was heard upon the floor, and back came Tiny\nTim before another word was spoken, escorted by his brother and\nsister to his stool beside the fire; and while Bob, turning up his\ncuffs--as if, poor fellow, they were capable of being made more\nshabby--compounded some hot mixture in a jug with gin and lemons, and\nstirred it round and round, and put it on the hob to simmer, Master\nPeter and the two ubiquitous young Cratchits went to fetch the goose,\nwith which they soon returned in high procession.\n\n[Illustration]\n\nSuch a bustle ensued that you might have thought a goose the rarest of\nall birds; a feathered phenomenon, to which a black swan was a matter of\ncourse--and, in truth, it was something very like it in that house. Mrs.\nCratchit made the gravy (ready beforehand in a little saucepan) hissing\nhot; Master Peter mashed the potatoes with incredible vigour; Miss\nBelinda sweetened up the apple sauce; Martha dust"}
16:34:32,519 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,519 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,636 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,638 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,717 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,721 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,721 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,722 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,810 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,812 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,812 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26856, Requested 6842. Please try again in 7.396s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:32,814 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "liked, I own, to have touched her lips; to have\nquestioned her, that she might have opened them; to have looked upon the\nlashes of her downcast eyes, and never raised a blush; to have let loose\nwaves of hair, an inch of which would be a keepsake beyond price: in\nshort, I should have liked, I do confess, to have had the lightest\nlicense of a child, and yet to have been man enough to know its value.\n\n[Illustration: _A flushed and boisterous group_]\n\nBut now a knocking at the door was heard, and such a rush immediately\nensued that she, with laughing face and plundered dress, was borne\ntowards it the centre of a flushed and boisterous group, just in time to\ngreet the father, who came home attended by a man laden with Christmas\ntoys and presents. Then the shouting and the struggling, and the\nonslaught that was made on the defenceless porter! The scaling him, with\nchairs for ladders, to dive into his pockets, despoil him of\nbrown-paper parcels, hold on tight by his cravat, hug him round his\nneck, pummel his back, and kick his legs in irrepressible affection! The\nshouts of wonder and delight with which the development of every package\nwas received! The terrible announcement that the baby had been taken in\nthe act of putting a doll's frying pan into his mouth, and was more than\nsuspected of having swallowed a fictitious turkey, glued on a wooden\nplatter! The immense relief of finding this a false alarm! The joy, and\ngratitude, and ecstasy! They are all indescribable alike. It is enough\nthat, by degrees, the children and their emotions got out of the\nparlour, and, by one stair at a time, up to the top of the house, where\nthey went to bed, and so subsided.\n\nAnd now Scrooge looked on more attentively than ever, when the master of\nthe house, having his daughter leaning fondly on him, sat down with her\nand her mother at his own fireside; and when he thought that such\nanother creature, quite as graceful and as full of promise, might have\ncalled him father, and been a spring-time in the haggard winter of his\nlife, his sight grew very dim indeed.\n\n'Belle,' said the husband, turning to his wife with a smile, 'I saw an\nold friend of yours this afternoon.'\n\n'Who was it?'\n\n'Guess!'\n\n'How can I? Tut, don't I know?' she added in the same breath, laughing\nas he laughed. 'Mr. Scrooge.'\n\n'Mr. Scrooge it was. I passed his office window; and as it was not shut\nup, and he had a candle inside, I could scarcely help seeing him. His\npartner lies upon the point of death, I hear; and there he sat alone.\nQuite alone in the world, I do believe.'\n\n'Spirit!' said Scrooge in a broken voice, 'remove me from this place.'\n\n'I told you these were shadows of the things that have been,' said the\nGhost. 'That they are what they are do not blame me!'\n\n'Remove me!' Scrooge exclaimed, 'I cannot bear it!'\n\nHe turned upon the Ghost, and seeing that it looked upon him with a\nface, in which in some strange way there were fragments of all the faces\nit had shown him, wrestled with it.\n\n'Leave me! Take me back. Haunt me no longer!'\n\nIn the struggle, if that can be called a struggle in which the Ghost\nwith no visible resistance on its own part was undisturbed by any effort\nof its adversary, Scrooge observed that its light was burning high and\nbright; and dimly connecting that with its influence over him, he seized\nthe extinguisher-cap, and by a sudden action pressed it down upon its\nhead.\n\n[Illustration: _Laden with Christmas toys and presents_]\n\nThe Spirit dropped beneath it, so that the extinguisher covered its\nwhole form; but though Scrooge pressed it down with all his force, he\ncould not hide the light, which streamed from under it, in an unbroken\nflood upon the ground.\n\nHe was conscious of being exhausted, and overcome by an irresistible\ndrowsiness; and, further, of being in his own bedroom. He gave the cap a\nparting squeeze, in which his hand relaxed; and had barely time to reel\nto bed, before he sank into a heavy sleep.\n\n[Illustration]\n\n\nSTAVE THREE\n\n\n[Illustration]\n\n\n\n\nTHE SECOND OF THE THREE SPIRITS\n\n\nAwaking in the middle of a prodigiously tough snore, and sitting up in\nbed to get his thoughts together, Scrooge had no occasion to be told\nthat the bell was again upon the stroke of One. He felt that he was\nrestored to consciousness in the right nick of time, for the especial\npurpose of holding a conference with the second messenger despatched to\nhim through Jacob Marley's intervention. But finding that he turned\nuncomfortably cold when he began to wonder which of his curtains this\nnew spectre would draw back, he put them every one aside with his own\nhands, and, lying down again, established a sharp look-out all round the\nbed. For he wished to challenge the Spirit on the moment of its\nappearance, and did not wish to be taken by surprise and made nervous.\n\nGentlemen of the free-and-easy sort, who plume themselves on being"}
16:34:32,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,837 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,838 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,839 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,854 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:32,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:32,942 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,137 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,313 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,316 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,502 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,824 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:33,826 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:33,879 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:33,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.501272916000744. input_tokens=2936, output_tokens=408
16:34:34,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,100 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,177 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,178 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,252 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,254 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,298 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,371 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,372 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,398 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,401 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,478 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,481 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,682 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,684 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:34,828 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:34,830 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,86 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,86 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,155 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,156 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,549 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,550 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,575 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,575 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,787 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:35,881 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:35,883 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,215 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,217 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,320 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,322 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,428 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,431 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,531 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,532 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,762 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:36,836 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:36,837 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,20 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,119 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,124 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,286 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,289 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,384 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,485 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:37,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:37,938 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,268 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,361 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:38,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 2 retries took 24.612589666998247. input_tokens=2937, output_tokens=494
16:34:38,367 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,368 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,414 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,415 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,544 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:38,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:38,697 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:39,501 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:39,504 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:39,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:39,943 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,124 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,125 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,560 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,561 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,897 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:40,898 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:40,899 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,231 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,233 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,802 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,806 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,808 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,853 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,854 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:41,941 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:41,943 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,267 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,270 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,448 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,494 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,648 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,650 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,789 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,791 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:42,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:42,975 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,437 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,445 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,446 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,449 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:43,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 8 retries took 16.82636666700273. input_tokens=2935, output_tokens=570
16:34:43,609 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,610 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:43,800 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:43,802 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:44,656 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:44,658 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:45,515 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:45,518 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:47,279 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:47,281 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:48,241 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:48,243 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:48,849 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:48,851 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:49,533 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:49,533 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,32 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,34 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,426 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,427 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,428 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 28787, Requested 6831. Please try again in 11.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:50,429 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "intently at the Spirit's robe, 'but I see something strange, and not\nbelonging to yourself, protruding from your skirts. Is it a foot or a\nclaw?'\n\n'It might be a claw, for the flesh there is upon it,' was the Spirit's\nsorrowful reply. 'Look here!'\n\nFrom the foldings of its robe it brought two children, wretched, abject,\nfrightful, hideous, miserable. They knelt down at its feet, and clung\nupon the outside of its garment.\n\n'O Man! look here! Look, look down here!' exclaimed the Ghost.\n\nThey were a boy and girl. Yellow, meagre, ragged, scowling, wolfish, but\nprostrate, too, in their humility. Where graceful youth should have\nfilled their features out, and touched them with its freshest tints, a\nstale and shrivelled hand, like that of age, had pinched and twisted\nthem, and pulled them into shreds. Where angels might have sat\nenthroned, devils lurked, and glared out menacing. No change, no\ndegradation, no perversion of humanity in any grade, through all the\nmysteries of wonderful creation, has monsters half so horrible and\ndread.\n\nScrooge started back, appalled. Having them shown to him in this way, he\ntried to say they were fine children, but the words choked themselves,\nrather than be parties to a lie of such enormous magnitude.\n\n'Spirit! are they yours?' Scrooge could say no more.\n\n'They are Man's,' said the Spirit, looking down upon them. 'And they\ncling to me, appealing from their fathers. This boy is Ignorance. This\ngirl is Want. Beware of them both, and all of their degree, but most of\nall beware this boy, for on his brow I see that written which is Doom,\nunless the writing be erased. Deny it!' cried the Spirit, stretching out\nhis hand towards the city. 'Slander those who tell it ye! Admit it for\nyour factious purposes, and make it worse! And bide the end!'\n\n'Have they no refuge or resource?' cried Scrooge.\n\n'Are there no prisons?' said the Spirit, turning on him for the last\ntime with his own words. 'Are there no workhouses?'\n\nThe bell struck Twelve.\n\nScrooge looked about him for the Ghost, and saw it not. As the last\nstroke ceased to vibrate, he remembered the prediction of old Jacob\nMarley, and, lifting up his eyes, beheld a solemn Phantom, draped and\nhooded, coming like a mist along the ground towards him.\n\n\nSTAVE FOUR\n\n\n\n\nTHE LAST OF THE SPIRITS\n\n\nThe Phantom slowly, gravely, silently approached. When it came near him,\nScrooge bent down upon his knee; for in the very air through which this\nSpirit moved it seemed to scatter gloom and mystery.\n\nIt was shrouded in a deep black garment, which concealed its head, its\nface, its form, and left nothing of it visible, save one outstretched\nhand. But for this, it would have been difficult to detach its figure\nfrom the night, and separate it from the darkness by which it was\nsurrounded.\n\nHe felt that it was tall and stately when it came beside him, and that\nits mysterious presence filled him with a solemn dread. He knew no more,\nfor the Spirit neither spoke nor moved.\n\n'I am in the presence of the Ghost of Christmas Yet to Come?' said\nScrooge.\n\nThe Spirit answered not, but pointed onward with its hand.\n\n'You are about to show me shadows of the things that have not happened,\nbut will happen in the time before us,' Scrooge pursued. 'Is that so,\nSpirit?'\n\nThe upper portion of the garment was contracted for an instant in its\nfolds, as if the Spirit had inclined its head. That was the only answer\nhe received.\n\nAlthough well used to ghostly company by this time, Scrooge feared the\nsilent shape so much that his legs trembled beneath him, and he found\nthat he could hardly stand when he prepared to follow it. The Spirit\npaused a moment, as observing his condition, and giving him time to\nrecover.\n\nBut Scrooge was all the worse for this. It thrilled him with a vague,\nuncertain horror to know that, behind the dusky shroud, there were\nghostly eyes intently fixed upon him, while he, though he stretched his\nown to the utmost, could see nothing but a spectral hand and one great\nheap of black.\n\n'Ghost of the Future!' he exclaimed, 'I fear you more than any spectre\nI have seen. But as I know your purpose is to do me good, and as I hope\nto live to be another man from what I was, I am prepared to bear your\ncompany, and do it with a thankful heart. Will you not speak to me?'\n\nIt gave him no reply. The hand was pointed straight before them.\n\n'Lead on!' said Scrooge. 'Lead on! The night is waning fast, and it is\nprecious time to me, I know. Lead on, Spirit!'\n\nThe Phantom moved away as it had come towards him. Scrooge followed in\nthe shadow of its dress, which bore him up, he thought, and carried him\nalong.\n\nThey scarcely seemed to enter the City; for the City rather seemed to\nspring up about them, and encompass them of its own act. But there they\nwere in the heart of it; on 'Change, amongst the merchants,"}
16:34:50,550 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,551 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,785 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,787 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,862 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:50,864 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:50,949 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:50,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 3 retries took 9.853620374997263. input_tokens=34, output_tokens=314
16:34:51,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,83 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,136 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,286 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,288 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,323 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,325 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,354 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,356 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,597 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,598 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:51,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:51,994 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,164 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,166 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,189 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,191 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,698 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,699 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:52,854 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:52,855 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:53,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:53,406 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:53,407 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23806, Requested 6854. Please try again in 1.32s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:34:53,409 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'precious time to me, I know. Lead on, Spirit!\'\n\nThe Phantom moved away as it had come towards him. Scrooge followed in\nthe shadow of its dress, which bore him up, he thought, and carried him\nalong.\n\nThey scarcely seemed to enter the City; for the City rather seemed to\nspring up about them, and encompass them of its own act. But there they\nwere in the heart of it; on \'Change, amongst the merchants, who hurried\nup and down, and chinked the money in their pockets, and conversed in\ngroups, and looked at their watches, and trifled thoughtfully with their\ngreat gold seals, and so forth, as Scrooge had seen them often.\n\nThe Spirit stopped beside one little knot of business men. Observing\nthat the hand was pointed to them, Scrooge advanced to listen to their\ntalk.\n\n\'No,\' said a great fat man with a monstrous chin, \'I don\'t know much\nabout it either way. I only know he\'s dead.\'\n\n\'When did he die?\' inquired another.\n\n\'Last night, I believe.\'\n\n\'Why, what was the matter with him?\' asked a third, taking a vast\nquantity of snuff out of a very large snuff-box. \'I thought he\'d never\ndie.\'\n\n\'God knows,\' said the first, with a yawn.\n\n\'What has he done with his money?\' asked a red-faced gentleman with a\npendulous excrescence on the end of his nose, that shook like the gills\nof a turkey-cock.\n\n\'I haven\'t heard,\' said the man with the large chin, yawning again.\n\'Left it to his company, perhaps. He hasn\'t left it to _me_. That\'s all\nI know.\'\n\nThis pleasantry was received with a general laugh.\n\n\'It\'s likely to be a very cheap funeral,\' said the same speaker; \'for,\nupon my life, I don\'t know of anybody to go to it. Suppose we make up a\nparty, and volunteer?\'\n\n\'I don\'t mind going if a lunch is provided,\' observed the gentleman with\nthe excrescence on his nose. \'But I must be fed if I make one.\'\n\nAnother laugh.\n\n[Illustration:\n\n  _"How are you?" said one.\n   "How are you?" returned the other.\n   "Well!" said the first. "Old Scratch has got his own at last, hey?"_\n\n]\n\n\'Well, I am the most disinterested among you, after all,\' said the first\nspeaker, \'for I never wear black gloves, and I never eat lunch. But I\'ll\noffer to go if anybody else will. When I come to think of it, I\'m not\nat all sure that I wasn\'t his most particular friend; for we used to\nstop and speak whenever we met. Bye, bye!\'\n\nSpeakers and listeners strolled away, and mixed with other groups.\nScrooge knew the men, and looked towards the Spirit for an explanation.\n\nThe phantom glided on into a street. Its finger pointed to two persons\nmeeting. Scrooge listened again, thinking that the explanation might lie\nhere.\n\nHe knew these men, also, perfectly. They were men of business: very\nwealthy, and of great importance. He had made a point always of standing\nwell in their esteem in a business point of view, that is; strictly in a\nbusiness point of view.\n\n\'How are you?\' said one.\n\n\'How are you?\' returned the other.\n\n\'Well!\' said the first, \'old Scratch has got his own at last, hey?\'\n\n\'So I am told,\' returned the second. \'Cold, isn\'t it?\'\n\n\'Seasonable for Christmas-time. You are not a skater, I suppose?\'\n\n\'No, no. Something else to think of. Good-morning!\'\n\nNot another word. That was their meeting, their conversation, and their\nparting.\n\nScrooge was at first inclined to be surprised that the Spirit should\nattach importance to conversations apparently so trivial; but feeling\nassured that they must have some hidden purpose, he set himself to\nconsider what it was likely to be. They could scarcely be supposed to\nhave any bearing on the death of Jacob, his old partner, for that was\nPast, and this Ghost\'s province was the Future. Nor could he think of\nany one immediately connected with himself to whom he could apply them.\nBut nothing doubting that, to whomsoever they applied, they had some\nlatent moral for his own improvement, he resolved to treasure up every\nword he heard, and everything he saw; and especially to observe the\nshadow of himself when it appeared. For he had an expectation that the\nconduct of his future self would give him the clue he missed, and would\nrender the solution of these riddles easy.\n\nHe looked about in that very place for his own image, but another man\nstood in his accustomed corner; and though the clock pointed to his\nusual time of day for being there, he saw no likeness of himself among\nthe multitudes that poured in through the Porch. It gave him little\nsurprise, however; for he had been revolving in his mind a change of\nlife, and thought and hoped he saw his new-born resolutions carried out\nin this.\n\nQuiet and dark, beside him stood the Phantom, with its outstretched\nhand. When he roused himself from his thoughtful quest, he fancied,\nfrom the turn of the hand, and its situation in reference to himself,\nthat the Unseen Eyes were looking at him keenly. It made him shudder,\nand feel very cold.\n\nThey left the busy scene, and went into an obscure part of the town,\nwhere Scro'}
16:34:54,731 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:34:54,735 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 21.210672416000307. input_tokens=2790, output_tokens=435
16:34:55,141 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:55,143 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:56,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:56,330 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:56,805 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:56,808 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:59,199 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:59,201 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:59,534 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:59,536 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:34:59,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:34:59,869 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:00,193 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:00,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 6 retries took 11.778437624998332. input_tokens=2936, output_tokens=388
16:35:00,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:00,564 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:00,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:00,924 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,130 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,131 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,391 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,392 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,538 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,539 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,647 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,648 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:01,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:01,940 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,50 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,80 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,256 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,258 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,339 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,394 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,395 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,563 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,565 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,581 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,583 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:02,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:02,754 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:03,75 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:03,76 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:03,984 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:03,985 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:04,599 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:04,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 5 retries took 9.94266491700182. input_tokens=2936, output_tokens=275
16:35:05,994 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:05,997 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:06,695 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:06,697 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:09,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:09,570 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:10,270 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:10,272 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:11,76 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:11,77 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:11,804 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:11,805 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:11,885 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:11,887 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,196 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,197 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,317 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,318 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,377 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,379 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,435 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,436 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,475 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,476 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,647 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,650 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,700 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,702 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,736 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,738 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:12,960 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:12,962 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,4 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,6 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,46 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,48 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:13,602 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:13,604 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:14,827 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:14,830 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 10.223011250000127. input_tokens=34, output_tokens=296
16:35:20,202 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:20,204 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:20,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:20,412 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:20,653 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:20,655 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:20,940 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:20,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 4.243167958000413. input_tokens=34, output_tokens=89
16:35:22,278 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:22,279 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:22,613 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:22,613 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:22,691 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:22,692 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,19 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,20 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,133 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,134 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,135 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,136 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,544 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,547 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,548 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,549 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,550 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:23,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:23,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 23.63106274999882. input_tokens=34, output_tokens=470
16:35:23,993 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:23,996 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:24,34 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:24,37 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:24,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:24,92 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:24,790 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:24,792 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:26,49 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:26,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 4.244035874999099. input_tokens=34, output_tokens=91
16:35:30,540 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:30,542 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 6/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:31,211 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:31,213 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:32,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:32,636 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,41 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,473 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,475 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,581 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,581 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,814 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:33,903 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:33,905 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,60 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,61 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,331 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,333 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,341 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,342 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,422 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,424 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,432 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 7/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:34,516 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:34,517 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:35,123 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:35,125 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:36,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:36,194 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 8 retries took 5.53671629100063. input_tokens=34, output_tokens=142
16:35:42,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:42,66 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:42,66 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 27658, Requested 7000. Please try again in 9.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:42,67 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '. \'A merrier Christmas,\nBob, my good fellow, than I have given you for many a year! I\'ll raise\nyour salary, and endeavour to assist your struggling family, and we will\ndiscuss your affairs this very afternoon, over a Christmas bowl of\nsmoking bishop, Bob! Make up the fires and buy another coal-scuttle\nbefore you dot another i, Bob Cratchit!\'\n\n[Illustration: _"Now, I\'ll tell you what, my friend," said Scrooge. "I\nam not going to stand this sort of thing any longer."_]\n\nScrooge was better than his word. He did it all, and infinitely more;\nand to Tiny Tim, who did NOT die, he was a second father. He became as\ngood a friend, as good a master, and as good a man as the good old\nCity knew, or any other good old city, town, or borough in the good old\nworld. Some people laughed to see the alteration in him, but he let them\nlaugh, and little heeded them; for he was wise enough to know that\nnothing ever happened on this globe, for good, at which some people did\nnot have their fill of laughter in the outset; and knowing that such as\nthese would be blind anyway, he thought it quite as well that they\nshould wrinkle up their eyes in grins as have the malady in less\nattractive forms. His own heart laughed, and that was quite enough for\nhim.\n\nHe had no further intercourse with Spirits, but lived upon the\nTotal-Abstinence Principle ever afterwards; and it was always said of\nhim that he knew how to keep Christmas well, if any man alive possessed\nthe knowledge. May that be truly said of us, and all of us! And so, as\nTiny Tim observed, God bless Us, Every One!\n\n[Illustration]\n\n+---------------------------------------------------------------+\n|Transcriber\'s note: The Contents were added by the transcriber.|\n+---------------------------------------------------------------+\n\n\n\n\n\n\n\n*** END OF THE PROJECT GUTENBERG EBOOK A CHRISTMAS CAROL ***\n\n\n    \n\nUpdated editions will replace the previous one—the old editions will\nbe renamed.\n\nCreating the works from print editions not protected by U.S. copyright\nlaw means that no one owns a United States copyright in these works,\nso the Foundation (and you!) can copy and distribute it in the United\nStates without permission and without paying copyright\nroyalties. Special rules, set forth in the General Terms of Use part\nof this license, apply to copying and distributing Project\nGutenberg™ electronic works to protect the PROJECT GUTENBERG™\nconcept and trademark. Project Gutenberg is a registered trademark,\nand may not be used if you charge for an eBook, except by following\nthe terms of the trademark license, including paying royalties for use\nof the Project Gutenberg trademark. If you do not charge anything for\ncopies of this eBook, complying with the trademark license is very\neasy. You may use this eBook for nearly any purpose such as creation\nof derivative works, reports, performances and research. Project\nGutenberg eBooks may be modified and printed and given away—you may\ndo practically ANYTHING in the United States with eBooks not protected\nby U.S. copyright law. Redistribution is subject to the trademark\nlicense, especially commercial redistribution.\n\n\nSTART: FULL LICENSE\n\nTHE FULL PROJECT GUTENBERG LICENSE\n\nPLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n\nTo protect the Project Gutenberg™ mission of promoting the free\ndistribution of electronic works, by using or distributing this work\n(or any other work associated in any way with the phrase “Project\nGutenberg”), you agree to comply with all the terms of the Full\nProject Gutenberg™ License available with this file or online at\nwww.gutenberg.org/license.\n\nSection 1. General Terms of Use and Redistributing Project Gutenberg™\nelectronic works\n\n1.A. By reading or using any part of this Project Gutenberg™\nelectronic work, you indicate that you have read, understand, agree to\nand accept all the terms of this license and intellectual property\n(trademark/copyright) agreement. If you do not agree to abide by all\nthe terms of this agreement, you must cease using and return or\ndestroy all copies of Project Gutenberg™ electronic works in your\npossession. If you paid a fee for obtaining a copy of or access to a\nProject Gutenberg™ electronic work and you do not agree to be bound\nby the terms of this agreement, you may obtain a refund from the person\nor entity to whom you paid the fee as set forth in paragraph 1.E.8.\n\n1.B. “Project Gutenberg” is a registered trademark. It may only be\nused on or associated in any way with an electronic work by people who\nagree to be bound by the terms of this agreement. There are a few\nthings that you can do with most Project Gutenberg™ electronic works\neven without complying with the full terms of this agreement. See\nparagraph 1.C below. There are a lot of things you can do with Project\nGutenberg™ electronic works if you follow the terms of this\nagreement and help preserve free future access to Project Gutenberg™\nelectronic works. See paragraph 1.E below.\n\n1.C. The Project Gutenberg Literary Archive Foundation (“the\nFoundation” or PGLAF), owns a compilation copyright in the collection\nof Project Gutenberg™ electronic works. Nearly all the individual\nworks in the collection are in the public domain in the United\nStates. If an individual work is unprotected by copyright law in the\nUnited States and you are located in the United States, we do not\nclaim a right to prevent you from copying, distributing, performing,\ndisplaying or creating derivative works based'}
16:35:42,337 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:42,340 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 7 retries took 19.599378208000417. input_tokens=34, output_tokens=422
16:35:42,989 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:42,991 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:42,992 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23719, Requested 7188. Please try again in 1.814s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:42,994 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "you or I could have told\nit him!) struck up 'Sir Roger de Coverley.' Then old Fezziwig stood\nout to dance with Mrs. Fezziwig. Top couple, too; with a good stiff\npiece of work cut out for them; three or four and twenty pair of\npartners; people who were not to be trifled with; people who would\ndance, and had no notion of walking.\n\nBut if they had been twice as many--ah! four times--old Fezziwig would\nhave been a match for them, and so would Mrs. Fezziwig. As to _her_, she\nwas worthy to be his partner in every sense of the term. If that's not\nhigh praise, tell me higher, and I'll use it. A positive light appeared\nto issue from Fezziwig's calves. They shone in every part of the dance\nlike moons. You couldn't have predicted, at any given time, what would\nbecome of them next. And when old Fezziwig and Mrs. Fezziwig had gone\nall through the dance; advance and retire, both hands to your partner,\nbow and curtsy, cork-screw, thread-the-needle, and back again to your\nplace: Fezziwig 'cut'--cut so deftly, that he appeared to wink with his\nlegs, and came upon his feet again without a stagger.\n\nWhen the clock struck eleven, this domestic ball broke up. Mr. and Mrs.\nFezziwig took their stations, one on either side the door, and, shaking\nhands with every person individually as he or she went out, wished him\nor her a Merry Christmas. When everybody had retired but the two\n'prentices, they did the same to them; and thus the cheerful voices died\naway, and the lads were left to their beds; which were under a counter\nin the back-shop.\n\nDuring the whole of this time Scrooge had acted like a man out of his\nwits. His heart and soul were in the scene, and with his former self. He\ncorroborated everything, remembered everything, enjoyed everything, and\nunderwent the strangest agitation. It was not until now, when the bright\nfaces of his former self and Dick were turned from them, that he\nremembered the Ghost, and became conscious that it was looking full upon\nhim, while the light upon its head burnt very clear.\n\n'A small matter,' said the Ghost, 'to make these silly folks so full of\ngratitude.'\n\n'Small!' echoed Scrooge.\n\nThe Spirit signed to him to listen to the two apprentices, who were\npouring out their hearts in praise of Fezziwig; and when he had done so,\nsaid:\n\n'Why! Is it not? He has spent but a few pounds of your mortal money:\nthree or four, perhaps. Is that so much that he deserves this praise?'\n\n'It isn't that,' said Scrooge, heated by the remark, and speaking\nunconsciously like his former, not his latter self. 'It isn't that,\nSpirit. He has the power to render us happy or unhappy; to make our\nservice light or burdensome; a pleasure or a toil. Say that his power\nlies in words and looks; in things so slight and insignificant that it\nis impossible to add and count 'em up: what then? The happiness he gives\nis quite as great as if it cost a fortune.'\n\nHe felt the Spirit's glance, and stopped.\n\n'What is the matter?' asked the Ghost.\n\n'Nothing particular,' said Scrooge.\n\n'Something, I think?' the Ghost insisted.\n\n'No,' said Scrooge, 'no. I should like to be able to say a word or two\nto my clerk just now. That's all.'\n\nHis former self turned down the lamps as he gave utterance to the wish;\nand Scrooge and the Ghost again stood side by side in the open air.\n\n'My time grows short,' observed the Spirit. 'Quick!'\n\nThis was not addressed to Scrooge, or to any one whom he could see, but\nit produced an immediate effect. For again Scrooge saw himself. He was\nolder now; a man in the prime of life. His face had not the harsh and\nrigid lines of later years; but it had begun to wear the signs of care\nand avarice. There was an eager, greedy, restless motion in the eye,\nwhich showed the passion that had taken root, and where the shadow of\nthe growing tree would fall.\n\nHe was not alone, but sat by the side of a fair young girl in a mourning\ndress: in whose eyes there were tears, which sparkled in the light that\nshone out of the Ghost of Christmas Past.\n\n'It matters little,' she said softly. 'To you, very little. Another idol\nhas displaced me; and, if it can cheer and comfort you in time to come\nas I would have tried to do, I have no just cause to grieve.'\n\n'What Idol has displaced you?' he rejoined.\n\n'A golden one.'\n\n'This is the even-handed dealing of the world!' he said. 'There is\nnothing on which it is so hard as poverty; and there is nothing it\nprofesses to condemn with such severity as the pursuit of wealth!'\n\n'You fear the world too much,' she answered gently. 'All your other\nhopes have merged into the hope of being beyond the chance of its sordid\nreproach. I have seen your nobler aspirations fall off one by one, until\nthe master passion, Gain, engrosses you. Have I not?'\n\n'"}
16:35:43,486 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:43,487 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:43,487 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23503, Requested 7021. Please try again in 1.048s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:43,487 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg™ electronic works\nprovided that:\n\n    • You pay a royalty fee of 20% of the gross profits you derive from\n        the use of Project Gutenberg™ works calculated using the method\n        you already use to calculate your applicable taxes. The fee is owed\n        to the owner of the Project Gutenberg™ trademark, but he has\n        agreed to donate royalties under this paragraph to the Project\n        Gutenberg Literary Archive Foundation. Royalty payments must be paid\n        within 60 days following each date on which you prepare (or are\n        legally required to prepare) your periodic tax returns. Royalty\n        payments should be clearly marked as such and sent to the Project\n        Gutenberg Literary Archive Foundation at the address specified in\n        Section 4, “Information about donations to the Project Gutenberg\n        Literary Archive Foundation.”\n    \n    • You provide a full refund of any money paid by a user who notifies\n        you in writing (or by e-mail) within 30 days of receipt that s/he\n        does not agree to the terms of the full Project Gutenberg™\n        License. You must require such a user to return or destroy all\n        copies of the works possessed in a physical medium and discontinue\n        all use of and all access to other copies of Project Gutenberg™\n        works.\n    \n    • You provide, in accordance with paragraph 1.F.3, a full refund of\n        any money paid for a work or a replacement copy, if a defect in the\n        electronic work is discovered and reported to you within 90 days of\n        receipt of the work.\n    \n    • You comply with all other terms of this agreement for free\n        distribution of Project Gutenberg™ works.\n    \n\n1.E.9. If you wish to charge a fee or distribute a Project\nGutenberg™ electronic work or group of works on different terms than\nare set forth in this agreement, you must obtain permission in writing\nfrom the Project Gutenberg Literary Archive Foundation, the manager of\nthe Project Gutenberg™ trademark. Contact the Foundation as set\nforth in Section 3 below.\n\n1.F.\n\n1.F.1. Project Gutenberg volunteers and employees expend considerable\neffort to identify, do copyright research on, transcribe and proofread\nworks not protected by U.S. copyright law in creating the Project\nGutenberg™ collection. Despite these efforts, Project Gutenberg™\nelectronic works, and the medium on which they may be stored, may\ncontain “Defects,” such as, but not limited to, incomplete, inaccurate\nor corrupt data, transcription errors, a copyright or other\nintellectual property infringement, a defective or damaged disk or\nother medium, a computer virus, or computer codes that damage or\ncannot be read by your equipment.\n\n1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the “Right\nof Replacement or Refund” described in paragraph 1.F.3, the Project\nGutenberg Literary Archive Foundation, the owner of the Project\nGutenberg™ trademark, and any other party distributing a Project\nGutenberg™ electronic work under this agreement, disclaim all\nliability to you for damages, costs and expenses, including legal\nfees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\nPROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\n1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\ndefect in this electronic work within 90 days of receiving it, you can\nreceive a refund of the money (if any) you paid for it by sending a\nwritten explanation to the person you received the work from. If you\nreceived the work on a physical medium, you must return the medium\nwith your written explanation. The person or entity that provided you\nwith the defective work may elect to provide a replacement copy in\nlieu of a refund. If you received the work electronically, the person\nor entity providing it to you may choose to give you a second\nopportunity to receive the work electronically in lieu of a refund. If\nthe second copy is also defective, you may demand a refund in writing\nwithout further opportunities to fix the problem.\n\n1.F.4. Except for the limited right of replacement or refund set forth\nin paragraph 1.F.3, this work is provided to you ‘AS-IS’, WITH NO\nOTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\nLIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\n\n1.F.5. Some states do not allow disclaimers of certain implied\nwarranties or the exclusion or limitation of certain types of\ndamages. If any disclaimer or limitation set forth in this agreement\nviolates the law of the state applicable to this agreement, the\nagreement shall be interpreted to make the maximum disclaimer or\nlimitation permitted by the applicable state law. The invalidity or\nunenforceability of any provision of this agreement shall not void the\nremaining provisions.\n\n1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the\ntrademark owner, any agent or employee of the Foundation, anyone\nproviding copies of Project Gutenberg™ electronic works in\naccordance with this agreement, and any volunteers associated with the\nproduction, promotion and distribution of Project Gutenberg™\nelectronic works,'}
16:35:43,811 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:43,813 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:43,813 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23310, Requested 7139. Please try again in 898ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:43,815 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'they were sharp girls too, as Topper could have told you.\nThere might have been twenty people there, young and old, but they all\nplayed, and so did Scrooge; for wholly forgetting, in the interest he\nhad in what was going on, that his voice made no sound in their ears, he\nsometimes came out with his guess quite loud, and very often guessed\nright, too; for the sharpest needle, best Whitechapel, warranted not to\ncut in the eye, was not sharper than Scrooge, blunt as he took it in\nhis head to be.\n\nThe Ghost was greatly pleased to find him in this mood, and looked upon\nhim with such favour that he begged like a boy to be allowed to stay\nuntil the guests departed. But this the Spirit said could not be done.\n\n\'Here is a new game,\' said Scrooge. \'One half-hour, Spirit, only one!\'\n\nIt was a game called Yes and No, where Scrooge\'s nephew had to think of\nsomething, and the rest must find out what, he only answering to their\nquestions yes or no, as the case was. The brisk fire of questioning to\nwhich he was exposed elicited from him that he was thinking of an\nanimal, a live animal, rather a disagreeable animal, a savage animal, an\nanimal that growled and grunted sometimes, and talked sometimes and\nlived in London, and walked about the streets, and wasn\'t made a show\nof, and wasn\'t led by anybody, and didn\'t live in a menagerie, and was\nnever killed in a market, and was not a horse, or an ass, or a cow, or a\nbull, or a tiger, or a dog, or a pig, or a cat, or a bear. At every\nfresh question that was put to him, this nephew burst into a fresh roar\nof laughter; and was so inexpressibly tickled, that he was obliged to\nget up off the sofa and stamp. At last the plump sister, falling into a\nsimilar state, cried out:\n\n\'I have found it out! I know what it is, Fred! I know what it is!\'\n\n\'What is it?\' cried Fred.\n\n\'It\'s your uncle Scro-o-o-o-oge.\'\n\nWhich it certainly was. Admiration was the universal sentiment, though\nsome objected that the reply to \'Is it a bear?\' ought to have been\n\'Yes\'; inasmuch as an answer in the negative was sufficient to have\ndiverted their thoughts from Mr. Scrooge, supposing they had ever had\nany tendency that way.\n\n\'He has given us plenty of merriment, I am sure,\' said Fred, \'and it\nwould be ungrateful not to drink his health. Here is a glass of mulled\nwine ready to our hand at the moment; and I say, "Uncle Scrooge!"\'\n\n\'Well! Uncle Scrooge!\' they cried.\n\n\'A merry Christmas and a happy New Year to the old man, whatever he is!\'\nsaid Scrooge\'s nephew. \'He wouldn\'t take it from me, but may he have it,\nnevertheless. Uncle Scrooge!\'\n\nUncle Scrooge had imperceptibly become so gay and light of heart, that\nhe would have pledged the unconscious company in return, and thanked\nthem in an inaudible speech, if the Ghost had given him time. But the\nwhole scene passed off in the breath of the last word spoken by his\nnephew; and he and the Spirit were again upon their travels.\n\nMuch they saw, and far they went, and many homes they visited, but\nalways with a happy end. The Spirit stood beside sick-beds, and they\nwere cheerful; on foreign lands, and they were close at home; by\nstruggling men, and they were patient in their greater hope; by poverty,\nand it was rich. In almshouse, hospital, and gaol, in misery\'s every\nrefuge, where vain man in his little brief authority had not made fast\nthe door, and barred the Spirit out, he left his blessing and taught\nScrooge his precepts.\n\nIt was a long night, if it were only a night; but Scrooge had his doubts\nof this, because the Christmas holidays appeared to be condensed into\nthe space of time they passed together. It was strange, too, that, while\nScrooge remained unaltered in his outward form, the Ghost grew older,\nclearly older. Scrooge had observed this change, but never spoke of it\nuntil they left a children\'s Twelfth-Night party, when, looking at the\nSpirit as they stood together in an open place, he noticed that its hair\nwas grey.\n\n\'Are spirits\' lives so short?\' asked Scrooge.\n\n\'My life upon this globe is very brief,\' replied the Ghost. \'It ends\nto-night.\'\n\n\'To-night!\' cried Scrooge.\n\n\'To-night at midnight. Hark! The time is drawing near.\'\n\nThe chimes were ringing the three-quarters past eleven at that moment.\n\n\'Forgive me if I am not justified in what I ask,\' said Scrooge, looking\nintently at the Spirit\'s robe, \'but I see something strange, and not\nbelonging to yourself, protruding from your skirts. Is it a foot or a\nclaw?\'\n\n\'It might be a claw, for the flesh there is upon it,\' was the Spirit\'s\nsorrowful reply. \'Look here!\'\n\nFrom the foldings of its robe it brought two children, wretched, abject,\nfrightful, hideous, miserable. They knelt'}
16:35:43,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:43,919 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,150 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,151 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23129, Requested 7089. Please try again in 436ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,151 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': ",\nto save himself from falling in a swoon. But how much greater was his\nhorror when the phantom, taking off the bandage round his head, as if it\nwere too warm to wear indoors, its lower jaw dropped down upon its\nbreast!\n\nScrooge fell upon his knees, and clasped his hands before his face.\n\n'Mercy!' he said. 'Dreadful apparition, why do you trouble me?'\n\n'Man of the worldly mind!' replied the Ghost, 'do you believe in me or\nnot?'\n\n'I do,' said Scrooge; 'I must. But why do spirits walk the earth, and\nwhy do they come to me?'\n\n'It is required of every man,' the Ghost returned, 'that the spirit\nwithin him should walk abroad among his fellow-men, and travel far and\nwide; and, if that spirit goes not forth in life, it is condemned to do\nso after death. It is doomed to wander through the world--oh, woe is\nme!--and witness what it cannot share, but might have shared on earth,\nand turned to happiness!'\n\nAgain the spectre raised a cry, and shook its chain and wrung its\nshadowy hands.\n\n'You are fettered,' said Scrooge, trembling. 'Tell me why?'\n\n'I wear the chain I forged in life,' replied the Ghost. 'I made it link\nby link, and yard by yard; I girded it on of my own free will, and of\nmy own free will I wore it. Is its pattern strange to _you_?'\n\nScrooge trembled more and more.\n\n'Or would you know,' pursued the Ghost, 'the weight and length of the\nstrong coil you bear yourself? It was full as heavy and as long as this\nseven Christmas Eves ago. You have laboured on it since. It is a\nponderous chain!'\n\nScrooge glanced about him on the floor, in the expectation of finding\nhimself surrounded by some fifty or sixty fathoms of iron cable; but he\ncould see nothing.\n\n'Jacob!' he said imploringly. 'Old Jacob Marley, tell me more! Speak\ncomfort to me, Jacob!'\n\n'I have none to give,' the Ghost replied. 'It comes from other regions,\nEbenezer Scrooge, and is conveyed by other ministers, to other kinds of\nmen. Nor can I tell you what I would. A very little more is all\npermitted to me. I cannot rest, I cannot stay, I cannot linger anywhere.\nMy spirit never walked beyond our counting-house--mark me;--in life my\nspirit never roved beyond the narrow limits of our money-changing hole;\nand weary journeys lie before me!'\n\nIt was a habit with Scrooge, whenever he became thoughtful, to put his\nhands in his breeches pockets. Pondering on what the Ghost had said, he\ndid so now, but without lifting up his eyes, or getting off his knees.\n\n[Illustration: ON THE WINGS OF THE WIND]\n\n'You must have been very slow about it, Jacob,' Scrooge observed in a\nbusiness-like manner, though with humility and deference.\n\n'Slow!' the Ghost repeated.\n\n'Seven years dead,' mused Scrooge. 'And travelling all the time?'\n\n'The whole time,' said the Ghost. 'No rest, no peace. Incessant torture\nof remorse.'\n\n'You travel fast?' said Scrooge.\n\n[Illustration]\n\n'On the wings of the wind,' replied the Ghost.\n\n'You might have got over a great quantity of ground in seven years,'\nsaid Scrooge.\n\nThe Ghost, on hearing this, set up another cry, and clanked its chain so\nhideously in the dead silence of the night, that the Ward would have\nbeen justified in indicting it for a nuisance.\n\n'Oh! captive, bound, and double-ironed,' cried the phantom, 'not to know\nthat ages of incessant labour, by immortal creatures, for this earth\nmust pass into eternity before the good of which it is susceptible is\nall developed! Not to know that any Christian spirit working kindly in\nits little sphere, whatever it may be, will find its mortal life too\nshort for its vast means of usefulness! Not to know that no space of\nregret can make amends for one life's opportunities misused! Yet such\nwas I! Oh, such was I!'\n\n'But you were always a good man of business, Jacob,' faltered Scrooge,\nwho now began to apply this to himself.\n\n'Business!' cried the Ghost, wringing its hands again. 'Mankind was my\nbusiness. The common welfare was my business; charity, mercy,\nforbearance, and benevolence were, all, my business. The dealings of my\ntrade were but a drop of water in the comprehensive ocean of my\nbusiness!'\n\nIt held up its chain at arm's-length, as if that were the cause of all\nits unavailing grief, and flung it heavily upon the ground again.\n\n'At this time of the rolling year,' the spectre said, 'I suffer most.\nWhy did I walk through crowds of fellow-beings with my eyes turned down,\nand never raise them to that blessed Star which led the Wise Men to a\npoor abode? Were there no poor homes to which its light would have\nconducted _me_?'\n\nScrooge was very much dismayed to hear the spectre going on at this\nrate, and began to quake exceedingly.\n\n'Hear me!' cried the Ghost. 'My time is nearly gone.'\n\n'I will,' said Scrooge. 'But don't be hard"}
16:35:44,395 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,396 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,396 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 23008, Requested 7168. Please try again in 352ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,397 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "had a Christmas thought, or spoke below his breath to his companion of\nsome bygone Christmas Day, with homeward hopes belonging to it. And\nevery man on board, waking or sleeping, good or bad, had had a kinder\nword for one another on that day than on any day in the year; and had\nshared to some extent in its festivities; and had remembered those he\ncared for at a distance, and had known that they delighted to remember\nhim.\n\nIt was a great surprise to Scrooge, while listening to the moaning of\nthe wind, and thinking what a solemn thing it was to move on through the\nlonely darkness over an unknown abyss, whose depths were secrets as\nprofound as death: it was a great surprise to Scrooge, while thus\nengaged, to hear a hearty laugh. It was a much greater surprise to\nScrooge to recognise it as his own nephew's and to find himself in a\nbright, dry, gleaming room, with the Spirit standing smiling by his\nside, and looking at that same nephew with approving affability!\n\n'Ha, ha!' laughed Scrooge's nephew. 'Ha, ha, ha!'\n\nIf you should happen, by any unlikely chance, to know a man more blessed\nin a laugh than Scrooge's nephew, all I can say is, I should like to\nknow him too. Introduce him to me, and I'll cultivate his acquaintance.\n\nIt is a fair, even-handed, noble adjustment of things, that while there\nis infection in disease and sorrow, there is nothing in the world so\nirresistibly contagious as laughter and good-humour. When Scrooge's\nnephew laughed in this way--holding his sides, rolling his head, and\ntwisting his face into the most extravagant contortions--Scrooge's\nniece, by marriage, laughed as heartily as he. And their assembled\nfriends, being not a bit behindhand, roared out lustily.\n\n'Ha, ha! Ha, ha, ha, ha!'\n\n'He said that Christmas was a humbug, as I live!' cried Scrooge's\nnephew. 'He believed it, too!'\n\n'More shame for him, Fred!' said Scrooge's niece indignantly. Bless\nthose women! they never do anything by halves. They are always in\nearnest.\n\nShe was very pretty; exceedingly pretty. With a dimpled,\nsurprised-looking, capital face; a ripe little mouth, that seemed made\nto be kissed--as no doubt it was; all kinds of good little dots about\nher chin, that melted into one another when she laughed; and the\nsunniest pair of eyes you ever saw in any little creature's head.\nAltogether she was what you would have called provoking, you know; but\nsatisfactory, too. Oh, perfectly satisfactory!\n\n'He's a comical old fellow,' said Scrooge's nephew, 'that's the truth;\nand not so pleasant as he might be. However, his offences carry their\nown punishment, and I have nothing to say against him.'\n\n'I'm sure he is very rich, Fred,' hinted Scrooge's niece. 'At least, you\nalways tell _me_ so.'\n\n'What of that, my dear?' said Scrooge's nephew. 'His wealth is of no use\nto him. He don't do any good with it. He don't make himself comfortable\nwith it. He hasn't the satisfaction of thinking--ha, ha, ha!--that he is\never going to benefit Us with it.'\n\n'I have no patience with him,' observed Scrooge's niece. Scrooge's\nniece's sisters, and all the other ladies, expressed the same opinion.\n\n'Oh, I have!' said Scrooge's nephew. 'I am sorry for him; I couldn't be\nangry with him if I tried. Who suffers by his ill whims? Himself always.\nHere he takes it into his head to dislike us, and he won't come and dine\nwith us. What's the consequence? He don't lose much of a dinner.'\n\n'Indeed, I think he loses a very good dinner,' interrupted Scrooge's\nniece. Everybody else said the same, and they must be allowed to have\nbeen competent judges, because they had just had dinner; and with the\ndessert upon the table, were clustered round the fire, by lamplight.\n\n'Well! I am very glad to hear it,' said Scrooge's nephew, 'because I\nhaven't any great faith in these young housekeepers. What do _you_ say,\nTopper?'\n\nTopper had clearly got his eye upon one of Scrooge's niece's sisters,\nfor he answered that a bachelor was a wretched outcast, who had no right\nto express an opinion on the subject. Whereat Scrooge's niece's\nsister--the plump one with the lace tucker: not the one with the\nroses--blushed.\n\n'Do go on, Fred,' said Scrooge's niece, clapping her hands. 'He never\nfinishes what he begins to say! He is such a ridiculous fellow!'\n\nScrooge's nephew revelled in another laugh, and as it was impossible to\nkeep the infection off, though the plump sister tried hard to do it with\naromatic vinegar, his example was unanimously followed.\n\n'I was only going to say,' said Scrooge's nephew, 'that the consequence\nof his taking a dislike to us, and not making merry with us, is, as I\nthink, that he loses some pleasant moments, which"}
16:35:44,467 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:44,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 6 retries took 3.924047041000449. input_tokens=34, output_tokens=83
16:35:44,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,666 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,667 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26313, Requested 7220. Please try again in 7.066s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,669 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': ",\ngnawed and mumbled by the hungry cold as bones are gnawed by dogs,\nstooped down at Scrooge's keyhole to regale him with a Christmas carol;\nbut, at the first sound of\n\n  'God bless you, merry gentleman,\n  May nothing you dismay!'\n\nScrooge seized the ruler with such energy of action that the singer fled\nin terror, leaving the keyhole to the fog, and even more congenial\nfrost.\n\nAt length the hour of shutting up the counting-house arrived. With an\nill-will Scrooge dismounted from his stool, and tacitly admitted the\nfact to the expectant clerk in the tank, who instantly snuffed his\ncandle out, and put on his hat.\n\n'You'll want all day to-morrow, I suppose?' said Scrooge.\n\n'If quite convenient, sir.'\n\n'It's not convenient,' said Scrooge, 'and it's not fair. If I was to\nstop half-a-crown for it, you'd think yourself ill used, I'll be bound?'\n\nThe clerk smiled faintly.\n\n'And yet,' said Scrooge, 'you don't think _me_ ill used when I pay a\nday's wages for no work.'\n\n[Illustration: _Bob Cratchit went down a slide on Cornhill, at the end\nof a lane of boys, twenty times, in honour of its being Christmas\nEve_]\n\nThe clerk observed that it was only once a year.\n\n'A poor excuse for picking a man's pocket every twenty-fifth of\nDecember!' said Scrooge, buttoning his greatcoat to the chin. 'But I\nsuppose you must have the whole day. Be here all the earlier next\nmorning.'\n\nThe clerk promised that he would; and Scrooge walked out with a growl.\nThe office was closed in a twinkling, and the clerk, with the long ends\nof his white comforter dangling below his waist (for he boasted no\ngreatcoat), went down a slide on Cornhill, at the end of a lane of boys,\ntwenty times, in honour of its being Christmas Eve, and then ran home to\nCamden Town as hard as he could pelt, to play at blind man's-buff.\n\nScrooge took his melancholy dinner in his usual melancholy tavern; and\nhaving read all the newspapers, and beguiled the rest of the evening\nwith his banker's book, went home to bed. He lived in chambers which had\nonce belonged to his deceased partner. They were a gloomy suite of\nrooms, in a lowering pile of building up a yard, where it had so little\nbusiness to be, that one could scarcely help fancying it must have run\nthere when it was a young house, playing at hide-and-seek with other\nhouses, and have forgotten the way out again. It was old enough now, and\ndreary enough; for nobody lived in it but Scrooge, the other rooms\nbeing all let out as offices. The yard was so dark that even Scrooge,\nwho knew its every stone, was fain to grope with his hands. The fog and\nfrost so hung about the black old gateway of the house, that it seemed\nas if the Genius of the Weather sat in mournful meditation on the\nthreshold.\n\nNow, it is a fact that there was nothing at all particular about the\nknocker on the door, except that it was very large. It is also a fact\nthat Scrooge had seen it, night and morning, during his whole residence\nin that place; also that Scrooge had as little of what is called fancy\nabout him as any man in the City of London, even including--which is a\nbold word--the corporation, aldermen, and livery. Let it also be borne\nin mind that Scrooge had not bestowed one thought on Marley since his\nlast mention of his seven-years'-dead partner that afternoon. And then\nlet any man explain to me, if he can, how it happened that Scrooge,\nhaving his key in the lock of the door, saw in the knocker, without its\nundergoing any intermediate process of change--not a knocker, but\nMarley's face.\n\nMarley's face. It was not in impenetrable shadow, as the other objects\nin the yard were, but had a dismal light about it, like a bad lobster in\na dark cellar. It was not angry or ferocious, but looked at Scrooge as\nMarley used to look; with ghostly spectacles turned up on its ghostly\nforehead. The hair was curiously stirred, as if by breath or hot air;\nand, though the eyes were wide open, they were perfectly motionless.\nThat, and its livid colour, made it horrible; but its horror seemed to\nbe in spite of the face, and beyond its control, rather than a part of\nits own expression.\n\nAs Scrooge looked fixedly at this phenomenon, it was a knocker again.\n\nTo say that he was not startled, or that his blood was not conscious of\na terrible sensation to which it had been a stranger from infancy, would\nbe untrue. But he put his hand upon the key he had relinquished, turned\nit sturdily, walked in, and lighted his candle.\n\nHe _did_ pause, with a moment's irresolution, before he shut the door;\nand he _did_ look cautiously behind it first, as if he half expected to\nbe terrified with the sight of Marley's pigtail sticking out into the\nhall. But there was nothing on the back of the door,"}
16:35:44,742 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,743 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,743 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26271, Requested 7171. Please try again in 6.884s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,744 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': "announced itself in awful language.\n\nThe room was very dark, too dark to be observed with any accuracy,\nthough Scrooge glanced round it in obedience to a secret impulse,\nanxious to know what kind of room it was. A pale light, rising in the\nouter air, fell straight upon the bed; and on it, plundered and bereft,\nunwatched, unwept, uncared for, was the body of this man.\n\nScrooge glanced towards the Phantom. Its steady hand was pointed to the\nhead. The cover was so carelessly adjusted that the slightest raising of\nit, the motion of a finger upon Scrooge's part, would have disclosed the\nface. He thought of it, felt how easy it would be to do, and longed to\ndo it; but he had no more power to withdraw the veil than to dismiss the\nspectre at his side.\n\nOh, cold, cold, rigid, dreadful Death, set up thine altar here, and\ndress it with such terrors as thou hast at thy command; for this is thy\ndominion! But of the loved, revered, and honoured head thou canst not\nturn one hair to thy dread purposes, or make one feature odious. It is\nnot that the hand is heavy, and will fall down when released; it is not\nthat the heart and pulse are still; but that the hand was open,\ngenerous, and true; the heart brave, warm, and tender, and the pulse a\nman's. Strike, Shadow, strike! And see his good deeds springing from the\nwound, to sow the world with life immortal!\n\nNo voice pronounced these words in Scrooge's ears, and yet he heard them\nwhen he looked upon the bed. He thought, if this man could be raised up\nnow, what would be his foremost thoughts? Avarice, hard dealing, griping\ncares? They have brought him to a rich end, truly!\n\nHe lay in the dark, empty house, with not a man, a woman, or a child to\nsay he was kind to me in this or that, and for the memory of one kind\nword I will be kind to him. A cat was tearing at the door, and there was\na sound of gnawing rats beneath the hearthstone. What _they_ wanted in\nthe room of death, and why they were so restless and disturbed, Scrooge\ndid not dare to think.\n\n'Spirit!' he said, 'this is a fearful place. In leaving it, I shall not\nleave its lesson, trust me. Let us go!'\n\nStill the Ghost pointed with an unmoved finger to the head.\n\n'I understand you,' Scrooge returned, 'and I would do it if I could. But\nI have not the power, Spirit. I have not the power.'\n\nAgain it seemed to look upon him.\n\n'If there is any person in the town who feels emotion caused by this\nman's death,' said Scrooge, quite agonised, 'show that person to me,\nSpirit, I beseech you!'\n\nThe Phantom spread its dark robe before him for a moment, like a wing;\nand, withdrawing it, revealed a room by daylight, where a mother and her\nchildren were.\n\nShe was expecting some one, and with anxious eagerness; for she walked\nup and down the room, started at every sound, looked out from the\nwindow, glanced at the clock, tried, but in vain, to work with her\nneedle, and could hardly bear the voices of her children in their play.\n\nAt length the long-expected knock was heard. She hurried to the door,\nand met her husband; a man whose face was careworn and depressed, though\nhe was young. There was a remarkable expression in it now, a kind of\nserious delight of which he felt ashamed, and which he struggled to\nrepress.\n\nHe sat down to the dinner that had been hoarding for him by the fire,\nand when she asked him faintly what news (which was not until after a\nlong silence), he appeared embarrassed how to answer.\n\n'Is it good,' she said, 'or bad?' to help him.\n\n'Bad,' he answered.\n\n'We are quite ruined?'\n\n'No. There is hope yet, Caroline.'\n\n'If _he_ relents,' she said, amazed, 'there is! Nothing is past hope, if\nsuch a miracle has happened.'\n\n'He is past relenting,' said her husband. 'He is dead.'\n\nShe was a mild and patient creature, if her face spoke truth; but she\nwas thankful in her soul to hear it, and she said so with clasped hands.\nShe prayed forgiveness the next moment, and was sorry; but the first was\nthe emotion of her heart.\n\n'What the half-drunken woman, whom I told you of last night, said to me\nwhen I tried to see him and obtain a week's delay--and what I thought\nwas a mere excuse to avoid me--turns out to have been quite true. He was\nnot only very ill, but dying, then.'\n\n'To whom will our debt be transferred?'\n\n'I don't know. But, before that time, we shall be ready with the money;\nand even though we were not, it would be bad fortune indeed to find so\nmerciless a creditor in his successor. We may sleep to-night with light\nhearts, Caroline!'\n\nYes. Soften it as they would, their hearts were lighter. The children's\nfaces, hushed and clustered round to hear what they so little\nunderstood, were brighter; and it was a happier house for this man's\ndeath! The only emotion that the Ghost could"}
16:35:44,838 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:44,839 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:44,839 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26226, Requested 7039. Please try again in 6.53s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:44,840 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': ',\' replied the woman: \'and it\nshould have been, you may depend upon it, if I could have laid my hands\non anything else. Open that bundle, old Joe, and let me know the value\nof it. Speak out plain. I\'m not afraid to be the first, nor afraid for\nthem to see it. We knew pretty well that we were helping ourselves\nbefore we met here, I believe. It\'s no sin. Open the bundle, Joe.\'\n\nBut the gallantry of her friends would not allow of this; and the man in\nfaded black, mounting the breach first, produced _his_ plunder. It was\nnot extensive. A seal or two, a pencil-case, a pair of sleeve-buttons,\nand a brooch of no great value, were all. They were severally examined\nand appraised by old Joe, who chalked the sums he was disposed to give\nfor each upon the wall, and added them up into a total when he found\nthat there was nothing more to come.\n\n\'That\'s your account,\' said Joe, \'and I wouldn\'t give another sixpence,\nif I was to be boiled for not doing it. Who\'s next?\'\n\n\n[Illustration: _"What do you call this?" said Joe. "Bed-curtains."_]\n\nMrs. Dilber was next. Sheets and towels, a little wearing apparel, two\nold fashioned silver teaspoons, a pair of sugar-tongs, and a few\nboots. Her account was stated on the wall in the same manner.\n\n\'I always give too much to ladies. It\'s a weakness of mine, and that\'s\nthe way I ruin myself,\' said old Joe. \'That\'s your account. If you asked\nme for another penny, and made it an open question, I\'d repent of being\nso liberal, and knock off half-a-crown.\'\n\n\'And now undo _my_ bundle, Joe,\' said the first woman.\n\nJoe went down on his knees for the greater convenience of opening it,\nand, having unfastened a great many knots, dragged out a large heavy\nroll of some dark stuff.\n\n\'What do you call this?\' said Joe. \'Bed-curtains?\'\n\n\'Ah!\' returned the woman, laughing and leaning forward on her crossed\narms. \'Bed-curtains!\'\n\n\'You don\'t mean to say you took \'em down, rings and all, with him lying\nthere?\' said Joe.\n\n\'Yes, I do,\' replied the woman. \'Why not?\'\n\n\'You were born to make your fortune,\' said Joe, \'and you\'ll certainly do\nit.\'\n\n\'I certainly shan\'t hold my hand, when I can get anything in it by\nreaching it out, for the sake of such a man as he was, I promise you,\nJoe,\' returned the woman coolly. \'Don\'t drop that oil upon the blankets,\nnow.\'\n\n\'His blankets?\' asked Joe.\n\n\'Whose else\'s do you think?\' replied the woman. \'He isn\'t likely to take\ncold without \'em, I dare say.\'\n\n\'I hope he didn\'t die of anything catching? Eh?\' said old Joe, stopping\nin his work, and looking up.\n\n\'Don\'t you be afraid of that,\' returned the woman. \'I an\'t so fond of\nhis company that I\'d loiter about him for such things, if he did. Ah!\nyou may look through that shirt till your eyes ache, but you won\'t find\na hole in it, nor a threadbare place. It\'s the best he had, and a fine\none too. They\'d have wasted it, if it hadn\'t been for me.\'\n\n\'What do you call wasting of it?\' asked old Joe.\n\n\'Putting it on him to be buried in, to be sure,\' replied the woman, with\na laugh. \'Somebody was fool enough to do it, but I took it off again. If\ncalico an\'t good enough for such a purpose, it isn\'t good enough for\nanything. It\'s quite as becoming to the body. He can\'t look uglier than\nhe did in that one.\'\n\nScrooge listened to this dialogue in horror. As they sat grouped about\ntheir spoil, in the scanty light afforded by the old man\'s lamp, he\nviewed them with a detestation and disgust which could hardly have been\ngreater, though they had been obscene demons marketing the corpse\nitself.\n\n\'Ha, ha!\' laughed the same woman when old Joe producing a flannel bag\nwith money in it, told out their several gains upon the ground. \'This\nis the end of it, you see! He frightened every one away from him when he\nwas alive, to profit us when he was dead! Ha, ha, ha!\'\n\n\'Spirit!\' said Scrooge, shuddering from head to foot. \'I see, I see. The\ncase of this unhappy man might be my own. My life tends that way now.\nMerciful heaven, what is this?\'\n\nHe recoiled in terror, for the scene had changed, and now he almost\ntouched a bed--a bare, uncurtained bed--on which, beneath a ragged\nsheet, there lay a something covered up, which, though it was dumb,\nannounced itself in awful language.\n\nThe room was very dark, too dark to be observed with any accuracy,\nthough Scrooge glanced round it in obedience to a secret impulse,\nanxious to know what kind of room it was. A pale light, rising in the\nouter air, fell straight upon the bed; and on it, plundered and bereft,\nunwatched, unwept, uncared for, was the body of this man.\n\nScrooge glanced towards the Phantom'}
16:35:45,98 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:45,99 graphrag.llm.base.rate_limiting_llm WARNING Process failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:45,99 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 153, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 26096, Requested 7030. Please try again in 6.252s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:45,100 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': '.C. The Project Gutenberg Literary Archive Foundation (“the\nFoundation” or PGLAF), owns a compilation copyright in the collection\nof Project Gutenberg™ electronic works. Nearly all the individual\nworks in the collection are in the public domain in the United\nStates. If an individual work is unprotected by copyright law in the\nUnited States and you are located in the United States, we do not\nclaim a right to prevent you from copying, distributing, performing,\ndisplaying or creating derivative works based on the work as long as\nall references to Project Gutenberg are removed. Of course, we hope\nthat you will support the Project Gutenberg™ mission of promoting\nfree access to electronic works by freely sharing Project Gutenberg™\nworks in compliance with the terms of this agreement for keeping the\nProject Gutenberg™ name associated with the work. You can easily\ncomply with the terms of this agreement by keeping this work in the\nsame format with its attached full Project Gutenberg™ License when\nyou share it without charge with others.\n\n1.D. The copyright laws of the place where you are located also govern\nwhat you can do with this work. Copyright laws in most countries are\nin a constant state of change. If you are outside the United States,\ncheck the laws of your country in addition to the terms of this\nagreement before downloading, copying, displaying, performing,\ndistributing or creating derivative works based on this work or any\nother Project Gutenberg™ work. The Foundation makes no\nrepresentations concerning the copyright status of any work in any\ncountry other than the United States.\n\n1.E. Unless you have removed all references to Project Gutenberg:\n\n1.E.1. The following sentence, with active links to, or other\nimmediate access to, the full Project Gutenberg™ License must appear\nprominently whenever any copy of a Project Gutenberg™ work (any work\non which the phrase “Project Gutenberg” appears, or with which the\nphrase “Project Gutenberg” is associated) is accessed, displayed,\nperformed, viewed, copied or distributed:\n\n    This eBook is for the use of anyone anywhere in the United States and most\n    other parts of the world at no cost and with almost no restrictions\n    whatsoever. You may copy it, give it away or re-use it under the terms\n    of the Project Gutenberg License included with this eBook or online\n    at www.gutenberg.org. If you\n    are not located in the United States, you will have to check the laws\n    of the country where you are located before using this eBook.\n  \n1.E.2. If an individual Project Gutenberg™ electronic work is\nderived from texts not protected by U.S. copyright law (does not\ncontain a notice indicating that it is posted with permission of the\ncopyright holder), the work can be copied and distributed to anyone in\nthe United States without paying any fees or charges. If you are\nredistributing or providing access to a work with the phrase “Project\nGutenberg” associated with or appearing on the work, you must comply\neither with the requirements of paragraphs 1.E.1 through 1.E.7 or\nobtain permission for the use of the work and the Project Gutenberg™\ntrademark as set forth in paragraphs 1.E.8 or 1.E.9.\n\n1.E.3. If an individual Project Gutenberg™ electronic work is posted\nwith the permission of the copyright holder, your use and distribution\nmust comply with both paragraphs 1.E.1 through 1.E.7 and any\nadditional terms imposed by the copyright holder. Additional terms\nwill be linked to the Project Gutenberg™ License for all works\nposted with the permission of the copyright holder found at the\nbeginning of this work.\n\n1.E.4. Do not unlink or detach or remove the full Project Gutenberg™\nLicense terms from this work, or any files containing a part of this\nwork or any other work associated with Project Gutenberg™.\n\n1.E.5. Do not copy, display, perform, distribute or redistribute this\nelectronic work, or any part of this electronic work, without\nprominently displaying the sentence set forth in paragraph 1.E.1 with\nactive links or immediate access to the full terms of the Project\nGutenberg™ License.\n\n1.E.6. You may convert to and distribute this work in any binary,\ncompressed, marked up, nonproprietary or proprietary form, including\nany word processing or hypertext form. However, if you provide access\nto or distribute copies of a Project Gutenberg™ work in a format\nother than “Plain Vanilla ASCII” or other format used in the official\nversion posted on the official Project Gutenberg™ website\n(www.gutenberg.org), you must, at no additional cost, fee or expense\nto the user, provide a copy, a means of exporting a copy, or a means\nof obtaining a copy upon request, of the work in its original “Plain\nVanilla ASCII” or other form. Any alternate format must include the\nfull Project Gutenberg™ License as specified in paragraph 1.E.1.\n\n1.E.7. Do not charge a fee for access to, viewing, displaying,\nperforming, copying or distributing any Project Gutenberg™ works\nunless you comply with paragraph 1.E.8 or 1.E.9.\n\n1.E.8. You may charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg™ electronic works\nprovided that:\n\n    • You pay a royalty fee of 20% of the gross profits you derive from\n        the use of Project Gutenberg™ works calculated using the method\n        you already use to calculate your applicable taxes. The fee is owed\n        to the owner of the Project Gutenberg™ trademark, but he has\n        agreed to donate royalties under this paragraph to the Project\n        Gutenberg Literary'}
16:35:45,232 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:45,233 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 8/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:45,547 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:45,549 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 10/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:45,550 root ERROR error extracting graph
Traceback (most recent call last):
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 125, in __call__
    result = await self._process_document(text, prompt_variables)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/index/graph/extractors/graph/graph_extractor.py", line 164, in _process_document
    response = await self._llm(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 151, in do_attempt
    await sleep_for(sleep_time)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 50, in __call__
    return await self._invoke(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/base/base_llm.py", line 54, in _invoke
    output = await self._execute_llm(input, **kwargs)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 53, in _execute_llm
    completion = await self.client.chat.completions.create(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py", line 1661, in create
    return await self._post(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1839, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1533, in request
    return await self._request(
  File "/Users/apple/Software/Conda/envs/llm/lib/python3.10/site-packages/openai/_base_client.py", line 1634, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-ArL0bWW5GpZceNsS7HX4U0zL on tokens per min (TPM): Limit 30000, Used 25918, Requested 7270. Please try again in 6.376s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
16:35:45,551 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'the money;\nand even though we were not, it would be bad fortune indeed to find so\nmerciless a creditor in his successor. We may sleep to-night with light\nhearts, Caroline!\'\n\nYes. Soften it as they would, their hearts were lighter. The children\'s\nfaces, hushed and clustered round to hear what they so little\nunderstood, were brighter; and it was a happier house for this man\'s\ndeath! The only emotion that the Ghost could show him, caused by the\nevent, was one of pleasure.\n\n\'Let me see some tenderness connected with a death,\' said Scrooge; \'or\nthat dark chamber, Spirit, which we left just now, will be for ever\npresent to me.\'\n\nThe Ghost conducted him through several streets familiar to his feet;\nand as they went along, Scrooge looked here and there to find himself,\nbut nowhere was he to be seen. They entered poor Bob Cratchit\'s house;\nthe dwelling he had visited before; and found the mother and the\nchildren seated round the fire.\n\nQuiet. Very quiet. The noisy little Cratchits were as still as statues\nin one corner, and sat looking up at Peter, who had a book before him.\nThe mother and her daughters were engaged in sewing. But surely they\nwere very quiet!\n\n\'"And he took a child, and set him in the midst of them."\'\n\nWhere had Scrooge heard those words? He had not dreamed them. The boy\nmust have read them out as he and the Spirit crossed the threshold. Why\ndid he not go on?\n\nThe mother laid her work upon the table, and put her hand up to her\nface.\n\n\'The colour hurts my eyes,\' she said.\n\nThe colour? Ah, poor Tiny Tim!\n\n\'They\'re better now again,\' said Cratchit\'s wife. \'It makes them weak by\ncandle-light; and I wouldn\'t show weak eyes to your father when he comes\nhome for the world. It must be near his time.\'\n\n\'Past it rather,\' Peter answered, shutting up his book. \'But I think he\nhas walked a little slower than he used, these few last evenings,\nmother.\'\n\nThey were very quiet again. At last she said, and in a steady, cheerful\nvoice, that only faltered once:\n\n\'I have known him walk with--I have known him walk with Tiny Tim upon\nhis shoulder very fast indeed.\'\n\n\'And so have I,\' cried Peter. \'Often.\'\n\n\'And so have I,\' exclaimed another. So had all.\n\n\'But he was very light to carry,\' she resumed, intent upon her work,\n\'and his father loved him so, that it was no trouble, no trouble. And\nthere is your father at the door!\'\n\nShe hurried out to meet him; and little Bob in his comforter--he had\nneed of it, poor fellow--came in. His tea was ready for him on the hob,\nand they all tried who should help him to it most. Then the two young\nCratchits got upon his knees, and laid, each child, a little cheek\nagainst his face, as if they said, \'Don\'t mind it, father. Don\'t be\ngrieved!\'\n\nBob was very cheerful with them, and spoke pleasantly to all the family.\nHe looked at the work upon the table, and praised the industry and speed\nof Mrs. Cratchit and the girls. They would be done long before Sunday,\nhe said.\n\n\'Sunday! You went to-day, then, Robert?\' said his wife.\n\n\'Yes, my dear,\' returned Bob. \'I wish you could have gone. It would have\ndone you good to see how green a place it is. But you\'ll see it often. I\npromised him that I would walk there on a Sunday. My little, little\nchild!\' cried Bob. \'My little child!\'\n\nHe broke down all at once. He couldn\'t help it. If he could have helped\nit, he and his child would have been farther apart, perhaps, than they\nwere.\n\nHe left the room, and went upstairs into the room above, which was\nlighted cheerfully, and hung with Christmas. There was a chair set close\nbeside the child, and there were signs of some one having been there\nlately. Poor Bob sat down in it, and when he had thought a little and\ncomposed himself, he kissed the little face. He was reconciled to what\nhad happened, and went down again quite happy.\n\nThey drew about the fire, and talked, the girls and mother working\nstill. Bob told them of the extraordinary kindness of Mr. Scrooge\'s\nnephew, whom he had scarcely seen but once, and who, meeting him in the\nstreet that day, and seeing that he looked a little--\'just a little\ndown, you know,\' said Bob, inquired what had happened to distress him.\n\'On which,\' said Bob, \'for he is the pleasantest-spoken gentleman you\never heard, I told him. "I am heartily sorry for it, Mr. Cratchit," he\nsaid, "and heartily sorry for your good wife." By-the-bye, how he ever\nknew _that_ I don\'t know.\'\n\n\'Knew what, my dear?\'\n\n\'Why, that you were a good wife,\' replied Bob.\n\n\'Everybody knows that,\' said Peter.\n\n\'Very well observed, my boy!\' cried Bob. \'I hope they do. "Heartily\nsorry," he said, "for your good wife. If I can be of service to you in\nany way," he said, giving me his card, "that\'s where I live. Pray come\nto me."'}
16:35:51,999 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:52,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 7.670633084002475. input_tokens=34, output_tokens=171
16:35:56,402 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:35:56,405 graphrag.llm.base.rate_limiting_llm WARNING extract-continuation-0 failed to invoke LLM 9/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:35:57,969 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:35:57,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 4.051290625000547. input_tokens=34, output_tokens=88
16:36:15,452 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:15,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 9 retries took 9.047385834001034. input_tokens=34, output_tokens=152
16:36:19,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.6462517920008395. input_tokens=171, output_tokens=80
16:36:19,558 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.0771052919990325. input_tokens=161, output_tokens=64
16:36:19,598 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.111007583000173. input_tokens=175, output_tokens=112
16:36:19,701 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.218211667001015. input_tokens=186, output_tokens=102
16:36:19,923 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:19,926 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.422390625000844. input_tokens=189, output_tokens=85
16:36:20,84 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.587705082998582. input_tokens=163, output_tokens=90
16:36:20,342 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.851693790998979. input_tokens=160, output_tokens=119
16:36:20,568 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.066203166999912. input_tokens=178, output_tokens=104
16:36:20,637 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.130069082999398. input_tokens=174, output_tokens=103
16:36:20,978 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:20,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.4808825830004935. input_tokens=186, output_tokens=78
16:36:21,494 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,498 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.990342624998448. input_tokens=183, output_tokens=85
16:36:21,499 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,502 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.006378292000591. input_tokens=164, output_tokens=88
16:36:21,652 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.171888500000932. input_tokens=173, output_tokens=93
16:36:21,833 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:21,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.324265999999625. input_tokens=178, output_tokens=117
16:36:22,900 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:22,904 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.412575166999886. input_tokens=170, output_tokens=124
16:36:24,358 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:24,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.84964279199994. input_tokens=191, output_tokens=143
16:36:32,959 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:36:32,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.488486958001886. input_tokens=380, output_tokens=339
16:37:01,271 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
16:37:01,272 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,275 datashaper.workflow.workflow INFO executing verb create_final_entities
16:37:01,280 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
16:37:01,409 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
16:37:01,409 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,413 datashaper.workflow.workflow INFO executing verb create_final_nodes
16:37:01,427 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
16:37:01,546 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
16:37:01,546 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,551 datashaper.workflow.workflow INFO executing verb create_final_communities
16:37:01,564 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
16:37:01,682 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph', 'create_final_nodes']
16:37:01,682 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,683 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
16:37:01,708 datashaper.workflow.workflow INFO executing verb create_final_relationships
16:37:01,714 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
16:37:01,830 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_base_text_units', 'create_final_entities']
16:37:01,830 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:37:01,833 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
16:37:01,833 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:37:01,840 datashaper.workflow.workflow INFO executing verb create_final_text_units
16:37:01,847 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
16:37:01,969 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_communities', 'create_final_nodes', 'create_final_entities']
16:37:01,970 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:37:01,972 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
16:37:01,974 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
16:37:01,976 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:37:01,983 datashaper.workflow.workflow INFO executing verb create_final_community_reports
16:37:01,987 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 59
16:37:02,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,936 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:02,938 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,940 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:02,980 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,981 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:02,985 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:02,986 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 1/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:04,454 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:04,457 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:04,559 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:04,561 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:04,822 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:04,825 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:05,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:05,107 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 2/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,517 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,519 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,590 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,592 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,777 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,778 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:07,910 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:07,912 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 3/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:12,733 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:12,735 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:12,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:12,898 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:13,296 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:13,298 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 4/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:22,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:22,632 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:22,918 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
16:37:22,919 graphrag.llm.base.rate_limiting_llm WARNING create_community_report failed to invoke LLM 5/10 attempts. Cause: rate limit exceeded, will retry. Recommended sleep for 0 seconds. Follow recommendation? True
16:37:23,968 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:23,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 21.941597334000107. input_tokens=2298, output_tokens=708
16:37:26,411 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:26,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.38557283300179. input_tokens=2485, output_tokens=786
16:37:26,719 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:26,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 24.69984545799889. input_tokens=2371, output_tokens=692
16:37:28,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:28,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 26.114666207999107. input_tokens=2058, output_tokens=500
16:37:37,178 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:37,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 3 retries took 25.071480375001556. input_tokens=2355, output_tokens=708
16:37:40,10 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:40,15 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 37.98843712500093. input_tokens=2478, output_tokens=781
16:37:42,343 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:42,348 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 40.32822229099838. input_tokens=4195, output_tokens=889
16:37:43,867 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:43,871 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 4 retries took 22.591396915999212. input_tokens=2217, output_tokens=459
16:37:57,33 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:37:57,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 24.40175087500029. input_tokens=2116, output_tokens=618
16:38:01,997 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
16:38:02,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 5 retries took 29.078852458002075. input_tokens=2368, output_tokens=678
16:38:02,14 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
16:38:02,148 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_documents', 'create_final_text_units', 'create_final_community_reports', 'create_final_relationships', 'create_final_entities']
16:38:02,150 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
16:38:02,153 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
16:38:02,155 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
16:38:02,157 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
16:38:02,159 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
16:38:02,167 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
16:38:02,168 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
16:38:02,168 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
16:38:02,170 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
16:38:02,187 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
16:38:02,187 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
16:38:02,191 graphrag.index.operations.embed_text.strategies.openai INFO embedding 59 inputs via 59 snippets using 4 batches. max_batch_size=16, max_tokens=8191
16:38:03,439 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,643 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,667 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:03,758 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5639834579997114. input_tokens=241, output_tokens=0
16:38:04,141 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9467990000011923. input_tokens=664, output_tokens=0
16:38:04,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.0545979579983396. input_tokens=938, output_tokens=0
16:38:04,387 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1938268750018324. input_tokens=602, output_tokens=0
16:38:04,449 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
16:38:04,501 graphrag.index.operations.embed_text.strategies.openai INFO embedding 42 inputs via 42 snippets using 7 batches. max_batch_size=16, max_tokens=8191
16:38:05,158 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:05,327 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:05,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:05,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9701062919994001. input_tokens=7200, output_tokens=0
16:38:05,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1150982079998357. input_tokens=7200, output_tokens=0
16:38:05,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.26774637500057. input_tokens=7200, output_tokens=0
16:38:05,858 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,65 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,81 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5538726249978936. input_tokens=7055, output_tokens=0
16:38:06,140 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.7801383749974775. input_tokens=7200, output_tokens=0
16:38:06,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:06,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.9572205420008686. input_tokens=7200, output_tokens=0
16:38:06,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 2.1077710409990686. input_tokens=7200, output_tokens=0
16:38:06,657 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
16:38:06,666 graphrag.index.operations.embed_text.strategies.openai INFO embedding 10 inputs via 10 snippets using 1 batches. max_batch_size=16, max_tokens=8191
16:38:07,249 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
16:38:07,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7823621249990538. input_tokens=5526, output_tokens=0
16:38:07,505 graphrag.cli.index INFO All workflows completed successfully.
